{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUw9eWQBmnCg"
      },
      "source": [
        "# **DATA SCIENCE 2 - ENTREGA PROYECTO FINAL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mxznh39mpaQ"
      },
      "source": [
        "###  **PREDICCI√ìN DE PM2.5 MEDIANTE MACHINE LEARNING:**\n",
        "### **An√°lisis Global de Factores Meteorol√≥gicos y Socioecon√≥micos**\n",
        "\n",
        "---\n",
        "\n",
        "### **INFORMACI√ìN ACAD√âMICA** üë®‚Äçüéì\n",
        "\n",
        "**Alumno:** Miguel √Ångel Di Rocco  \n",
        "**Carrera:** Data Science II  \n",
        "**Instituci√≥n:** Coderhouse  \n",
        "**Fecha de entrega:** 21/10/2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A38tHNobmv-e"
      },
      "source": [
        "# **1. METADATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9akWYp7omyPE"
      },
      "source": [
        "## 1. **DESCRIPCI√ìN DEL DATASET Y DATOS**\n",
        "\n",
        "---\n",
        "\n",
        "### **Descripci√≥n del Dataset Final**\n",
        "\n",
        "El **Global Weather Repository** contiene informaci√≥n meteorol√≥gica y de calidad del aire de capitales mundiales, con datos capturados entre **mayo 2024 y abril 2025**. Despu√©s del procesamiento, limpieza, transformaciones y Feature Engineering, el dataset final para modelado (`df_ml`) presenta las siguientes caracter√≠sticas:\n",
        "\n",
        "**Dimensiones:**\n",
        "* **78,238 registros** (filtrados por outliers extremos con umbrales f√≠sicos)\n",
        "* **13 columnas** (1 target + 9 features num√©ricas + 3 variables categ√≥ricas/contextuales)\n",
        "* **Cobertura global**: 190 pa√≠ses\n",
        "\n",
        "**Variables del Dataset:**\n",
        "\n",
        "**Target (1):**\n",
        "- `pm25_log`: Transformaci√≥n logar√≠tmica de PM2.5 (log1p)\n",
        "\n",
        "**Features Num√©ricas (9):**\n",
        "- `wind_log`: Velocidad del viento transformada (log1p)\n",
        "- `pop_density_log`: Densidad poblacional transformada (log1p)\n",
        "- `temperature_celsius`: Temperatura en grados Celsius\n",
        "- `humidity`: Humedad relativa (%)\n",
        "- `cloud`: Nubosidad (%)\n",
        "- `uv_index`: √çndice UV\n",
        "- `precip_log`: Precipitaci√≥n transformada (log1p)\n",
        "- `month`: Mes del a√±o (1-12)\n",
        "- `hour`: Hora del d√≠a (0-23)\n",
        "\n",
        "**Variables Binarias/Categ√≥ricas (3):**\n",
        "- `rained`: Indicador de lluvia (0/1)\n",
        "- `high_wind`: Indicador de viento fuerte >20 kph (0/1)\n",
        "- `country`: Pa√≠s (190 categor√≠as √∫nicas)\n",
        "\n",
        "**Enriquecimiento Externo:**\n",
        "- Poblaci√≥n y densidad poblacional obtenidas mediante API RestCountries\n",
        "\n",
        "### **Fuente y Disponibilidad**\n",
        "\n",
        "* **Fuente primaria:** Kaggle - Global Weather Repository\n",
        "* **URL:** `https://www.kaggle.com/datasets/nelgiriyewithana/global-weather-repository`\n",
        "* **Enriquecimiento:** API RestCountries (poblaci√≥n, densidad).\n",
        "* **Per√≠odo:** Mayo 2024 - Abril 2025.\n",
        "* **Licencia:** Uso acad√©mico y de investigaci√≥n.\n",
        "\n",
        "**Caracter√≠sticas del Dataset Original (`df_original` al inicio de la fase):**\n",
        "* **Dimensiones:** $\\mathbf{78,331}$ **filas** $\\times$ $\\mathbf{41}$ **columnas** (datos originales).\n",
        "* **Variables Clave:** Inclu√≠a las variables originales no transformadas (`air_quality_PM2.5`, `precip_mm`, `pressure_mb`, etc.) antes de la selecci√≥n por VIF y las transformaciones logar√≠tmicas.\n",
        "\n",
        "### **Valor para Machine Learning**\n",
        "\n",
        "Este *dataset* procesado (**df_ml**) es ideal para predicci√≥n de PM2.5 porque:\n",
        "\n",
        "* **Asimetr√≠a Corregida:** Correcci√≥n de asimetr√≠as mediante **transformaci√≥n logar√≠tmica** en el *target* y *features* clave.\n",
        "* **Efectos No Lineales Capturados:** Variables derivadas binarias (**`rained`**, **`high_wind`**) capturan efectos de umbral clave.\n",
        "* **Multicolinealidad Controlada:** La variable `pressure_mb` fue **eliminada** tras la validaci√≥n VIF, reduciendo la redundancia cr√≠tica.\n",
        "* **Diversidad Temporal:** Patrones estacionales (`month`) y diurnos (`hour`) capturados como *features* categ√≥ricas.\n",
        "* **Diversidad Geogr√°fica:** 190 pa√≠ses (`country`) para an√°lisis estratificado posterior.\n",
        "\n",
        "---\n",
        "\n",
        "Este diccionario describe las variables incluidas en el *DataFrame* de Machine Learning (`df_ml`), tras las etapas de Feature Engineering y limpieza.\n",
        "\n",
        "**Dimensiones:** 78,238 registros √ó 13 variables\n",
        "\n",
        "| Variable | Tipo de Dato | Rol | Descripci√≥n y Origen | Transformaci√≥n Aplicada | Rango |\n",
        "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
        "| **`pm25_log`** | Flotante | **Target (Y)** | Logaritmo natural transformado del promedio de PM2.5: $\\log(1+x)$ donde $x$ es la concentraci√≥n en ¬µg/m¬≥. | **log1p** | 0.00 - 6.21 |\n",
        "| **`wind_log`** | Flotante | Feature (X) | Logaritmo natural transformado de la velocidad del viento: $\\log(1+x)$ donde $x$ es la velocidad en kph. | **log1p** | 0.00 - 4.62 |\n",
        "| **`pop_density_log`** | Flotante | Feature (X) | Logaritmo natural transformado de la densidad poblacional: $\\log(1+x)$ donde $x$ es habitantes/km¬≤. Fuente: API RestCountries. | **log1p** | 0.69 - 9.63 |\n",
        "| **`temperature_celsius`** | Flotante | Feature (X) | Temperatura del aire en grados Celsius. Rango f√≠sico: -40¬∞C a 50¬∞C. | **Ninguna** | -40.0 - 50.0 |\n",
        "| **`humidity`** | Entero | Feature (X) | Porcentaje de humedad relativa del aire. | **Ninguna** | 0 - 100 |\n",
        "| **`cloud`** | Entero | Feature (X) | Porcentaje de cobertura de nubes. | **Ninguna** | 0 - 100 |\n",
        "| **`uv_index`** | Flotante | Feature (X) | √çndice de radiaci√≥n ultravioleta (escala OMS: 0-11+). | **Ninguna** | 0.0 - 12.0 |\n",
        "| **`precip_log`** | Flotante | Feature (X) | Logaritmo natural transformado de precipitaci√≥n: $\\log(1+x)$ donde $x$ es precipitaci√≥n en mm. | **log1p** | 0.00 - 6.93 |\n",
        "| **`rained`** | Binario | Feature (X) | $\\mathbf{1}$ si precipitaci√≥n $\\mathbf{> 0 \\text{ mm}}$, $\\mathbf{0}$ en caso contrario. Efecto de umbral validado (p < 0.001). | **Derivada (Umbral)** | {0, 1} |\n",
        "| **`high_wind`** | Binario | Feature (X) | $\\mathbf{1}$ si velocidad del viento $\\mathbf{> 20 \\text{ kph}}$, $\\mathbf{0}$ en caso contrario. Efecto de umbral validado (p < 0.001). | **Derivada (Umbral)** | {0, 1} |\n",
        "| **`month`** | Entero | Feature (X) | Mes de la observaci√≥n (1=Enero, ..., 12=Diciembre). Captura estacionalidad. | **Ordinal** | 1 - 12 |\n",
        "| **`hour`** | Entero | Feature (X) | Hora del d√≠a (formato 24h). Captura patr√≥n diurno. | **Ordinal** | 0 - 23 |\n",
        "| **`country`** | Cadena | Contexto/Feature | Pa√≠s de la observaci√≥n. 190 categor√≠as √∫nicas. Usado para an√°lisis estratificado y target encoding (Entrega 2). | **Categ√≥rica** | 190 pa√≠ses |\n",
        "\n",
        "---\n",
        "\n",
        "## **Notas sobre el Preprocesamiento**\n",
        "\n",
        "* **Tratamiento Categ√≥rico:** Las variables `month`, `hour` y `country` ser√°n tratadas como **categ√≥ricas** mediante **One-Hot Encoding (OHE)** en el `ColumnTransformer` (preprocesador).\n",
        "* **Escalado:** Las variables num√©ricas continuas sin transformaci√≥n logar√≠tmica (`temperature_celsius`, `humidity`, `cloud`, `uv_index`) ser√°n **estandarizadas** (`StandardScaler`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrT1i8DVnFby"
      },
      "source": [
        "# **2. CONTEXTO DEL PROYECTO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPzV1z-Vnq3r"
      },
      "source": [
        "### **DESCRIPCI√ìN DEL PROYECTO**\n",
        "\n",
        "Este proyecto busca **predecir los niveles de PM2.5** en el aire utilizando datos del clima. Las part√≠culas PM2.5 son contaminantes muy peque√±os que afectan la salud humana, especialmente el sistema respiratorio.\n",
        "\n",
        "La idea es simple: **usar informaci√≥n meteorol√≥gica** (como viento, lluvia, temperatura y humedad) para anticipar cu√°ndo la contaminaci√≥n ser√° alta. Esto es √∫til porque:\n",
        "\n",
        "- Las autoridades pueden **emitir alertas tempranas** a la poblaci√≥n\n",
        "- Las personas con problemas respiratorios pueden **tomar precauciones**\n",
        "- Los gobiernos pueden **implementar medidas preventivas**\n",
        "\n",
        "El proyecto utiliza **Machine Learning** para encontrar patrones entre las condiciones del clima y los niveles de contaminaci√≥n, creando un modelo que puede hacer predicciones autom√°ticas.\n",
        "\n",
        "**En resumen:** Convertir datos del clima en predicciones √∫tiles sobre calidad del aire para proteger la salud p√∫blica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7nsiKREpklJ"
      },
      "source": [
        "# **3. ABSTRACTO Y PREGUNTAS DE INVESTIGACI√ìN** #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ88XVWCpoM3"
      },
      "source": [
        "### **3.1 MOTIVACI√ìN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n--khrI9psoG"
      },
      "source": [
        "La **contaminaci√≥n por PM2.5** es uno de los principales factores de riesgo para la salud respiratoria global. Las part√≠culas finas menores a 2.5 micras penetran profundamente en los pulmones y est√°n asociadas con enfermedades cardiovasculares y respiratorias.\n",
        "\n",
        "La **predicci√≥n temprana** de niveles altos de PM2.5 es crucial para:\n",
        "- **Alertas de salud p√∫blica** a poblaciones vulnerables\n",
        "- **Medidas preventivas** por parte de autoridades\n",
        "- **Decisiones individuales** de protecci√≥n personal\n",
        "- **Pol√≠ticas ambientales** basadas en evidencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO0QhckApvBg"
      },
      "source": [
        "### **3.2 AUDIENCIA OBJETIVO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir5MawxNpybp"
      },
      "source": [
        "- **Autoridades de salud p√∫blica**: Para sistemas de alerta temprana\n",
        "- **Organismos ambientales**: Para monitoreo y pol√≠ticas\n",
        "- **Investigadores**: En calidad del aire y salud ambiental\n",
        "- **Desarrolladores**: De sistemas de monitoreo ambiental"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EyaIQEnp08g"
      },
      "source": [
        "### **3.3 PREGUNTAS DE INVESTIGACI√ìN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy5FdgBqp3l7"
      },
      "source": [
        "**Pregunta Principal 1: Factores Predictivos de PM2.5**\n",
        "\n",
        "¬øQu√© factores (meteorol√≥gicos, geogr√°ficos y socioecon√≥micos) muestran mayor asociaci√≥n con las concentraciones de PM2.5 en un an√°lisis global?\n",
        "\n",
        "**Hip√≥tesis:**\n",
        "\n",
        "En un contexto global, se espera que tanto factores meteorol√≥gicos como socioecon√≥micos contribuyan a explicar las concentraciones de PM2.5. Dado que las relaciones entre contaminaci√≥n atmosf√©rica y sus determinantes pueden ser complejas, se anticipa que:\n",
        "\n",
        "- Las variables meteorol√≥gicas (viento, precipitaci√≥n, humedad, presi√≥n) mostrar√°n relaciones no lineales con PM2.5\n",
        "- Los factores socioecon√≥micos (poblaci√≥n, densidad poblacional) capturar√°n el efecto de emisiones antropog√©nicas\n",
        "- Las transformaciones logar√≠tmicas y variables categ√≥ricas mejorar√°n la detecci√≥n de patrones\n",
        "- Las interacciones entre variables meteorol√≥gicas y socioecon√≥micas revelar√°n efectos sin√©rgicos\n",
        "\n",
        "---\n",
        "\n",
        "**Pregunta Principal 2: Capacidad Predictiva del Modelo**\n",
        "\n",
        "¬øQu√© nivel de precisi√≥n puede alcanzar un modelo multifactorial para predecir PM2.5 utilizando variables meteorol√≥gicas, geogr√°ficas y socioecon√≥micas?\n",
        "\n",
        "**Hip√≥tesis:**\n",
        "\n",
        "- La capacidad predictiva variar√° seg√∫n la complejidad del modelo empleado\n",
        "- La predicci√≥n de PM2.5 a escala global presenta desaf√≠os inherentes debido a la heterogeneidad de fuentes de emisi√≥n y condiciones atmosf√©ricas locales, por lo que se espera una precisi√≥n moderada que refleje esta complejidad.\n",
        "\n",
        "---\n",
        "\n",
        "**Pregunta Principal 3: Patrones Geogr√°ficos y Regionales**\n",
        "\n",
        "¬øExisten diferencias significativas en las relaciones entre variables predictoras y PM2.5 seg√∫n caracter√≠sticas regionales?\n",
        "\n",
        "**Hip√≥tesis:**\n",
        "\n",
        "Se anticipa una variaci√≥n sustancial en los patrones predictivos seg√∫n contexto regional:\n",
        "\n",
        "- **Pa√≠ses con alta poblaci√≥n:** Mayor influencia de factores antropog√©nicos (tr√°fico, industria, densidad urbana)\n",
        "- **Pa√≠ses con baja poblaci√≥n:** Mayor peso relativo de variables meteorol√≥gicas en la dispersi√≥n de contaminantes\n",
        "- **Diferencias clim√°ticas:** Regiones tropicales vs templadas mostrar√°n distintos patrones de acumulaci√≥n y dispersi√≥n\n",
        "- **Nivel de desarrollo:** Pa√≠ses industrializados vs en desarrollo presentar√°n diferentes perfiles de emisi√≥n\n",
        "\n",
        "El an√°lisis estratificado por regi√≥n revelar√° relaciones m√°s fuertes que el an√°lisis global agregado.\n",
        "\n",
        "---\n",
        "\n",
        "**Pregunta Principal 4: An√°lisis de Errores de Predicci√≥n**\n",
        "\n",
        "¬øQu√© patrones presentan los residuos del modelo y qu√© factores no capturados explican los mayores errores de predicci√≥n?\n",
        "\n",
        "**Hip√≥tesis:**\n",
        "\n",
        "Los errores de predicci√≥n m√°s significativos se asociar√°n con:\n",
        "\n",
        "- **Eventos epis√≥dicos:** Incendios forestales, tormentas de arena, actividad volc√°nica\n",
        "- **Fuentes puntuales de emisi√≥n:** Complejos industriales, centrales el√©ctricas no capturadas por variables agregadas\n",
        "- **Condiciones meteorol√≥gicas especiales:** Inversiones t√©rmicas, situaciones de estancamiento atmosf√©rico\n",
        "- **Factores geogr√°ficos locales:** Topograf√≠a compleja (valles, cuencas), proximidad al mar\n",
        "- **Variabilidad estacional:** Per√≠odos de transici√≥n clim√°tica, eventos meteorol√≥gicos extremos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMAekwnUDw9p"
      },
      "source": [
        "# **4. CONFIGURACI√ìN Y CARGA DE DATOS**   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-u2437Ynlnl"
      },
      "source": [
        "### **4.1 CONFIGURACI√ìN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4pILO5KoG-C"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Instala las dependencias necesarias e importa las librer√≠as del proyecto.\n",
        "\n",
        "    Maneja excepciones para que la ejecuci√≥n no falle si una instalaci√≥n o\n",
        "    importaci√≥n es fallida.\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Iniciando la configuraci√≥n del entorno...\")\n",
        "\n",
        "    # Instalar dependencias\n",
        "\n",
        "    print(\"üõ†Ô∏è Instalando dependencias necesarias...\")\n",
        "\n",
        "    # Lista de paquetes a instalar\n",
        "    packages_to_install = [\n",
        "        \"plotly\", \"folium\", \"kaleido\", \"wbdata\", \"requests-cache\",\n",
        "        \"country_converter\", \"gdown\", \"pycountry\", \"tqdm\"\n",
        "    ]\n",
        "\n",
        "    # Comando de pip\n",
        "    pip_command = [sys.executable, \"-m\", \"pip\", \"install\"] + packages_to_install\n",
        "\n",
        "    try:\n",
        "        # Ejecutar la instalaci√≥n\n",
        "        process = subprocess.run(pip_command, check=True, capture_output=True, text=True)\n",
        "        print(\"‚úÖ Instalaci√≥n de dependencias completada con √©xito.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Error durante la instalaci√≥n de dependencias:\")\n",
        "        print(f\"Comando fallido: {e.cmd}\")\n",
        "        print(\"Continuando con la ejecuci√≥n, aunque algunas funcionalidades podr√≠an estar limitadas.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Ocurri√≥ un error inesperado al intentar instalar dependencias: {e}\")\n",
        "        print(\"Continuando con la ejecuci√≥n, aunque algunas funcionalidades podr√≠an estar limitadas.\")\n",
        "\n",
        "    # Importar librer√≠as\n",
        "\n",
        "    print(\"\\nüìö Importando librer√≠as...\")\n",
        "\n",
        "    # Usamos un diccionario para mapear los nombres de las librer√≠as a importar\n",
        "    libraries = {\n",
        "        'pandas': 'pd', 'numpy': 'np', 'matplotlib.pyplot': 'plt',\n",
        "        'seaborn': 'sns', 'plotly.express': 'px', 'plotly.graph_objects': 'go',\n",
        "        'plotly.subplots': 'make_subplots',\n",
        "        'scipy.stats': 'stats',  # ‚Üê CORRECCI√ìN: Importar todo stats, no solo ttest_ind\n",
        "        'json': None, 'time': None, 'datetime': 'datetime', 'requests': None,\n",
        "        'warnings': None, 'gdown': None,\n",
        "        'pycountry': None,\n",
        "        'tqdm': None,\n",
        "        'folium': None,\n",
        "        'folium.plugins': 'HeatMap'\n",
        "    }\n",
        "\n",
        "    successful_imports = 0\n",
        "\n",
        "    for module_name, alias in libraries.items():\n",
        "        try:\n",
        "            # Importa la librer√≠a y la asigna al espacio de nombres global\n",
        "            if alias:\n",
        "                # Manejo especial para importaciones con alias\n",
        "                if module_name == 'plotly.subplots':\n",
        "                    from plotly.subplots import make_subplots\n",
        "                    globals()['make_subplots'] = make_subplots\n",
        "\n",
        "                elif module_name == 'scipy.stats':\n",
        "                    # ‚Üê CORRECCI√ìN PRINCIPAL: Importar m√≥dulo completo + funci√≥n espec√≠fica\n",
        "                    from scipy import stats\n",
        "                    from scipy.stats import ttest_ind\n",
        "                    globals()['stats'] = stats\n",
        "                    globals()['ttest_ind'] = ttest_ind\n",
        "\n",
        "                elif module_name == 'folium.plugins':\n",
        "                    from folium.plugins import HeatMap\n",
        "                    globals()['HeatMap'] = HeatMap\n",
        "\n",
        "                else:\n",
        "                    globals()[alias] = __import__(module_name, fromlist=[alias])\n",
        "            else:\n",
        "                # Para librer√≠as sin alias (como folium, pycountry, tqdm)\n",
        "                globals()[module_name] = __import__(module_name)\n",
        "\n",
        "            successful_imports += 1\n",
        "\n",
        "        except ImportError:\n",
        "            print(f\"‚ö†Ô∏è La librer√≠a '{module_name}' no pudo ser importada. (¬øSe instal√≥ correctamente?)\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Ocurri√≥ un error al importar '{module_name}': {e}\")\n",
        "\n",
        "    # Manejo de la configuraci√≥n de warnings\n",
        "    try:\n",
        "        if 'warnings' in globals():\n",
        "            globals()['warnings'].filterwarnings('ignore')\n",
        "            print(\"Configuraci√≥n de warnings ignorados aplicada.\")\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo configurar la ignorancia de warnings: {e}\")\n",
        "\n",
        "    # Eliminar notaci√≥n cient√≠fica\n",
        "\n",
        "    try:\n",
        "        if 'pd' in globals() and 'np' in globals():\n",
        "            globals()['pd'].set_option('display.float_format', '{:.2f}'.format)\n",
        "            globals()['np'].set_printoptions(suppress=True)\n",
        "            print(\"‚úÖ Configuraci√≥n de notaci√≥n cient√≠fica aplicada.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è No se pudo configurar la notaci√≥n cient√≠fica: {e}\")\n",
        "\n",
        "    print(f\"\\nResumen: {successful_imports} de {len(libraries)} librer√≠as importadas.\")\n",
        "    print(\"‚ú® Configuraci√≥n del entorno finalizada.\")\n",
        "\n",
        "# Ejecuci√≥n de la funci√≥n\n",
        "\n",
        "# Llamar a la funci√≥n para configurar el entorno\n",
        "setup_environment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVCo9C9jqOj6"
      },
      "source": [
        "### **4.2 CARGA DEL DATASET ORIGINAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhZPua6tqaaJ"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "    \"\"\"\n",
        "    Descarga el dataset desde Google Drive y lo carga en un DataFrame de pandas.\n",
        "    Maneja excepciones si falla la descarga o la lectura del archivo.\n",
        "\n",
        "    Retorna:\n",
        "        pandas.DataFrame o None si ocurre un error.\n",
        "    \"\"\"\n",
        "    print(\"üöÄ INICIANDO CARGA DEL DATASET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Par√°metros del dataset\n",
        "    google_drive_id = '1q4cNLxkWeaJ_UmQQy0DWfW9SAT2__JuB'\n",
        "    output_filename = 'GlobalWeatherRepository.csv'\n",
        "\n",
        "    # Asegurarse de que pandas est√© disponible\n",
        "    if 'pd' not in globals():\n",
        "        print(\"‚ùå Error: La librer√≠a 'pandas' (pd) no est√° importada. Ejecuta la funci√≥n de setup primero.\")\n",
        "        return None\n",
        "\n",
        "    # =====================\n",
        "    # Descargar dataset\n",
        "    # =====================\n",
        "    print(f\"üì• Descargando dataset (ID: {google_drive_id})...\")\n",
        "\n",
        "    try:\n",
        "        # Usamos gdown.download() para la descarga\n",
        "        if 'gdown' in globals():\n",
        "            gdown.download(id=google_drive_id, output=output_filename, quiet=False)\n",
        "            print(\"‚úÖ Descarga completada.\")\n",
        "        else:\n",
        "            # Fallback a subprocess o manejar error si gdown no se import√≥\n",
        "            print(\"‚ö†Ô∏è Advertencia: La librer√≠a 'gdown' no est√° disponible. Intentando ejecutar con !gdown (solo funciona en Jupyter/Colab).\")\n",
        "            # Este es un fallback menos robusto, pero respeta la l√≥gica original si gdown.download() falla\n",
        "            import subprocess\n",
        "            subprocess.run(['gdown', google_drive_id], check=True, capture_output=False)\n",
        "            print(\"‚úÖ Descarga completada.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al descargar el archivo: {e}\")\n",
        "        print(\"No se puede continuar sin el archivo descargado.\")\n",
        "        return None\n",
        "\n",
        "    # =====================\n",
        "    # Cargar archivo CSV\n",
        "    # =====================\n",
        "    print(f\"üìÇ Cargando archivo '{output_filename}' en un DataFrame...\")\n",
        "    df = None\n",
        "    try:\n",
        "        df = pd.read_csv(output_filename)\n",
        "        print(\"‚úÖ Carga del DataFrame completada.\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä DATASET LISTO PARA AN√ÅLISIS\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error de carga: El archivo '{output_filename}' no se encontr√≥ despu√©s de la descarga.\")\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"‚ùå Error de carga: El archivo CSV est√° vac√≠o.\")\n",
        "    except pd.errors.ParserError:\n",
        "        print(\"‚ùå Error de carga: Fallo al parsear el archivo CSV. ¬øFormato incorrecto?\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Ocurri√≥ un error inesperado al cargar el CSV: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ====================\n",
        "# Ejecuci√≥n de la funci√≥n\n",
        "# ====================\n",
        "\n",
        "# Ahora puedes llamar a la funci√≥n para obtener tu DataFrame\n",
        "df_original = load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yHEI4NIERbj"
      },
      "source": [
        "# **5. PREPROCESAMIENTO (Data Preparation)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uae10zTiriBt"
      },
      "source": [
        "## **5.1 EXPLORACI√ìN INICIAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vn1Khgiro1r"
      },
      "outputs": [],
      "source": [
        "# Exploraci√≥n inicial para entender los datos\n",
        "print('Estructura del dataset', '\\n', df_original.shape)\n",
        "print('\\n','Primeras columnas del dataset', '\\n', df_original.head(5))\n",
        "print(df_original.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIVxKnH5v-A5"
      },
      "outputs": [],
      "source": [
        "# Exploraci√≥n columna Country\n",
        "print(df_original[\"country\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_R-XelR-44U"
      },
      "source": [
        "**Detectamos nombres de paises con caracteres especiales u en otro idioma, requiere normalizaci√≥n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaeM8BXswuSJ"
      },
      "outputs": [],
      "source": [
        "# Variables num√©ricas\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "print(\"ESTAD√çSTICAS NUM√âRICAS:\")\n",
        "print(df_original.describe().T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw5eFNmVw7E5"
      },
      "outputs": [],
      "source": [
        "# Variables categ√≥ricas\n",
        "print(\"\\nESTAD√çSTICAS CATEG√ìRICAS:\")\n",
        "print(df_original.describe(include=['object']).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42roO_2Qw9NT"
      },
      "outputs": [],
      "source": [
        "# Variable target espec√≠fica\n",
        "print(\"\\nTARGET PM2.5:\")\n",
        "print(df_original['air_quality_PM2.5'].describe().T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggVSdhIzxBzZ"
      },
      "source": [
        "## **5.2 MANIPULACI√ìN Y LIMPIEZA DE LOS DATOS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2pYp5BuxGNH"
      },
      "source": [
        "### **5.2.1 COLUMNA COUNTRY**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46-AVaCnyN5H"
      },
      "source": [
        "Antes de realizar cualquier an√°lisis, es fundamental garantizar que los datos sean consistentes. En este paso se aplican transformaciones iniciales para preparar el dataset:\n",
        "\n",
        "- Eliminaci√≥n de espacios invisibles: Se utiliza .str.strip() para remover posibles espacios en blanco al inicio o final de los nombres de pa√≠ses.\n",
        "\n",
        "- Estandarizaci√≥n de nombres: Se aplica un diccionario de traducci√≥n que unifica todos los nombres de pa√≠ses al espa√±ol, corrigiendo variantes en otros idiomas, caracteres √°rabes, cir√≠licos o duplicados.\n",
        "\n",
        "- Normalizaci√≥n de claves: Esto asegura que las agregaciones posteriores (como promedios o conteo de registros por pa√≠s) no se vean afectadas por inconsistencias en los nombres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhN6omuMySuY"
      },
      "outputs": [],
      "source": [
        "# Limpieza y correcci√≥n de la columna 'country'\n",
        "\n",
        "# Eliminar espacios en blanco al inicio y final de cada valor en la columna 'country'\n",
        "# Esto es importante porque los datos pueden tener espacios extra que impidan\n",
        "# las coincidencias exactas al hacer an√°lisis o filtros\n",
        "df_original[\"country\"] = df_original[\"country\"].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXeKgX7NyVby"
      },
      "outputs": [],
      "source": [
        "# Creamos diccionario de correcciones a la columna country\n",
        "correcciones = {\n",
        "    \"Afghanistan\": \"Afghanistan\",\n",
        "    \"Albania\": \"Albania\",\n",
        "    \"Algeria\": \"Algeria\",\n",
        "    \"Andorra\": \"Andorra\",\n",
        "    \"Angola\": \"Angola\",\n",
        "    \"Antigua and Barbuda\": \"Antigua and Barbuda\",\n",
        "    \"Argentina\": \"Argentina\",\n",
        "    \"Armenia\": \"Armenia\",\n",
        "    \"Australia\": \"Australia\",\n",
        "    \"Austria\": \"Austria\",\n",
        "    \"Azerbaijan\": \"Azerbaijan\",\n",
        "    \"Bahamas\": \"Bahamas\",\n",
        "    \"Bahrain\": \"Bahrain\",\n",
        "    \"Bangladesh\": \"Bangladesh\",\n",
        "    \"Barbados\": \"Barbados\",\n",
        "    \"Belarus\": \"Belarus\",\n",
        "    \"Belgium\": \"Belgium\",\n",
        "    \"Belize\": \"Belize\",\n",
        "    \"Benin\": \"Benin\",\n",
        "    \"Bhutan\": \"Bhutan\",\n",
        "    \"Bolivia\": \"Bolivia\",\n",
        "    \"Bosnia and Herzegovina\": \"Bosnia and Herzegovina\",\n",
        "    \"Botswana\": \"Botswana\",\n",
        "    \"Brazil\": \"Brazil\",\n",
        "    \"Brunei Darussalam\": \"Brunei\",\n",
        "    \"Bulgaria\": \"Bulgaria\",\n",
        "    \"Burkina Faso\": \"Burkina Faso\",\n",
        "    \"Burundi\": \"Burundi\",\n",
        "    \"Madagascar\": \"Madagascar\",\n",
        "    \"Cape Verde\": \"Cape Verde\",\n",
        "    \"Cambodia\": \"Cambodia\",\n",
        "    \"Cameroon\": \"Cameroon\",\n",
        "    \"Canada\": \"Canada\",\n",
        "    \"Central African Republic\": \"Central African Republic\",\n",
        "    \"Chad\": \"Chad\",\n",
        "    \"Chile\": \"Chile\",\n",
        "    \"China\": \"China\",\n",
        "    \"Comoros\": \"Comoros\",\n",
        "    \"Congo\": \"Congo\",\n",
        "    \"Costa Rica\": \"Costa Rica\",\n",
        "    \"Croatia\": \"Croatia\",\n",
        "    \"Cuba\": \"Cuba\",\n",
        "    \"Cyprus\": \"Cyprus\",\n",
        "    \"Czech Republic\": \"Czechia\",\n",
        "    \"Democratic Republic of Congo\": \"Congo, The Democratic Republic of the\",\n",
        "    \"Denmark\": \"Denmark\",\n",
        "    \"Djibouti\": \"Djibouti\",\n",
        "    \"Dominica\": \"Dominica\",\n",
        "    \"Dominican Republic\": \"Dominican Republic\",\n",
        "    \"Ecuador\": \"Ecuador\",\n",
        "    \"Egypt\": \"Egypt\",\n",
        "    \"El Salvador\": \"El Salvador\",\n",
        "    \"Equatorial Guinea\": \"Equatorial Guinea\",\n",
        "    \"Eritrea\": \"Eritrea\",\n",
        "    \"Estonia\": \"Estonia\",\n",
        "    \"Swaziland\": \"Eswatini\",\n",
        "    \"Ethiopia\": \"Ethiopia\",\n",
        "    \"Fiji Islands\": \"Fiji\",\n",
        "    \"Finland\": \"Finland\",\n",
        "    \"France\": \"France\",\n",
        "    \"Gabon\": \"Gabon\",\n",
        "    \"Gambia\": \"Gambia\",\n",
        "    \"Georgia\": \"Georgia\",\n",
        "    \"Germany\": \"Germany\",\n",
        "    \"Ghana\": \"Ghana\",\n",
        "    \"Greece\": \"Greece\",\n",
        "    \"Grenada\": \"Grenada\",\n",
        "    \"Guatemala\": \"Guatemala\",\n",
        "    \"Guinea\": \"Guinea\",\n",
        "    \"Guinea-Bissau\": \"Guinea-Bissau\",\n",
        "    \"Guyana\": \"Guyana\",\n",
        "    \"Haiti\": \"Haiti\",\n",
        "    \"Vatican City\": \"Vatican City\",\n",
        "    \"Honduras\": \"Honduras\",\n",
        "    \"Hungary\": \"Hungary\",\n",
        "    \"Iceland\": \"Iceland\",\n",
        "    \"India\": \"India\",\n",
        "    \"Indonesia\": \"Indonesia\",\n",
        "    \"Iran\": \"Iran\",\n",
        "    \"Iraq\": \"Iraq\",\n",
        "    \"Ireland\": \"Ireland\",\n",
        "    \"Israel\": \"Israel\",\n",
        "    \"Italy\": \"Italy\",\n",
        "    \"Jamaica\": \"Jamaica\",\n",
        "    \"Japan\": \"Japan\",\n",
        "    \"Jordan\": \"Jordan\",\n",
        "    \"Kazakhstan\": \"Kazakhstan\",\n",
        "    \"Kenya\": \"Kenya\",\n",
        "    \"Kiribati\": \"Kiribati\",\n",
        "    \"Kuwait\": \"Kuwait\",\n",
        "    \"Kyrghyzstan\": \"Kyrgyzstan\",\n",
        "    \"Latvia\": \"Latvia\",\n",
        "    \"Lebanon\": \"Lebanon\",\n",
        "    \"Lesotho\": \"Lesotho\",\n",
        "    \"Liberia\": \"Liberia\",\n",
        "    \"Thailand\": \"Thailand\",\n",
        "    \"Liechtenstein\": \"Liechtenstein\",\n",
        "    \"Lithuania\": \"Lithuania\",\n",
        "    \"Luxembourg\": \"Luxembourg\",\n",
        "    \"Malawi\": \"Malawi\",\n",
        "    \"Malaysia\": \"Malaysia\",\n",
        "    \"Maldives\": \"Maldives\",\n",
        "    \"Mali\": \"Mali\",\n",
        "    \"Malta\": \"Malta\",\n",
        "    \"Marshall Islands\": \"Marshall Islands\",\n",
        "    \"Mauritania\": \"Mauritania\",\n",
        "    \"Mauritius\": \"Mauritius\",\n",
        "    \"Mexico\": \"Mexico\",\n",
        "    \"Micronesia\": \"Micronesia\",\n",
        "    \"Monaco\": \"Monaco\",\n",
        "    \"Mongolia\": \"Mongolia\",\n",
        "    \"Montenegro\": \"Montenegro\",\n",
        "    \"Morocco\": \"Morocco\",\n",
        "    \"Mozambique\": \"Mozambique\",\n",
        "    \"Myanmar\": \"Myanmar\",\n",
        "    \"Namibia\": \"Namibia\",\n",
        "    \"Turkey\": \"Turkey\",\n",
        "    \"Nepal\": \"Nepal\",\n",
        "    \"Netherlands\": \"Netherlands\",\n",
        "    \"New Zealand\": \"New Zealand\",\n",
        "    \"Nicaragua\": \"Nicaragua\",\n",
        "    \"Niger\": \"Niger\",\n",
        "    \"Nigeria\": \"Nigeria\",\n",
        "    \"North Korea\": \"North Korea\",\n",
        "    \"Macedonia\": \"North Macedonia\",\n",
        "    \"Norway\": \"Norway\",\n",
        "    \"Oman\": \"Oman\",\n",
        "    \"Pakistan\": \"Pakistan\",\n",
        "    \"Palau\": \"Palau\",\n",
        "    \"Panama\": \"Panama\",\n",
        "    \"Papua New Guinea\": \"Papua New Guinea\",\n",
        "    \"Paraguay\": \"Paraguay\",\n",
        "    \"Peru\": \"Peru\",\n",
        "    \"Philippines\": \"Philippines\",\n",
        "    \"Poland\": \"Poland\",\n",
        "    \"Portugal\": \"Portugal\",\n",
        "    \"Qatar\": \"Qatar\",\n",
        "    \"Romania\": \"Romania\",\n",
        "    \"Russia\": \"Russia\",\n",
        "    \"Rwanda\": \"Rwanda\",\n",
        "    \"Saint Kitts and Nevis\": \"Saint Kitts and Nevis\",\n",
        "    \"Saint Lucia\": \"Saint Lucia\",\n",
        "    \"Saint Vincent and the Grenadines\": \"Saint Vincent and the Grenadines\",\n",
        "    \"Samoa\": \"Samoa\",\n",
        "    \"San Marino\": \"San Marino\",\n",
        "    \"Saudi Arabia\": \"Saudi Arabia\",\n",
        "    \"Senegal\": \"Senegal\",\n",
        "    \"Serbia\": \"Serbia\",\n",
        "    \"Seychelles Islands\": \"Seychelles\",\n",
        "    \"Sierra Leone\": \"Sierra Leone\",\n",
        "    \"Singapore\": \"Singapore\",\n",
        "    \"Slovakia\": \"Slovakia\",\n",
        "    \"Slovenia\": \"Slovenia\",\n",
        "    \"Solomon Islands\": \"Solomon Islands\",\n",
        "    \"Somalia\": \"Somalia\",\n",
        "    \"South Africa\": \"South Africa\",\n",
        "    \"South Korea\": \"South Korea\",\n",
        "    \"Sudan\": \"Sudan\",\n",
        "    \"Spain\": \"Spain\",\n",
        "    \"Sri Lanka\": \"Sri Lanka\",\n",
        "    \"Suriname\": \"Suriname\",\n",
        "    \"Sweden\": \"Sweden\",\n",
        "    \"Switzerland\": \"Switzerland\",\n",
        "    \"Syria\": \"Syria\",\n",
        "    \"Tajikistan\": \"Tajikistan\",\n",
        "    \"Tanzania\": \"Tanzania\",\n",
        "    \"Timor-Leste\": \"Timor-Leste\",\n",
        "    \"Tonga\": \"Tonga\",\n",
        "    \"Trinidad and Tobago\": \"Trinidad and Tobago\",\n",
        "    \"Tunisia\": \"Tunisia\",\n",
        "    \"Turkmenistan\": \"Turkmenistan\",\n",
        "    \"Tuvalu\": \"Tuvalu\",\n",
        "    \"Uganda\": \"Uganda\",\n",
        "    \"Ukraine\": \"Ukraine\",\n",
        "    \"United Arab Emirates\": \"United Arab Emirates\",\n",
        "    \"United Kingdom\": \"United Kingdom\",\n",
        "    \"United States of America\": \"United States\",\n",
        "    \"Uruguay\": \"Uruguay\",\n",
        "    \"Uzbekistan\": \"Uzbekistan\",\n",
        "    \"Vanuatu\": \"Vanuatu\",\n",
        "    \"Venezuela\": \"Venezuela\",\n",
        "    \"Vietnam\": \"Vietnam\",\n",
        "    \"Yemen\": \"Yemen\",\n",
        "    \"Zambia\": \"Zambia\",\n",
        "    \"Zimbabwe\": \"Zimbabwe\",\n",
        "    \"Colombia\": \"Colombia\",\n",
        "    \"USA United States of America\": \"United States\",\n",
        "    \"Lao People's Democratic Republic\": \"Laos\",\n",
        "    \"Libya\": \"Libya\",\n",
        "    \"Kosovo\": \"Kosovo\",\n",
        "    \"Togo\": \"Togo\",\n",
        "    \"Mal√°sia\": \"Malaysia\",\n",
        "    \"Komoren\": \"Comoros\",\n",
        "    \"ŸÉŸàŸÑŸàŸÖÿ®Ÿäÿß\": \"Colombia\",\n",
        "    \"Estonie\": \"Estonia\",\n",
        "    \"Inde\": \"India\",\n",
        "    \"Letonia\": \"Latvia\",\n",
        "    \"–ü–æ–ª—å—à–∞\": \"Poland\",\n",
        "    \"Mexique\": \"Mexico\",\n",
        "    \"Pol√¥nia\": \"Poland\",\n",
        "    \"Marrocos\": \"Morocco\",\n",
        "    \"Saint-Vincent-et-les-Grenadines\": \"Saint Vincent and the Grenadines\",\n",
        "    \"Saudi Arabien\": \"Saudi Arabia\",\n",
        "    \"S√ºdkorea\": \"South Korea\",\n",
        "    \"B√©lgica\": \"Belgium\",\n",
        "    \"Turkm√©nistan\": \"Turkmenistan\",\n",
        "    \"–¢—É—Ä—Ü–∏—è\": \"Turkey\",\n",
        "    \"ÁÅ´È∏°\": \"Turkey\",\n",
        "    \"Jemen\": \"Yemen\",\n",
        "    \"–ì–≤–∞—Ç–µ–º–∞–ª–∞\": \"Guatemala\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heKqTp95ykhw"
      },
      "outputs": [],
      "source": [
        "# Aplicar correcciones predefinidas a los nombres de pa√≠ses\n",
        "# El diccionario 'correcciones' mapea nombres incorrectos/inconsistentes a la versi√≥n correcta\n",
        "df_original[\"country\"] = df_original[\"country\"].replace(correcciones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5uB8ZIUyoVM"
      },
      "outputs": [],
      "source": [
        "# Mostrando la columna country con las correciones\n",
        "print(df_original[\"country\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCwQ7xJjE-_V"
      },
      "source": [
        "### **5.2.2 CREACI√ìN DE COLUMNAS TEMPORALES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j90gUTOizS9u"
      },
      "outputs": [],
      "source": [
        "# Crear columnas temporales\n",
        "\n",
        "# 1. Conversi√≥n a datetime\n",
        "df_original['last_updated'] = pd.to_datetime(\n",
        "    df_original['last_updated'],\n",
        "    errors='coerce'\n",
        ")\n",
        "\n",
        "# 2. Extracci√≥n del mes y la hora\n",
        "# dt.month y dt.hour autom√°ticamente manejar√°n los valores NaT como NaN\n",
        "df_original['month'] = df_original['last_updated'].dt.month\n",
        "df_original['hour'] = df_original['last_updated'].dt.hour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgxVVA4zFPsU"
      },
      "source": [
        "### **5.2.3 ELIMINACI√ìN DE COLUMNAS IRRELEVANTES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsjdLcmgziUO"
      },
      "outputs": [],
      "source": [
        "# ELIMINAR COLUMNAS IRRELEVANTES PARA PREDICCI√ìN DE PM2.5\n",
        "print(\"üìã Eliminando columnas irrelevantes...\")\n",
        "\n",
        "columnas_eliminar = [\n",
        "    # Redundantes\n",
        "    'location_name', 'temperature_fahrenheit', 'wind_mph', 'pressure_in',\n",
        "    'precip_in', 'gust_mph', 'visibility_miles', 'last_updated_epoch',\n",
        "\n",
        "    # Variables derivadas\n",
        "    'feels_like_celsius', 'feels_like_fahrenheit',\n",
        "\n",
        "    # No predictivas para PM2.5\n",
        "    'timezone', 'condition_text', 'visibility_km',\n",
        "\n",
        "    # Variables astron√≥micas\n",
        "    'sunrise', 'sunset', 'moonrise', 'moonset', 'moon_phase', 'moon_illumination',\n",
        "\n",
        "    # √çndices que incluyen PM2.5 en su c√°lculo\n",
        "    'air_quality_us-epa-index', 'air_quality_gb-defra-index',\n",
        "\n",
        "    # Variables de contaminaci√≥n (TARGET LEAKAGE)\n",
        "    'air_quality_Carbon_Monoxide',\n",
        "    'air_quality_Ozone',\n",
        "    'air_quality_Nitrogen_dioxide',\n",
        "    'air_quality_Sulphur_dioxide',\n",
        "    'air_quality_PM10',\n",
        "    'gust_kph'  # Correlaci√≥n 0.951 con wind_kph\n",
        "]\n",
        "\n",
        "# Verificar existencia y eliminar\n",
        "columnas_existentes = [col for col in columnas_eliminar if col in df_original.columns]\n",
        "df_clean = df_original.drop(columns=columnas_existentes)\n",
        "\n",
        "print(f\"Dataset original: {df_original.shape}\")\n",
        "print(f\"Dataset limpio: {df_clean.shape}\")\n",
        "print(f\"Columnas eliminadas: {len(columnas_existentes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTnqZcC1z167"
      },
      "outputs": [],
      "source": [
        "# Columnas finales\n",
        "print(f\"\\nüìã COLUMNAS FINALES ({df_clean.shape[1]}):\")\n",
        "for i, col in enumerate(df_clean.columns, 1):\n",
        "    tipo = df_clean[col].dtype\n",
        "    print(f\"{i:2d}. {col:<30} ({tipo})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuuyS7Y2FiIJ"
      },
      "source": [
        "### **5.2.4 OPTIMIZACI√ìN DE TIPOS DE DATOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keWXwknY0KH2"
      },
      "outputs": [],
      "source": [
        "# Optimizando tipos de datos\n",
        "print(\"üîß Optimizando tipos de datos en df_clean...\")\n",
        "\n",
        "# Variables categ√≥ricas\n",
        "df_clean['country'] = df_clean['country'].astype('category')\n",
        "df_clean['wind_direction'] = df_clean['wind_direction'].astype('category')\n",
        "\n",
        "# Variables enteras optimizadas\n",
        "df_clean['wind_degree'] = df_clean['wind_degree'].astype('int16')  # 0-360\n",
        "df_clean['humidity'] = df_clean['humidity'].astype('int8')         # 0-100\n",
        "df_clean['cloud'] = df_clean['cloud'].astype('int8')              # 0-100\n",
        "\n",
        "# Fecha a datetime\n",
        "df_clean['last_updated'] = pd.to_datetime(df_clean['last_updated'])\n",
        "\n",
        "# Verificar cambios\n",
        "print(\"‚úÖ Optimizaciones aplicadas\")\n",
        "print(\"Nuevos tipos de datos:\")\n",
        "print(df_clean.dtypes)\n",
        "\n",
        "columnas_float = [\n",
        "    'latitude',\n",
        "    'longitude',\n",
        "    'temperature_celsius',\n",
        "    'wind_kph',\n",
        "    'pressure_mb',\n",
        "    'precip_mm',\n",
        "    'uv_index',\n",
        "    'air_quality_PM2.5'\n",
        "]\n",
        "\n",
        "# Convertir cada columna\n",
        "for col in columnas_float:\n",
        "    df_clean[col] = df_clean[col].astype('float32')\n",
        "    print(f\"‚úÖ {col:30s} ‚Üí float32\")\n",
        "\n",
        "# Verificar ahorro de memoria\n",
        "print(f\"\\nMemoria utilizada: {df_clean.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HvcYT6uCBfo"
      },
      "source": [
        "### **5.2.5 CREACI√ìN DE VARIABLES DERIVADAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVTtfhrfCG2H"
      },
      "outputs": [],
      "source": [
        "# CREACI√ìN DE VARIABLES BINARIAS PARA EDA\n",
        "\n",
        "print(\"üõ†Ô∏è Creando variables binarias derivadas en df_clean...\")\n",
        "\n",
        "# Crear variable binaria 'rained' (1 si hay precipitaci√≥n, 0 si no)\n",
        "\n",
        "df_clean['rained'] = (df_clean['precip_mm'] > 0).astype(int)\n",
        "\n",
        "# Crear variable binaria 'high_wind' (1 si el viento supera los 20 kph, 0 si no)\n",
        "\n",
        "df_clean['high_wind'] = (df_clean['wind_kph'] > 20).astype(int)\n",
        "\n",
        "print(f\"‚úÖ Columnas 'rained' y 'high_wind' a√±adidas a df_clean. Ahora contiene {len(df_clean.columns)} columnas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3iwPoOK0aM7"
      },
      "source": [
        "## **5.3 ENRIQUECIMIENTO CON APIS P√öBLICAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm8z8Myg7Sav"
      },
      "outputs": [],
      "source": [
        "# ENRIQUECIMIENTO DE DATOS - API RESTCOUNTRIES\n",
        "\n",
        "print(\"üåç ENRIQUECIENDO DATOS CON INFORMACI√ìN DE POBLACI√ìN...\")\n",
        "\n",
        "# Instalar e importar librer√≠as necesarias\n",
        "!pip install -q pycountry\n",
        "\n",
        "import pycountry\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Funci√≥n para obtener c√≥digo ISO\n",
        "\n",
        "def obtener_codigo_pais(nombre_es):\n",
        "    \"\"\"\n",
        "    Obtiene el c√≥digo ISO de 3 letras de un pa√≠s.\n",
        "    Usa b√∫squeda fuzzy de pycountry con casos especiales manuales.\n",
        "    \"\"\"\n",
        "    # Casos especiales que search_fuzzy no encuentra bien\n",
        "    codigos_manuales = {\n",
        "        'Cape Verde': 'CPV',\n",
        "        'Turkey': 'TUR'\n",
        "    }\n",
        "\n",
        "    if nombre_es in codigos_manuales:\n",
        "        return codigos_manuales[nombre_es]\n",
        "\n",
        "    try:\n",
        "        country = pycountry.countries.search_fuzzy(nombre_es)[0]\n",
        "        return country.alpha_3\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Funci√≥n API\n",
        "\n",
        "def get_country_data_mejorada(country_name_es):\n",
        "    \"\"\"\n",
        "    Obtiene poblaci√≥n y densidad poblacional de un pa√≠s mediante API RestCountries.\n",
        "    Retorna: (poblaci√≥n, densidad) o (None, None) si falla.\n",
        "    \"\"\"\n",
        "    codigo = obtener_codigo_pais(country_name_es)\n",
        "\n",
        "    if not codigo:\n",
        "        return None, None\n",
        "\n",
        "    url = f\"https://restcountries.com/v3.1/alpha/{codigo}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()[0]\n",
        "            population = data.get('population', None)\n",
        "            area = data.get('area', None)\n",
        "\n",
        "            # Calcular densidad (manejar divisi√≥n por cero)\n",
        "            if population and area and area > 0:\n",
        "                density = population / area\n",
        "            else:\n",
        "                density = None\n",
        "\n",
        "            return population, density\n",
        "        else:\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        # Opcional: descomentar para debug\n",
        "        # print(f\"Error con {country_name_es}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Llamar API para todos los pa√≠ses\n",
        "\n",
        "print(\"\\nüì° Consultando API RestCountries...\")\n",
        "\n",
        "paises_unicos = df_clean['country'].unique()\n",
        "datos_paises = {}\n",
        "errores = []\n",
        "\n",
        "for pais in tqdm(paises_unicos, desc=\"Consultando pa√≠ses\"):\n",
        "    pop, dens = get_country_data_mejorada(pais)\n",
        "\n",
        "    if pop is not None:  # Si encontr√≥ datos\n",
        "        datos_paises[pais] = {\n",
        "            'population': pop,\n",
        "            'population_density': dens\n",
        "        }\n",
        "    else:  # Si fall√≥\n",
        "        datos_paises[pais] = {\n",
        "            'population': None,\n",
        "            'population_density': None\n",
        "        }\n",
        "        errores.append(pais)\n",
        "\n",
        "    # Pausa para no saturar la API\n",
        "    time.sleep(0.1)\n",
        "\n",
        "# Crear nuevo dataframe enriquecido\n",
        "\n",
        "print(\"\\nüìä Creando dataframe enriquecido...\")\n",
        "\n",
        "# Crear copia del dataframe limpio\n",
        "df_enriched = df_clean.copy()\n",
        "\n",
        "# Agregar columnas nuevas\n",
        "df_enriched['population'] = df_enriched['country'].map(\n",
        "    lambda x: datos_paises.get(x, {}).get('population')\n",
        ")\n",
        "df_enriched['population_density'] = df_enriched['country'].map(\n",
        "    lambda x: datos_paises.get(x, {}).get('population_density')\n",
        ")\n",
        "\n",
        "# Verificar resultados\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTADOS DEL ENRIQUECIMIENTO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"üìä Dataset limpio (df_clean): {df_clean.shape}\")\n",
        "print(f\"üìä Dataset enriquecido (df_enriched): {df_enriched.shape}\")\n",
        "print(f\"   ‚îî‚îÄ Columnas agregadas: population, population_density\")\n",
        "\n",
        "print(f\"\\n‚úÖ Pa√≠ses con datos completos: {df_enriched['population'].notna().sum()}\")\n",
        "print(f\"‚ùå Pa√≠ses sin datos: {len(errores)}\")\n",
        "print(f\"üìä Total de pa√≠ses √∫nicos: {len(paises_unicos)}\")\n",
        "\n",
        "if len(errores) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  Pa√≠ses que fallaron ({len(errores)}):\")\n",
        "    for pais in errores[:10]:  # Mostrar solo primeros 10\n",
        "        print(f\"   ‚Ä¢ {pais}\")\n",
        "    if len(errores) > 10:\n",
        "        print(f\"   ... y {len(errores)-10} m√°s\")\n",
        "else:\n",
        "    print(\"\\nüéâ Todos los pa√≠ses tienen datos!\")\n",
        "\n",
        "# Verificar NaN finales\n",
        "nulos_population = df_enriched['population'].isna().sum()\n",
        "nulos_density = df_enriched['population_density'].isna().sum()\n",
        "\n",
        "print(f\"\\nNaN en 'population': {nulos_population}\")\n",
        "print(f\"NaN en 'population_density': {nulos_density}\")\n",
        "\n",
        "# Mostrar ejemplos de datos obtenidos\n",
        "print(\"\\nüìã Ejemplos de datos obtenidos:\")\n",
        "print(df_enriched[['country', 'population', 'population_density']].drop_duplicates().head(10))\n",
        "\n",
        "# Comparar columnas\n",
        "print(f\"\\nüìã Columnas en df_clean: {df_clean.shape[1]}\")\n",
        "print(f\"üìã Columnas en df_enriched: {df_enriched.shape[1]} (+2 nuevas)\")\n",
        "\n",
        "print(\"\\n‚úÖ ENRIQUECIMIENTO COMPLETADO\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbjqWPWlGhtF"
      },
      "source": [
        "# **6. EDA (EXPLORATORY DATA ANALYSIS)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNq5jAzvBk2J"
      },
      "source": [
        "**MARCO CONCEPTUAL DEL EDA**\n",
        "\n",
        "Este an√°lisis exploratorio tiene tres objetivos principales:\n",
        "\n",
        "1. **Validaci√≥n de Supuestos**: Verificar normalidad, linealidad y homocedasticidad\n",
        "2. **Detecci√≥n de Patrones**: Identificar relaciones entre variables y PM2.5\n",
        "3. **Fundamentaci√≥n del Feature Engineering**: Justificar transformaciones necesarias\n",
        "\n",
        "**Metodolog√≠a aplicada:**\n",
        "- An√°lisis univariado (distribuciones, outliers)\n",
        "- An√°lisis bivariado (correlaciones, efectos categ√≥ricos)\n",
        "- An√°lisis multivariado (patrones temporales y geogr√°ficos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU_0pR54ANtG"
      },
      "source": [
        "### **6.1 AN√ÅLISIS DE OUTLIERS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O54Rnke-AUKr"
      },
      "source": [
        "**Justificaci√≥n del Tratamiento de Outliers**\n",
        "\n",
        "En este an√°lisis se consideraron dos enfoques distintos para la detecci√≥n de valores at√≠picos (outliers): el uso de **umbrales fijos** y el m√©todo estad√≠stico del **rango intercuart√≠lico (IQR)**. Ambos se representan mediante *boxplots* para comparar c√≥mo cada criterio identifica los outliers.\n",
        "\n",
        "**Boxplots con Umbrales Fijos (Criterio F√≠sico)**\n",
        "\n",
        "Se establecieron umbrales manuales basados en criterios f√≠sicos y realistas:\n",
        "\n",
        "* **Velocidad del viento:** 150 kph, correspondiente a un hurac√°n de categor√≠a 1 en la escala Saffir-Simpson.\n",
        "* **PM2.5:** 500 $\\mu g/m^3$, considerado un nivel extremadamente peligroso para la salud seg√∫n la OMS.\n",
        "* **Presi√≥n atmosf√©rica:** 900-1100 mb, rango que incluye fen√≥menos meteorol√≥gicos extremos pero f√≠sicamente posibles.\n",
        "\n",
        "Este enfoque no depende de la dispersi√≥n estad√≠stica, sino que se fundamenta en **condiciones l√≠mite del mundo real**, de modo que se eliminan √∫nicamente valores que son muy poco probables o f√≠sicamente irreales.\n",
        "\n",
        "**Boxplots con IQR (Criterio Estad√≠stico)**\n",
        "\n",
        "Su principal ventaja es que **se adapta a la distribuci√≥n de los datos** sin necesidad de supuestos externos. Sin embargo, en *datasets* globales como este, tiende a ser demasiado restrictivo, etiquetando como *outliers* valores que s√≠ pueden ocurrir en contextos espec√≠ficos (ej. tormentas intensas o ciudades con alta contaminaci√≥n).\n",
        "\n",
        "**Elecci√≥n del Criterio**\n",
        "\n",
        "Aunque se muestran ambos enfoques para fines comparativos, se opt√≥ por trabajar con los **umbrales fijos**. Este criterio permite:\n",
        "\n",
        "1.  **Mantener la coherencia** con fen√≥menos f√≠sicos y l√≠mites reconocidos por organismos internacionales.\n",
        "2.  **Evitar la eliminaci√≥n** de datos v√°lidos que el IQR podr√≠a descartar de forma excesiva.\n",
        "3.  **Asegurar** que los modelos se entrenen con informaci√≥n representativa de situaciones reales, sin sesgarse por una definici√≥n puramente estad√≠stica de *outlier*.\n",
        "\n",
        "En conclusi√≥n, los boxplots con IQR se incluyen como **referencia estad√≠stica**, pero el an√°lisis final se basa en los **umbrales fijos**, ya que reflejan mejor la realidad f√≠sica y garantizan resultados m√°s interpretables en un contexto global."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKZfziv7AsNJ"
      },
      "outputs": [],
      "source": [
        "# DETECCI√ìN DE OUTLIERS CON IQR (REFERENCIA ESTAD√çSTICA)\n",
        "\n",
        "# Usamos df_enriched\n",
        "df = df_enriched.copy()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DETECCI√ìN DE OUTLIERS - M√âTODO IQR\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. VELOCIDAD DEL VIENTO\n",
        "\n",
        "Q1_wind = df['wind_kph'].quantile(0.25)\n",
        "Q3_wind = df['wind_kph'].quantile(0.75)\n",
        "IQR_wind = Q3_wind - Q1_wind\n",
        "lim_inf_wind = Q1_wind - 1.5 * IQR_wind\n",
        "lim_sup_wind = Q3_wind + 1.5 * IQR_wind\n",
        "\n",
        "print(\"\\n1. VELOCIDAD DEL VIENTO\")\n",
        "print(f\"   IQR: {IQR_wind:.2f}\")\n",
        "print(f\"   L√≠mite inferior: {lim_inf_wind:.2f} kph\")\n",
        "print(f\"   L√≠mite superior: {lim_sup_wind:.2f} kph\")\n",
        "\n",
        "outliers_wind = df[(df['wind_kph'] < lim_inf_wind) | (df['wind_kph'] > lim_sup_wind)]\n",
        "print(f\"   Cantidad de outliers: {len(outliers_wind)} ({len(outliers_wind)/len(df)*100:.2f}%)\")\n",
        "\n",
        "# 2. PM2.5\n",
        "\n",
        "Q1_pm25 = df['air_quality_PM2.5'].quantile(0.25)\n",
        "Q3_pm25 = df['air_quality_PM2.5'].quantile(0.75)\n",
        "IQR_pm25 = Q3_pm25 - Q1_pm25\n",
        "lim_inf_pm25 = Q1_pm25 - 1.5 * IQR_pm25\n",
        "lim_sup_pm25 = Q3_pm25 + 1.5 * IQR_pm25\n",
        "\n",
        "print(\"\\n2. PM2.5\")\n",
        "print(f\"   IQR: {IQR_pm25:.2f}\")\n",
        "print(f\"   L√≠mite inferior: {lim_inf_pm25:.2f} ¬µg/m¬≥\")\n",
        "print(f\"   L√≠mite superior: {lim_sup_pm25:.2f} ¬µg/m¬≥\")\n",
        "\n",
        "outliers_pm25 = df[(df['air_quality_PM2.5'] < lim_inf_pm25) | (df['air_quality_PM2.5'] > lim_sup_pm25)]\n",
        "print(f\"   Cantidad de outliers: {len(outliers_pm25)} ({len(outliers_pm25)/len(df)*100:.2f}%)\")\n",
        "\n",
        "# 3. PRESI√ìN ATMOSF√âRICA\n",
        "\n",
        "Q1_pressure = df['pressure_mb'].quantile(0.25)\n",
        "Q3_pressure = df['pressure_mb'].quantile(0.75)\n",
        "IQR_pressure = Q3_pressure - Q1_pressure\n",
        "lim_inf_pressure = Q1_pressure - 1.5 * IQR_pressure\n",
        "lim_sup_pressure = Q3_pressure + 1.5 * IQR_pressure\n",
        "\n",
        "print(\"\\n3. PRESI√ìN ATMOSF√âRICA\")\n",
        "print(f\"   IQR: {IQR_pressure:.2f}\")\n",
        "print(f\"   L√≠mite inferior: {lim_inf_pressure:.2f} mb\")\n",
        "print(f\"   L√≠mite superior: {lim_sup_pressure:.2f} mb\")\n",
        "\n",
        "outliers_pressure = df[(df['pressure_mb'] < lim_inf_pressure) | (df['pressure_mb'] > lim_sup_pressure)]\n",
        "print(f\"   Cantidad de outliers: {len(outliers_pressure)} ({len(outliers_pressure)/len(df)*100:.2f}%)\")\n",
        "\n",
        "# Mostrar ejemplos de outliers extremos en presi√≥n\n",
        "if len(outliers_pressure) > 0:\n",
        "    print(\"\\n   Ejemplos de valores an√≥malos en presi√≥n:\")\n",
        "    # Usamos .sort_values para ver los m√°s extremos\n",
        "    top_outliers = outliers_pressure[['country', 'pressure_mb', 'last_updated']].sort_values('pressure_mb', ascending=False).head(5)\n",
        "    for _, row in top_outliers.iterrows():\n",
        "        print(f\"      {row['country']:20s} {row['pressure_mb']:8.1f} mb  ({row['last_updated']})\")\n",
        "\n",
        "\n",
        "\n",
        "# Dataset con IQR aplicado (solo para comparaci√≥n)\n",
        "\n",
        "# Solo aplicamos los filtros de las columnas originales\n",
        "df_iqr = df[\n",
        "    (df['wind_kph'] >= lim_inf_wind) & (df['wind_kph'] <= lim_sup_wind) &\n",
        "    (df['air_quality_PM2.5'] >= lim_inf_pm25) & (df['air_quality_PM2.5'] <= lim_sup_pm25) &\n",
        "    (df['pressure_mb'] >= lim_inf_pressure) & (df['pressure_mb'] <= lim_sup_pressure)\n",
        "].copy()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"Tama√±o original del dataset (df_enriched): {len(df):,}\")\n",
        "print(f\"Tama√±o despu√©s de IQR: {len(df_iqr):,}\")\n",
        "print(f\"Registros eliminados: {len(df) - len(df_iqr):,} ({(len(df) - len(df_iqr))/len(df)*100:.2f}%)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfjyPm81CNeX"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZACI√ìN: BOXPLOTS CON L√çMITES IQR\n",
        "# ============================================================================\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=(\"Velocidad del viento (IQR)\", \"PM2.5 (IQR)\", \"Presi√≥n atmosf√©rica (IQR)\")\n",
        ")\n",
        "\n",
        "# Boxplot 1: Velocidad del viento\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_enriched[\"wind_kph\"], name='',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "           line=dict(color='rgb(99, 110, 250)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_hline(y=lim_inf_wind, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=f\"L√≠m inf: {lim_inf_wind:.1f}\", row=1, col=1)\n",
        "fig.add_hline(y=lim_sup_wind, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=f\"L√≠m sup: {lim_sup_wind:.1f}\", row=1, col=1)\n",
        "\n",
        "# Boxplot 2: PM2.5\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_enriched[\"air_quality_PM2.5\"], name='',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "           line=dict(color='rgb(99, 110, 250)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=2\n",
        ")\n",
        "fig.add_hline(y=lim_inf_pm25, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=f\"L√≠m inf: {lim_inf_pm25:.1f}\", row=1, col=2)\n",
        "fig.add_hline(y=lim_sup_pm25, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=f\"L√≠m sup: {lim_sup_pm25:.1f}\", row=1, col=2)\n",
        "\n",
        "# Boxplot 3: Presi√≥n atmosf√©rica\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_enriched[\"pressure_mb\"], name='',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "           line=dict(color='rgb(99, 110, 250)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=3\n",
        ")\n",
        "fig.add_hline(y=lim_inf_pressure, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=f\"L√≠m inf: {lim_inf_pressure:.1f}\", row=1, col=3)\n",
        "fig.add_hline(y=lim_sup_pressure, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=f\"L√≠m sup: {lim_sup_pressure:.1f}\", row=1, col=3)\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Detecci√≥n de Outliers - M√©todo IQR\",\n",
        "    height=500,\n",
        "    width=1400,\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-Zx42q7CQ4D"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# UMBRALES FIJOS (CRITERIO FINAL)\n",
        "# ============================================================================\n",
        "\n",
        "df = df_enriched.copy()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DETECCI√ìN DE OUTLIERS - UMBRALES FIJOS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nUmbrales establecidos basados en criterios f√≠sicos:\")\n",
        "print(\"  ‚Ä¢ Velocidad del viento: < 150 kph (Hurac√°n categor√≠a 1)\")\n",
        "print(\"  ‚Ä¢ PM2.5: < 500 ¬µg/m¬≥ (Nivel extremadamente peligroso OMS)\")\n",
        "print(\"  ‚Ä¢ Presi√≥n atmosf√©rica: 900-1100 mb (Rango de fen√≥menos extremos)\")\n",
        "\n",
        "# Contar outliers con umbrales fijos\n",
        "# Las operaciones se realizan sobre la copia local 'df' (que es df_enriched)\n",
        "outliers_wind_fijo = df[df[\"wind_kph\"] >= 150]\n",
        "outliers_pm25_fijo = df[df[\"air_quality_PM2.5\"] >= 500]\n",
        "outliers_pressure_fijo = df[(df[\"pressure_mb\"] < 900) | (df[\"pressure_mb\"] > 1100)]\n",
        "\n",
        "print(f\"\\nOutliers detectados:\")\n",
        "print(f\"  ‚Ä¢ Viento >= 150 kph: {len(outliers_wind_fijo)} registros\")\n",
        "print(f\"  ‚Ä¢ PM2.5 >= 500 ¬µg/m¬≥: {len(outliers_pm25_fijo)} registros\")\n",
        "print(f\"  ‚Ä¢ Presi√≥n fuera de 900-1100 mb: {len(outliers_pressure_fijo)} registros\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIcOcdVaCTnq"
      },
      "outputs": [],
      "source": [
        "# VISUALIZACI√ìN: BOXPLOTS CON UMBRALES FIJOS (ANTES DEL FILTRADO)\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=(\"Velocidad del viento (antes)\", \"PM2.5 (antes)\", \"Presi√≥n atmosf√©rica (antes)\")\n",
        ")\n",
        "\n",
        "# Boxplot 1: Velocidad del viento\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_enriched[\"wind_kph\"], name='',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "           line=dict(color='rgb(99, 110, 250)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_hline(y=150, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=\"Umbral 150 kph\", row=1, col=1)\n",
        "\n",
        "# Boxplot 2: PM2.5\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_enriched[\"air_quality_PM2.5\"], name='',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "           line=dict(color='rgb(99, 110, 250)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=2\n",
        ")\n",
        "fig.add_hline(y=500, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=\"Umbral 500 ¬µg/m¬≥\", row=1, col=2)\n",
        "\n",
        "# Boxplot 3: Presi√≥n atmosf√©rica\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_enriched[\"pressure_mb\"], name='',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "           line=dict(color='rgb(99, 110, 250)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=3\n",
        ")\n",
        "fig.add_hline(y=900, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=\"Umbral 900 mb\", row=1, col=3)\n",
        "fig.add_hline(y=1100, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=\"Umbral 1100 mb\", row=1, col=3)\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Detecci√≥n de Outliers - Umbrales Fijos (ANTES del filtrado)\",\n",
        "    height=500,\n",
        "    width=1400,\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7utWBgaHCX5u"
      },
      "outputs": [],
      "source": [
        "# APLICAR FILTRADO CON UMBRALES FIJOS\n",
        "\n",
        "# Creamos df_filt\n",
        "df_filt = df_enriched[\n",
        "    (df_enriched[\"wind_kph\"] < 150) &\n",
        "    (df_enriched[\"air_quality_PM2.5\"] < 500) &\n",
        "    (df_enriched[\"pressure_mb\"] > 900) &\n",
        "    (df_enriched[\"pressure_mb\"] < 1100)\n",
        "].copy()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTADO DEL FILTRADO CON UMBRALES FIJOS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Registros ANTES del filtro (df_enriched): {len(df_enriched):,}\")\n",
        "print(f\"Registros DESPU√âS del filtro (df_filt): {len(df_filt):,}\")\n",
        "print(f\"Registros eliminados: {len(df_enriched) - len(df_filt):,} ({(len(df_enriched) - len(df_filt))/len(df_enriched)*100:.2f}%)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8S69R_1CZyt"
      },
      "outputs": [],
      "source": [
        "# VISUALIZACI√ìN: BOXPLOTS DESPU√âS DEL FILTRADO\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=(\"Velocidad del viento (despu√©s)\", \"PM2.5 (despu√©s)\", \"Presi√≥n atmosf√©rica (despu√©s)\")\n",
        ")\n",
        "\n",
        "# Boxplot 1: Velocidad del viento\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_filt[\"wind_kph\"], name='',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "           line=dict(color='rgb(99, 110, 250)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_hline(y=150, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=\"Umbral 150 kph\", row=1, col=1)\n",
        "\n",
        "# Boxplot 2: PM2.5\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_filt[\"air_quality_PM2.5\"], name='',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "           line=dict(color='rgb(99, 110, 250)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=2\n",
        ")\n",
        "fig.add_hline(y=500, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=\"Umbral 500 ¬µg/m¬≥\", row=1, col=2)\n",
        "\n",
        "# Boxplot 3: Presi√≥n atmosf√©rica\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_filt[\"pressure_mb\"], name='',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "           line=dict(color='rgb(99, 110, 250)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=3\n",
        ")\n",
        "fig.add_hline(y=900, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=\"Umbral 900 mb\", row=1, col=3)\n",
        "fig.add_hline(y=1100, line_dash=\"dash\", line_color=\"purple\",\n",
        "              annotation_text=\"Umbral 1100 mb\", row=1, col=3)\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Distribuci√≥n de Variables DESPU√âS del Filtrado\",\n",
        "    height=500,\n",
        "    width=1400,\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQehjBVsCcNp"
      },
      "outputs": [],
      "source": [
        "# COMPARACI√ìN FINAL: IQR VS UMBRALES FIJOS\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=(\"Velocidad del viento\", \"PM2.5\", \"Presi√≥n atmosf√©rica\")\n",
        ")\n",
        "\n",
        "# Boxplot 1: Viento - Fijo\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_filt[\"wind_kph\"], name='Fijo',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.6)'),\n",
        "           showlegend=True),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Boxplot 1: Viento - IQR\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_iqr[\"wind_kph\"], name='IQR',\n",
        "           marker=dict(color='rgba(255, 99, 71, 0.6)'),\n",
        "           showlegend=True),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Boxplot 2: PM2.5 - Fijo\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_filt[\"air_quality_PM2.5\"], name='Fijo',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.6)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Boxplot 2: PM2.5 - IQR\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_iqr[\"air_quality_PM2.5\"], name='IQR',\n",
        "           marker=dict(color='rgba(255, 99, 71, 0.6)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Boxplot 3: Presi√≥n - Fijo\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_filt[\"pressure_mb\"], name='Fijo',\n",
        "           marker=dict(color='rgba(99, 110, 250, 0.6)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=3\n",
        ")\n",
        "\n",
        "# Boxplot 3: Presi√≥n - IQR\n",
        "fig.add_trace(\n",
        "    go.Box(y=df_iqr[\"pressure_mb\"], name='IQR',\n",
        "           marker=dict(color='rgba(255, 99, 71, 0.6)'),\n",
        "           showlegend=False),\n",
        "    row=1, col=3\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Comparaci√≥n: Umbrales Fijos vs IQR\",\n",
        "    height=600,\n",
        "    width=1400,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.update_xaxes(title_text=\"M√©todo\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"M√©todo\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"M√©todo\", row=1, col=3)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bKMRsXYCfi2"
      },
      "source": [
        "**CONCLUSI√ìN**\n",
        "\n",
        "Mientras que el m√©todo IQR se adapta estrictamente a la estad√≠stica del *dataset*, los umbrales fijos aportan un control basado en **criterios f√≠sicos y de contexto**.\n",
        "\n",
        "**Para este an√°lisis global de calidad del aire, donde eventos extremos (tormentas, sistemas meteorol√≥gicos intensos y alta contaminaci√≥n) son **informativos** y no errores, se ha optado por el enfoque de **umbrales fijos** que preserva la variabilidad real del fen√≥meno estudiado y garantiza resultados m√°s interpretables en un contexto f√≠sico.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiaV0mO-KrHO"
      },
      "source": [
        "### **6.2 AN√ÅLISIS DE CALIDAD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDmLFJzAK3vt"
      },
      "outputs": [],
      "source": [
        "# Valores faltantes\n",
        "print('Valores nulos por columna: ', df_filt.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nQ6y53mK_bb"
      },
      "outputs": [],
      "source": [
        "# Valores duplicados\n",
        "print('Cantidad de duplicados: ', df_filt.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv_g_xYoLKMN"
      },
      "outputs": [],
      "source": [
        "# Valores √∫nicos por columna\n",
        "print('Valores √∫nicos por columna: ', df_filt.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4j7Ft0bG8vE"
      },
      "source": [
        "### **6.3 AN√ÅLISIS DE CORRELACIONES LINEALES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kLqOZxSHGOX"
      },
      "outputs": [],
      "source": [
        "# AN√ÅLISIS DE CORRELACIONES CON PM2.5\n",
        "\n",
        "# Seleccionar solo variables num√©ricas relevantes para el an√°lisis\n",
        "variables_numericas = [\n",
        "    'latitude', 'longitude', 'temperature_celsius', 'wind_kph',\n",
        "    'wind_degree', 'pressure_mb', 'precip_mm', 'humidity',\n",
        "    'cloud', 'uv_index', 'air_quality_PM2.5', 'month', 'hour',\n",
        "    'population', 'population_density'\n",
        "]\n",
        "\n",
        "# Crear subset de datos num√©ricos\n",
        "df_numeric = df_filt[variables_numericas].copy()\n",
        "\n",
        "# Calcular matriz de correlaciones\n",
        "correlation_matrix = df_numeric.corr()\n",
        "\n",
        "# Extraer correlaciones espec√≠ficas con PM2.5 y ordenar\n",
        "correlaciones_pm25 = correlation_matrix['air_quality_PM2.5'].sort_values(ascending=False)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CORRELACIONES CON PM2.5 (ordenadas de mayor a menor)\")\n",
        "print(\"=\" * 70)\n",
        "print(correlaciones_pm25)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Identificar variables con correlaci√≥n moderada/fuerte (|r| > 0.3)\n",
        "correlaciones_relevantes = correlaciones_pm25[abs(correlaciones_pm25) > 0.3]\n",
        "print(\"Variables con correlaci√≥n relevante (|r| > 0.3):\")\n",
        "print(correlaciones_relevantes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1miP1CpNHIgs"
      },
      "outputs": [],
      "source": [
        "# Crear heatmap interactivo\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Preparar datos para el heatmap\n",
        "corr_values = correlation_matrix.values\n",
        "variables_labels = correlation_matrix.columns.tolist()\n",
        "\n",
        "# Crear heatmap\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=corr_values,\n",
        "    x=variables_labels,\n",
        "    y=variables_labels,\n",
        "    colorscale='RdBu',\n",
        "    zmid=0,\n",
        "    showscale=True,\n",
        "    annotation_text=np.around(corr_values, decimals=2)\n",
        ")\n",
        "\n",
        "# Configurar layout\n",
        "fig.update_layout(\n",
        "    title='Matriz de Correlaciones - Variables Meteorol√≥gicas y PM2.5',\n",
        "    width=1200,\n",
        "    height=800,\n",
        "    xaxis={'side': 'bottom'},\n",
        "    font=dict(size=10, weight='bold')\n",
        ")\n",
        "\n",
        "fig.update_xaxes(tickangle=-45)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoO7OLIYHLUQ"
      },
      "outputs": [],
      "source": [
        "# Crear gr√°fico de barras horizontal para correlaciones con PM2.5\n",
        "# Excluir la autocorrelaci√≥n (PM2.5 consigo mismo = 1.0)\n",
        "correlaciones_plot = correlaciones_pm25.drop('air_quality_PM2.5')\n",
        "\n",
        "# Crear figura\n",
        "fig = go.Figure()\n",
        "\n",
        "# Colores contrastantes: NARANJA para negativas, AZUL para positivas\n",
        "colors = ['rgba(255, 99, 71, 0.7)' if x < 0 else 'rgba(99, 110, 250, 0.7)'\n",
        "          for x in correlaciones_plot.values]\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "    y=correlaciones_plot.index,\n",
        "    x=correlaciones_plot.values,\n",
        "    orientation='h',\n",
        "    marker=dict(\n",
        "        color=colors,\n",
        "        line=dict(width=1, color='white')\n",
        "    ),\n",
        "    text=np.round(correlaciones_plot.values, 3),\n",
        "    textposition='outside'\n",
        "))\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    title='Correlaciones de Variables Meteorol√≥gicas con PM2.5',\n",
        "    xaxis_title='Coeficiente de Correlaci√≥n (r)',\n",
        "    yaxis_title='Variables',\n",
        "    height=600,\n",
        "    width=900,\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "# A√±adir l√≠nea vertical en x=0\n",
        "fig.add_vline(x=0, line_dash=\"dash\", line_color=\"black\", line_width=1)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOg17Kf8HN_W"
      },
      "outputs": [],
      "source": [
        "# Interpretaci√≥n autom√°tica de resultados\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INTERPRETACI√ìN DE CORRELACIONES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Correlaciones positivas m√°s fuertes\n",
        "corr_positivas = correlaciones_pm25[correlaciones_pm25 > 0.3].drop('air_quality_PM2.5')\n",
        "if len(corr_positivas) > 0:\n",
        "    print(\"\\n‚úì Variables con correlaci√≥n POSITIVA relevante:\")\n",
        "    for var, valor in corr_positivas.items():\n",
        "        print(f\"  ‚Ä¢ {var}: r = {valor:.3f}\")\n",
        "        if 'population' in var:\n",
        "            print(f\"    ‚Üí A mayor poblaci√≥n, mayor PM2.5 (factor socioecon√≥mico)\")\n",
        "        elif 'humidity' in var:\n",
        "            print(f\"    ‚Üí Mayor humedad asociada con mayor PM2.5 (aglomeraci√≥n de part√≠culas)\")\n",
        "        elif 'cloud' in var:\n",
        "            print(f\"    ‚Üí Mayor nubosidad asociada con peor dispersi√≥n\")\n",
        "\n",
        "# Correlaciones negativas m√°s fuertes\n",
        "corr_negativas = correlaciones_pm25[correlaciones_pm25 < -0.3]\n",
        "if len(corr_negativas) > 0:\n",
        "    print(\"\\n‚úì Variables con correlaci√≥n NEGATIVA relevante:\")\n",
        "    for var, valor in corr_negativas.items():\n",
        "        print(f\"  ‚Ä¢ {var}: r = {valor:.3f}\")\n",
        "        if 'wind' in var:\n",
        "            print(f\"    ‚Üí Mayor viento dispersa contaminantes (esperado)\")\n",
        "        elif 'precip' in var:\n",
        "            print(f\"    ‚Üí Lluvia limpia el aire de part√≠culas (esperado)\")\n",
        "\n",
        "# Correlaciones d√©biles\n",
        "corr_debiles = correlaciones_pm25[(abs(correlaciones_pm25) < 0.3) & (correlaciones_pm25.index != 'air_quality_PM2.5')]\n",
        "if len(corr_debiles) > 0:\n",
        "    print(f\"\\n‚úì Variables con correlaci√≥n D√âBIL (|r| < 0.3): {len(corr_debiles)}\")\n",
        "    print(\"  Estas variables tienen poco poder predictivo lineal para PM2.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxvF3RNqHsZo"
      },
      "source": [
        "### **6.4 AN√ÅLISIS DE DISTRIBUCIONES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILHNLupGHv5R"
      },
      "outputs": [],
      "source": [
        "# AN√ÅLISIS DE DISTRIBUCIONES - VARIABLES CLAVE\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Seleccionar variables clave para analizar\n",
        "variables_analizar = [\n",
        "    'air_quality_PM2.5',\n",
        "    'temperature_celsius',\n",
        "    'wind_kph',\n",
        "    'pressure_mb',\n",
        "    'precip_mm',\n",
        "    'humidity',\n",
        "    'population_density'\n",
        "]\n",
        "\n",
        "# Calcular estad√≠sticos de forma para cada variable\n",
        "print(\"=\" * 80)\n",
        "print(\"ESTAD√çSTICOS DE DISTRIBUCI√ìN\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Variable':<25} {'Media':<10} {'Mediana':<10} {'Asimetr√≠a':<12} {'Curtosis':<10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for var in variables_analizar:\n",
        "    media = df_filt[var].mean()\n",
        "    mediana = df_filt[var].median()\n",
        "    asimetria = stats.skew(df_filt[var])\n",
        "    curtosis = stats.kurtosis(df_filt[var])\n",
        "\n",
        "    print(f\"{var:<25} {media:<10.2f} {mediana:<10.2f} {asimetria:<12.2f} {curtosis:<10.2f}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Interpretaci√≥n de asimetr√≠a:\")\n",
        "print(\"  ‚Ä¢ Asimetr√≠a ‚âà 0: Distribuci√≥n sim√©trica\")\n",
        "print(\"  ‚Ä¢ Asimetr√≠a > 1: Asimetr√≠a positiva fuerte (cola derecha)\")\n",
        "print(\"  ‚Ä¢ Asimetr√≠a < -1: Asimetr√≠a negativa fuerte (cola izquierda)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ialkx2uHza2"
      },
      "outputs": [],
      "source": [
        "# Subplots para histogramas\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Grid de gr√°ficos\n",
        "n_vars = len(variables_analizar)\n",
        "n_cols = 3\n",
        "n_rows = (n_vars + n_cols - 1) // n_cols\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=n_rows,\n",
        "    cols=n_cols,\n",
        "    subplot_titles=[var.replace('_', ' ').title() for var in variables_analizar],\n",
        "    vertical_spacing=0.12,\n",
        "    horizontal_spacing=0.1\n",
        ")\n",
        "\n",
        "# Histograma para cada variable\n",
        "for idx, var in enumerate(variables_analizar):\n",
        "    row = idx // n_cols + 1\n",
        "    col = idx % n_cols + 1\n",
        "\n",
        "    # Histograma\n",
        "    fig.add_trace(\n",
        "        go.Histogram(\n",
        "            x=df_filt[var],\n",
        "            name=var,\n",
        "            nbinsx=50,\n",
        "            marker=dict(\n",
        "                color='rgba(99, 110, 250, 0.7)',\n",
        "                line=dict(color='rgb(99, 110, 250)', width=1)\n",
        "            ),\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "# Layout general\n",
        "fig.update_layout(\n",
        "    title_text=\"Distribuciones de Variables Clave\",\n",
        "    height=300 * n_rows,\n",
        "    width=1400,\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='rgb(229, 236, 246)'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzvGRwRbH22Q"
      },
      "outputs": [],
      "source": [
        "# Crear boxplots para comparar distribuciones\n",
        "fig = make_subplots(\n",
        "    rows=2,\n",
        "    cols=4,\n",
        "    subplot_titles=[var.replace('_', ' ').title() for var in variables_analizar],\n",
        "    vertical_spacing=0.15,\n",
        "    horizontal_spacing=0.08\n",
        ")\n",
        "\n",
        "for idx, var in enumerate(variables_analizar):\n",
        "    row = idx // 4 + 1\n",
        "    col = idx % 4 + 1\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Box(\n",
        "            y=df_filt[var],\n",
        "            name='',\n",
        "            marker=dict(color='rgba(99, 110, 250, 0.5)'),\n",
        "            line=dict(color='rgb(99, 110, 250)'),\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Boxplots - Detecci√≥n de Outliers\",\n",
        "    height=700,\n",
        "    width=1400,\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='rgb(229, 236, 246)'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZYsq0gUH6ur"
      },
      "outputs": [],
      "source": [
        "# An√°lisis autom√°tico de distribuciones\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INTERPRETACI√ìN DE DISTRIBUCIONES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for var in variables_analizar:\n",
        "    asimetria = stats.skew(df_filt[var])\n",
        "\n",
        "    print(f\"\\n‚Ä¢ {var.upper()}:\")\n",
        "\n",
        "    if abs(asimetria) < 0.5:\n",
        "        print(f\"  ‚úì Distribuci√≥n aproximadamente sim√©trica (asimetr√≠a = {asimetria:.2f})\")\n",
        "    elif asimetria > 1:\n",
        "        print(f\"  ‚ö† Distribuci√≥n con asimetr√≠a positiva fuerte (asimetr√≠a = {asimetria:.2f})\")\n",
        "        print(f\"    ‚Üí Valores extremos altos. Considerar transformaci√≥n log para modelado.\")\n",
        "    elif asimetria < -1:\n",
        "        print(f\"  ‚ö† Distribuci√≥n con asimetr√≠a negativa fuerte (asimetr√≠a = {asimetria:.2f})\")\n",
        "        print(f\"    ‚Üí Valores extremos bajos.\")\n",
        "    else:\n",
        "        print(f\"  ~ Asimetr√≠a moderada (asimetr√≠a = {asimetria:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkeTIFeLIEUI"
      },
      "source": [
        "### **6.5 AN√ÅLISIS DE EFECTOS DE UMBRAL (THRESHOLD EFFECTS)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yWcHJxHIKo6"
      },
      "outputs": [],
      "source": [
        "# AN√ÅLISIS DE EFECTOS CATEG√ìRICOS: LLUVIA Y VIENTO\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "print(\"=\"*80)\n",
        "print(\"AN√ÅLISIS DE EFECTOS NO LINEALES (Variables Binarias)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. EFECTO DE LLUVIA\n",
        "\n",
        "print(\"\\nüåßÔ∏è EFECTO DE LLUVIA:\")\n",
        "pm25_sin_lluvia = df_filt[df_filt['rained']==0]['air_quality_PM2.5'].mean()\n",
        "pm25_con_lluvia = df_filt[df_filt['rained']==1]['air_quality_PM2.5'].mean()\n",
        "diferencia = pm25_sin_lluvia - pm25_con_lluvia\n",
        "print(f\" PM2.5 SIN lluvia: {pm25_sin_lluvia:.2f} ¬µg/m¬≥\")\n",
        "print(f\" PM2.5 CON lluvia: {pm25_con_lluvia:.2f} ¬µg/m¬≥\")\n",
        "print(f\" Diferencia: {diferencia:.2f} ¬µg/m¬≥ ({diferencia/pm25_sin_lluvia*100:.1f}% de reducci√≥n)\")\n",
        "sin_lluvia = df_filt[df_filt['rained']==0]['air_quality_PM2.5']\n",
        "con_lluvia = df_filt[df_filt['rained']==1]['air_quality_PM2.5']\n",
        "t_stat, p_value = ttest_ind(sin_lluvia, con_lluvia)\n",
        "print(f\" T-test: t={t_stat:.3f}, p-value={p_value:.6f}\")\n",
        "if p_value < 0.001:\n",
        "    print(\" ‚úÖ Diferencia ESTAD√çSTICAMENTE SIGNIFICATIVA (p < 0.001)\")\n",
        "\n",
        "# 2. EFECTO DE VIENTO FUERTE\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí® EFECTO DE VIENTO FUERTE (>20 kph):\")\n",
        "pm25_viento_bajo = df_filt[df_filt['high_wind']==0]['air_quality_PM2.5'].mean()\n",
        "pm25_viento_alto = df_filt[df_filt['high_wind']==1]['air_quality_PM2.5'].mean()\n",
        "diferencia_viento = pm25_viento_bajo - pm25_viento_alto\n",
        "print(f\" PM2.5 con viento bajo: {pm25_viento_bajo:.2f} ¬µg/m¬≥\")\n",
        "print(f\" PM2.5 con viento alto: {pm25_viento_alto:.2f} ¬µg/m¬≥\")\n",
        "print(f\" Diferencia: {diferencia_viento:.2f} ¬µg/m¬≥ ({diferencia_viento/pm25_viento_bajo*100:.1f}% de reducci√≥n)\")\n",
        "viento_bajo = df_filt[df_filt['high_wind']==0]['air_quality_PM2.5']\n",
        "viento_alto = df_filt[df_filt['high_wind']==1]['air_quality_PM2.5']\n",
        "t_stat_v, p_value_v = ttest_ind(viento_bajo, viento_alto)\n",
        "print(f\" T-test: t={t_stat_v:.3f}, p-value={p_value_v:.6f}\")\n",
        "if p_value_v < 0.001:\n",
        "    print(\" ‚úÖ Diferencia ESTAD√çSTICAMENTE SIGNIFICATIVA (p < 0.001)\")\n",
        "\n",
        "# 3. VISUALIZACI√ìN COMPARATIVA\n",
        "\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Efecto Lluvia\", \"Efecto Viento Fuerte\"))\n",
        "\n",
        "# Subplot 1: Lluvia\n",
        "fig.add_trace(go.Box(\n",
        "    y=sin_lluvia,\n",
        "    name='Sin lluvia',\n",
        "    marker=dict(color='rgba(99, 110, 250, 0.6)'),\n",
        "    showlegend=True\n",
        "), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Box(\n",
        "    y=con_lluvia,\n",
        "    name='Con lluvia',\n",
        "    marker=dict(color='rgba(255, 99, 71, 0.6)'),\n",
        "    showlegend=True\n",
        "), row=1, col=1)\n",
        "\n",
        "# Subplot 2: Viento\n",
        "fig.add_trace(go.Box(\n",
        "    y=viento_bajo,\n",
        "    name='Viento bajo',\n",
        "    marker=dict(color='rgba(99, 110, 250, 0.6)'),\n",
        "    showlegend=True\n",
        "), row=1, col=2)\n",
        "\n",
        "fig.add_trace(go.Box(\n",
        "    y=viento_alto,\n",
        "    name='Viento alto',\n",
        "    marker=dict(color='rgba(255, 99, 71, 0.6)'),\n",
        "    showlegend=True\n",
        "), row=1, col=2)\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Efectos Categ√≥ricos en PM2.5',\n",
        "    height=500,\n",
        "    width=1200,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "fig.update_yaxes(title_text=\"PM2.5 (¬µg/m¬≥)\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"PM2.5 (¬µg/m¬≥)\", row=1, col=2)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaU4EDI3L2cd"
      },
      "source": [
        "### **6.6 AN√ÅLISIS TEMPORAL**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5tskheZL9UK"
      },
      "source": [
        "En esta secci√≥n exploramos patrones temporales en las concentraciones de PM2.5 para identificar:\n",
        "- Variaciones estacionales (por mes)\n",
        "- Patrones diurnos (por hora del d√≠a)\n",
        "- Interacciones entre ambos factores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaYeinqkMFDt"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# AN√ÅLISIS TEMPORAL - PATRONES POR MES Y HORA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AN√ÅLISIS TEMPORAL DE PM2.5\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Estad√≠sticas de PM2.5 por mes\n",
        "pm25_por_mes = df_filt.groupby('month')['air_quality_PM2.5'].agg([\n",
        "    ('Media', 'mean'),\n",
        "    ('Mediana', 'median'),\n",
        "    ('Desv_Std', 'std'),\n",
        "    ('Min', 'min'),\n",
        "    ('Max', 'max'),\n",
        "    ('Registros', 'count')\n",
        "]).round(2)\n",
        "\n",
        "print(\"\\nüìÖ PM2.5 POR MES:\")\n",
        "print(pm25_por_mes)\n",
        "\n",
        "# Estad√≠sticas de PM2.5 por hora\n",
        "pm25_por_hora = df_filt.groupby('hour')['air_quality_PM2.5'].agg([\n",
        "    ('Media', 'mean'),\n",
        "    ('Mediana', 'median'),\n",
        "    ('Desv_Std', 'std'),\n",
        "    ('Registros', 'count')\n",
        "]).round(2)\n",
        "\n",
        "print(\"\\n‚è∞ PM2.5 POR HORA DEL D√çA:\")\n",
        "print(pm25_por_hora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqsrcgVAMKR3"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZACI√ìN: PM2.5 POR MES (BOXPLOTS)\n",
        "# ============================================================================\n",
        "\n",
        "# Mapear n√∫meros de mes a nombres\n",
        "meses_labels = {\n",
        "    5: 'Mayo', 6: 'Junio', 7: 'Julio', 8: 'Agosto',\n",
        "    9: 'Septiembre', 10: 'Octubre', 11: 'Noviembre', 12: 'Diciembre',\n",
        "    1: 'Enero', 2: 'Febrero', 3: 'Marzo', 4: 'Abril'\n",
        "}\n",
        "\n",
        "# Agregar etiqueta de mes\n",
        "df_filt['mes_label'] = df_filt['month'].map(meses_labels) #\n",
        "\n",
        "# Ordenar meses cronol√≥gicamente (Mayo 2024 - Abril 2025)\n",
        "orden_meses = ['Mayo', 'Junio', 'Julio', 'Agosto', 'Septiembre', 'Octubre',\n",
        "               'Noviembre', 'Diciembre', 'Enero', 'Febrero', 'Marzo', 'Abril']\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "for mes in orden_meses:\n",
        "    datos_mes = df_filt[df_filt['mes_label'] == mes]['air_quality_PM2.5']\n",
        "\n",
        "    fig.add_trace(go.Box(\n",
        "        y=datos_mes,\n",
        "        name=mes,\n",
        "        marker=dict(color='rgba(99, 110, 250, 0.6)'),\n",
        "        boxmean='sd'  # Muestra media y desviaci√≥n est√°ndar\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Distribuci√≥n de PM2.5 por Mes (Mayo 2024 - Abril 2025)',\n",
        "    yaxis_title='PM2.5 (¬µg/m¬≥)',\n",
        "    xaxis_title='Mes',\n",
        "    height=600,\n",
        "    width=1400,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white',\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82HOX-CXMQ1f"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZACI√ìN: PM2.5 POR HORA (PATR√ìN DIURNO)\n",
        "# ============================================================================\n",
        "# Calcular media y error est√°ndar por hora (usando df_filt)\n",
        "pm25_hora_stats = df_filt.groupby('hour')['air_quality_PM2.5'].agg([\n",
        "    'mean', 'median', 'std', 'count'\n",
        "]).reset_index()\n",
        "# Calcular error est√°ndar\n",
        "pm25_hora_stats['error_std'] = pm25_hora_stats['std'] / np.sqrt(pm25_hora_stats['count'])\n",
        "\n",
        "fig = go.Figure()\n",
        "# L√≠nea de media\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pm25_hora_stats['hour'],\n",
        "    y=pm25_hora_stats['mean'],\n",
        "    mode='lines+markers',\n",
        "    name='Media',\n",
        "    line=dict(color='rgba(99, 110, 250, 1)', width=3),  #\n",
        "    marker=dict(size=8, color='rgba(99, 110, 250, 1)')\n",
        "))\n",
        "# L√≠nea de mediana\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pm25_hora_stats['hour'],\n",
        "    y=pm25_hora_stats['median'],\n",
        "    mode='lines+markers',\n",
        "    name='Mediana',\n",
        "    line=dict(color='rgba(99, 110, 250, 0.7)', width=2, dash='dash'),\n",
        "    marker=dict(size=6, color='rgba(99, 110, 250, 0.7)')\n",
        "))\n",
        "# √Årea de error est√°ndar\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pm25_hora_stats['hour'].tolist() + pm25_hora_stats['hour'].tolist()[::-1],\n",
        "    y=(pm25_hora_stats['mean'] + pm25_hora_stats['error_std']).tolist() +\n",
        "      (pm25_hora_stats['mean'] - pm25_hora_stats['error_std']).tolist()[::-1],\n",
        "    fill='toself',\n",
        "    fillcolor='rgba(99, 110, 250, 0.2)',\n",
        "    line=dict(color='rgba(255,255,255,0)'),\n",
        "    showlegend=True,\n",
        "    name='Error Est√°ndar'\n",
        "))\n",
        "fig.update_layout(\n",
        "    title='Patr√≥n Diurno de PM2.5 (24 horas)',\n",
        "    xaxis_title='Hora del D√≠a',\n",
        "    yaxis_title='PM2.5 (¬µg/m¬≥)',\n",
        "    height=600,\n",
        "    width=1200,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white',\n",
        "    xaxis=dict(\n",
        "        tickmode='linear',\n",
        "        tick0=0,\n",
        "        dtick=2,\n",
        "        range=[-0.5, 23.5]\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA6_uEDjMVwX"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZACI√ìN: HEATMAP MES √ó HORA\n",
        "# ============================================================================\n",
        "# Crear matriz de promedios mes √ó hora (usando df_filt)\n",
        "matriz_temporal = df_filt.pivot_table(\n",
        "    values='air_quality_PM2.5',\n",
        "    index='month',\n",
        "    columns='hour',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "# Reordenar meses cronol√≥gicamente\n",
        "orden_meses_num = [5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4]\n",
        "matriz_temporal = matriz_temporal.reindex(orden_meses_num)\n",
        "# Crear etiquetas\n",
        "meses_labels_cortos = ['Mayo', 'Jun', 'Jul', 'Ago', 'Sep', 'Oct', 'Nov', 'Dic', 'Ene', 'Feb', 'Mar', 'Abr']\n",
        "\n",
        "fig = go.Figure(data=go.Heatmap(\n",
        "    z=matriz_temporal.values,\n",
        "    x=[f'{h:02d}:00' for h in matriz_temporal.columns],\n",
        "    y=meses_labels_cortos,\n",
        "    colorscale=[\n",
        "        [0, 'rgba(229, 236, 246, 0.3)'],      # Valores bajos: azul muy claro\n",
        "        [0.5, 'rgba(99, 110, 250, 0.6)'],     # Valores medios: tu color\n",
        "        [1, 'rgba(99, 110, 250, 1)']          # Valores altos: tu color m√°s intenso\n",
        "    ],\n",
        "    colorbar=dict(title='PM2.5<br>(¬µg/m¬≥)')\n",
        "))\n",
        "fig.update_layout(\n",
        "    title='Heatmap: PM2.5 Promedio por Mes y Hora',\n",
        "    xaxis_title='Hora del D√≠a',\n",
        "    yaxis_title='Mes',\n",
        "    height=600,\n",
        "    width=1200\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMrv7vJbMarb"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# INTERPRETACI√ìN DE PATRONES TEMPORALES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INTERPRETACI√ìN DE PATRONES TEMPORALES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Identificar mes con mayor/menor PM2.5\n",
        "mes_max = pm25_por_mes['Media'].idxmax()\n",
        "mes_min = pm25_por_mes['Media'].idxmin()\n",
        "pm25_max = pm25_por_mes.loc[mes_max, 'Media']\n",
        "pm25_min = pm25_por_mes.loc[mes_min, 'Media']\n",
        "\n",
        "print(f\"\\nüìÖ VARIACI√ìN ESTACIONAL:\")\n",
        "print(f\"  ‚Ä¢ Mes con MAYOR PM2.5: {meses_labels[mes_max]} ({pm25_max:.2f} ¬µg/m¬≥)\")\n",
        "print(f\"  ‚Ä¢ Mes con MENOR PM2.5: {meses_labels[mes_min]} ({pm25_min:.2f} ¬µg/m¬≥)\")\n",
        "print(f\"  ‚Ä¢ Diferencia: {pm25_max - pm25_min:.2f} ¬µg/m¬≥ ({((pm25_max/pm25_min - 1) * 100):.1f}% m√°s alto)\")\n",
        "\n",
        "# Identificar hora con mayor/menor PM2.5\n",
        "hora_max = pm25_por_hora['Media'].idxmax()\n",
        "hora_min = pm25_por_hora['Media'].idxmin()\n",
        "pm25_hora_max = pm25_por_hora.loc[hora_max, 'Media']\n",
        "pm25_hora_min = pm25_por_hora.loc[hora_min, 'Media']\n",
        "\n",
        "print(f\"\\n‚è∞ VARIACI√ìN DIURNA:\")\n",
        "print(f\"  ‚Ä¢ Hora con MAYOR PM2.5: {hora_max:02d}:00 ({pm25_hora_max:.2f} ¬µg/m¬≥)\")\n",
        "print(f\"  ‚Ä¢ Hora con MENOR PM2.5: {hora_min:02d}:00 ({pm25_hora_min:.2f} ¬µg/m¬≥)\")\n",
        "print(f\"  ‚Ä¢ Diferencia: {pm25_hora_max - pm25_hora_min:.2f} ¬µg/m¬≥ ({((pm25_hora_max/pm25_hora_min - 1) * 100):.1f}% m√°s alto)\")\n",
        "\n",
        "# Evaluar magnitud de variaci√≥n\n",
        "variacion_mensual = pm25_por_mes['Media'].std()\n",
        "variacion_horaria = pm25_por_hora['Media'].std()\n",
        "\n",
        "print(f\"\\nüìä MAGNITUD DE VARIACIONES:\")\n",
        "print(f\"  ‚Ä¢ Desviaci√≥n est√°ndar mensual: {variacion_mensual:.2f} ¬µg/m¬≥\")\n",
        "print(f\"  ‚Ä¢ Desviaci√≥n est√°ndar horaria: {variacion_horaria:.2f} ¬µg/m¬≥\")\n",
        "\n",
        "if variacion_mensual > variacion_horaria * 2:\n",
        "    print(\"  ‚Üí La variaci√≥n ESTACIONAL es m√°s importante que la diurna\")\n",
        "elif variacion_horaria > variacion_mensual * 2:\n",
        "    print(\"  ‚Üí La variaci√≥n DIURNA es m√°s importante que la estacional\")\n",
        "else:\n",
        "    print(\"  ‚Üí Ambas variaciones (estacional y diurna) son comparables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbCRyp4vMdJm"
      },
      "source": [
        "### **6.7 AN√ÅLISIS GEOGR√ÅFICO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cs4__6dMhGw"
      },
      "source": [
        "Exploramos la distribuci√≥n espacial de PM2.5 para identificar:\n",
        "- Pa√≠ses con mayor y menor contaminaci√≥n\n",
        "- Patrones regionales\n",
        "- Relaci√≥n entre poblaci√≥n y contaminaci√≥n por pa√≠s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uOY02olMnnE"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# AN√ÅLISIS GEOGR√ÅFICO - PM2.5 POR PA√çS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AN√ÅLISIS GEOGR√ÅFICO DE PM2.5\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calcular estad√≠sticas por pa√≠s (usando df_filt)\n",
        "pm25_por_pais = df_filt.groupby('country').agg({\n",
        "    'air_quality_PM2.5': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
        "    'population': 'first',\n",
        "    'population_density': 'first'\n",
        "}).round(2)\n",
        "\n",
        "# Aplanar columnas multinivel\n",
        "pm25_por_pais.columns = ['PM25_Media', 'PM25_Mediana', 'PM25_Std',\n",
        "                          'PM25_Min', 'PM25_Max', 'Registros',\n",
        "                          'Poblacion', 'Densidad']\n",
        "\n",
        "# Resetear √≠ndice\n",
        "pm25_por_pais = pm25_por_pais.reset_index()\n",
        "\n",
        "# Ordenar por PM2.5 media\n",
        "pm25_por_pais = pm25_por_pais.sort_values('PM25_Media', ascending=False)\n",
        "\n",
        "print(\"\\nüìä TOP 20 PA√çSES M√ÅS CONTAMINADOS:\")\n",
        "print(pm25_por_pais[['country', 'PM25_Media', 'PM25_Mediana', 'Poblacion']].head(20))\n",
        "\n",
        "print(\"\\nüìä TOP 20 PA√çSES MENOS CONTAMINADOS:\")\n",
        "print(pm25_por_pais[['country', 'PM25_Media', 'PM25_Mediana', 'Poblacion']].tail(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1rRzuRJMwj6"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZACI√ìN: TOP 20 PA√çSES M√ÅS CONTAMINADOS\n",
        "# ============================================================================\n",
        "top20_contaminados = pm25_por_pais.head(20).copy()\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Bar(\n",
        "    x=top20_contaminados['PM25_Media'],\n",
        "    y=top20_contaminados['country'],\n",
        "    orientation='h',\n",
        "    marker=dict(\n",
        "        color='rgba(99, 110, 250, 0.6)',\n",
        "        line=dict(width=1, color='white')\n",
        "    ),\n",
        "    text=top20_contaminados['PM25_Media'].round(1),\n",
        "    textposition='outside'\n",
        "))\n",
        "fig.update_layout(\n",
        "    title='Top 20 Pa√≠ses con Mayor Contaminaci√≥n por PM2.5',\n",
        "    xaxis_title='PM2.5 Promedio (¬µg/m¬≥)',\n",
        "    yaxis_title='Pa√≠s',\n",
        "    height=700,\n",
        "    width=1000,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "# Invertir orden y axis para que el m√°s contaminado est√© arriba\n",
        "fig.update_yaxes(autorange=\"reversed\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-hOIpo3M4Wu"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# AN√ÅLISIS: RELACI√ìN POBLACI√ìN - PM2.5 POR PA√çS\n",
        "# ============================================================================\n",
        "\n",
        "# Crear categor√≠as de poblaci√≥n\n",
        "pm25_por_pais['cat_poblacion'] = pd.cut(\n",
        "    pm25_por_pais['Poblacion'],\n",
        "    bins=[0, 10_000_000, 50_000_000, np.inf],\n",
        "    labels=['<10M', '10-50M', '>50M']\n",
        ")\n",
        "\n",
        "# Calcular estad√≠sticas por categor√≠a\n",
        "stats_by_pop = pm25_por_pais.groupby('cat_poblacion')['PM25_Media'].agg([\n",
        "    'mean', 'median', 'std', 'count'\n",
        "]).round(2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PM2.5 PROMEDIO POR CATEGOR√çA DE POBLACI√ìN\")\n",
        "print(\"=\" * 70)\n",
        "print(stats_by_pop)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "fig = go.Figure()\n",
        "\n",
        "for cat in ['<10M', '10-50M', '>50M']:\n",
        "    datos = pm25_por_pais[pm25_por_pais['cat_poblacion'] == cat]['PM25_Media']\n",
        "\n",
        "    fig.add_trace(go.Box(\n",
        "        y=datos,\n",
        "        name=cat,\n",
        "        marker=dict(color='rgba(99, 110, 250, 0.6)'),\n",
        "        boxmean='sd'\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Distribuci√≥n de PM2.5 por Categor√≠a de Poblaci√≥n del Pa√≠s',\n",
        "    xaxis_title='Categor√≠a de Poblaci√≥n',\n",
        "    yaxis_title='PM2.5 (¬µg/m¬≥)',\n",
        "    height=600,\n",
        "    width=900,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXVxZrw4NAGy"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SCATTER PLOT: POBLACI√ìN VS PM2.5 (CON ANOTACIONES)\n",
        "# ============================================================================\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=pm25_por_pais['Poblacion'] / 1_000_000,  # Convertir a millones\n",
        "    y=pm25_por_pais['PM25_Media'],\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        size=pm25_por_pais['Densidad'] / 10,  # Tama√±o por densidad\n",
        "        color='rgba(99, 110, 250, 0.6)',\n",
        "        line=dict(width=1, color='white')\n",
        "    ),\n",
        "    text=pm25_por_pais['country'],\n",
        "    hovertemplate='<b>%{text}</b><br>' +\n",
        "                  'Poblaci√≥n: %{x:.0f}M<br>' +\n",
        "                  'PM2.5: %{y:.1f} ¬µg/m¬≥<br>' +\n",
        "                  '<extra></extra>'\n",
        "))\n",
        "\n",
        "# Anotar pa√≠ses extremos\n",
        "for idx, row in pm25_por_pais.head(5).iterrows():\n",
        "    fig.add_annotation(\n",
        "        x=row['Poblacion'] / 1_000_000,\n",
        "        y=row['PM25_Media'],\n",
        "        text=row['country'],\n",
        "        showarrow=True,\n",
        "        arrowhead=2,\n",
        "        ax=40,\n",
        "        ay=-40\n",
        "    )\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Relaci√≥n entre Poblaci√≥n y PM2.5 por Pa√≠s',\n",
        "    xaxis_title='Poblaci√≥n (millones)',\n",
        "    yaxis_title='PM2.5 Promedio (¬µg/m¬≥)',\n",
        "    height=700,\n",
        "    width=1200,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_Z23vzeNG3_"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# INTERPRETACI√ìN GEOGR√ÅFICA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INTERPRETACI√ìN GEOGR√ÅFICA\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Pa√≠s m√°s/menos contaminado\n",
        "pais_max = pm25_por_pais.iloc[0]\n",
        "pais_min = pm25_por_pais.iloc[-1]\n",
        "\n",
        "print(f\"\\nüåç EXTREMOS GLOBALES:\")\n",
        "print(f\"  ‚Ä¢ Pa√≠s M√ÅS contaminado: {pais_max['country']} ({pais_max['PM25_Media']:.2f} ¬µg/m¬≥)\")\n",
        "print(f\"  ‚Ä¢ Pa√≠s MENOS contaminado: {pais_min['country']} ({pais_min['PM25_Media']:.2f} ¬µg/m¬≥)\")\n",
        "print(f\"  ‚Ä¢ Diferencia: {pais_max['PM25_Media'] - pais_min['PM25_Media']:.2f} ¬µg/m¬≥\")\n",
        "\n",
        "# An√°lisis por poblaci√≥n\n",
        "print(f\"\\nüë• RELACI√ìN CON POBLACI√ìN:\")\n",
        "for cat in ['<10M', '10-50M', '>50M']:\n",
        "    media = stats_by_pop.loc[cat, 'mean']\n",
        "    n_paises = stats_by_pop.loc[cat, 'count']\n",
        "    print(f\"  ‚Ä¢ Pa√≠ses con poblaci√≥n {cat}: {media:.2f} ¬µg/m¬≥ promedio (n={n_paises:.0f})\")\n",
        "\n",
        "# Correlaci√≥n poblaci√≥n-PM2.5\n",
        "corr_pop = pm25_por_pais[['Poblacion', 'PM25_Media']].corr().iloc[0, 1]\n",
        "print(f\"\\nüìä CORRELACI√ìN:\")\n",
        "print(f\"  ‚Ä¢ Poblaci√≥n vs PM2.5: r = {corr_pop:.3f}\")\n",
        "if corr_pop > 0.3:\n",
        "    print(\"  ‚Üí Correlaci√≥n positiva moderada: pa√≠ses m√°s poblados tienden a mayor PM2.5\")\n",
        "elif corr_pop > 0:\n",
        "    print(\"  ‚Üí Correlaci√≥n positiva d√©bil: ligera tendencia\")\n",
        "else:\n",
        "    print(\"  ‚Üí Correlaci√≥n negativa o nula\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxHOLVg3O5n1"
      },
      "source": [
        "**Observaciones Clave por Regi√≥n y Contexto**\n",
        "\n",
        "---\n",
        "\n",
        "El an√°lisis de los datos de PM2.5 revela patrones de contaminaci√≥n fuertemente influenciados por factores geogr√°ficos, demogr√°ficos e industriales espec√≠ficos:\n",
        "\n",
        "**1. Gigantes Industriales y Demogr√°ficos**\n",
        "\n",
        "* **China e India:** Representan los casos m√°s significativos de contaminaci√≥n debido a su **masiva poblaci√≥n** y la **r√°pida industrializaci√≥n** (quema de combustibles f√≥siles, tr√°fico, etc.). Su escala demogr√°fica y econ√≥mica amplifica el problema de la calidad del aire.\n",
        "\n",
        "**2. Pa√≠ses del Golfo (Espec√≠ficos)**\n",
        "\n",
        "* **Arabia Saudita, Kuwait, Qatar, UAE:** La contaminaci√≥n en esta regi√≥n es impulsada por una combinaci√≥n de **emisiones de la industria del petr√≥leo y gas** y **factores naturales** como las frecuentes **tormentas de arena**. El viento levanta grandes cantidades de part√≠culas naturales (polvo y arena), sum√°ndose a los contaminantes industriales.\n",
        "\n",
        "**3. Caso At√≠pico en Sudam√©rica**\n",
        "\n",
        "* **Chile:** Se presenta como un caso particular debido a su **geograf√≠a de valle** (que atrapa los contaminantes) y la influencia de la **miner√≠a**. Este factor subraya que la contaminaci√≥n es un problema altamente **local** y dependiente de la topograf√≠a.\n",
        "\n",
        "**4. La Correlaci√≥n entre Poblaci√≥n y Contaminaci√≥n**\n",
        "\n",
        "* La **correlaci√≥n entre la densidad poblacional y la contaminaci√≥n es visible, pero no absoluta**.\n",
        "* **Ejemplo:** Pa√≠ses como Kuwait demuestran que, a pesar de tener un territorio peque√±o o una poblaci√≥n relativamente menor a los gigantes asi√°ticos, factores industriales intensivos (petr√≥leo) y naturales (tormentas) pueden generar niveles de contaminaci√≥n extremadamente altos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMnVAad_QBb1"
      },
      "source": [
        "# **7. FEATURE ENGINEERING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Mjk3IYQQUm"
      },
      "source": [
        "### **7.1 TRANSFORMACIONES LOGAR√çTMICAS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHG1U5TGQT2W"
      },
      "source": [
        "**Bas√°ndome en el an√°lisis de distribuciones, varias variables presentan asimetr√≠a positiva extrema que puede afectar el rendimiento de modelos de regresi√≥n lineal. Aplicaremos transformaciones logar√≠tmicas para normalizar estas distribuciones.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVE5N1SmQszg"
      },
      "outputs": [],
      "source": [
        "# Crear una copia para Feature Engineering y Modelado\n",
        "df_model = df_filt.copy()\n",
        "\n",
        "print(f\"DataFrame base para modelado creado con {len(df_model):,} registros.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXh1p_jWQ4Re"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRANSFORMACIONES LOGAR√çTMICAS PARA VARIABLES ASIM√âTRICAS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"APLICACI√ìN DE TRANSFORMACIONES LOGAR√çTMICAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Variables a transformar (asimetr√≠a > 1). Se excluye pressure_mb.\n",
        "variables_a_transformar = {\n",
        "    'air_quality_PM2.5': 'pm25_log',\n",
        "    'precip_mm': 'precip_log',\n",
        "    'wind_kph': 'wind_log',\n",
        "    'population_density': 'pop_density_log',\n",
        "    # 'pressure_mb': 'pressure_log' <-- No se recomienda transformar la presi√≥n\n",
        "}\n",
        "\n",
        "print(\"\\nüìä Aplicando transformaci√≥n log1p (log(1+x)) a variables asim√©tricas...\\n\")\n",
        "\n",
        "# Aplicar transformaci√≥n log1p (maneja valores cero)\n",
        "for original, transformada in variables_a_transformar.items():\n",
        "    df_model[transformada] = np.log1p(df_model[original])\n",
        "\n",
        "    # Calcular asimetr√≠a antes y despu√©s\n",
        "    asim_antes = stats.skew(df_model[original])\n",
        "    asim_despues = stats.skew(df_model[transformada])\n",
        "\n",
        "    print(f\"‚úì {original:30s}\")\n",
        "    print(f\"  Asimetr√≠a ANTES:  {asim_antes:8.2f}\")\n",
        "    print(f\"  Asimetr√≠a DESPU√âS: {asim_despues:8.2f}\")\n",
        "    print(f\"  Mejora: {abs(asim_despues) < abs(asim_antes)}\\n\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reSZbGWWRwp2"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPARACI√ìN VISUAL: DISTRIBUCIONES ORIGINALES VS TRANSFORMADAS\n",
        "# ============================================================================\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Variables a visualizar\n",
        "vars_a_graficar = [\n",
        "    ('air_quality_PM2.5', 'pm25_log', 'PM2.5'),\n",
        "    ('precip_mm', 'precip_log', 'Precipitaci√≥n'),\n",
        "    ('population_density', 'pop_density_log', 'Densidad Poblacional'),\n",
        "    ('wind_kph', 'wind_log', 'Velocidad del Viento')\n",
        "]\n",
        "\n",
        "# Crear subplots (4 filas x 2 columnas)\n",
        "fig = make_subplots(\n",
        "    rows=4, cols=2,\n",
        "    subplot_titles=[\n",
        "        f'{nombre} Original' if i % 2 == 0 else f'{nombre} Transformada (log)'\n",
        "        for nombre in [v[2] for v in vars_a_graficar]\n",
        "        for i in range(2)\n",
        "    ],\n",
        "    vertical_spacing=0.08,\n",
        "    horizontal_spacing=0.1\n",
        ")\n",
        "\n",
        "# Generar histogramas\n",
        "for idx, (var_original, var_log, nombre) in enumerate(vars_a_graficar):\n",
        "    row = idx + 1\n",
        "\n",
        "    # Histograma original\n",
        "    fig.add_trace(\n",
        "        go.Histogram(\n",
        "            x=df_model[var_original],\n",
        "            name=f'{nombre} Original',\n",
        "            nbinsx=50,\n",
        "            marker=dict(color='rgba(255, 99, 71, 0.7)', line=dict(color='rgb(255, 99, 71)', width=1)),\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=row, col=1\n",
        "    )\n",
        "\n",
        "    # Histograma transformado\n",
        "    fig.add_trace(\n",
        "        go.Histogram(\n",
        "            x=df_model[var_log],\n",
        "            name=f'{nombre} Log',\n",
        "            nbinsx=50,\n",
        "            marker=dict(color='rgba(99, 110, 250, 0.7)', line=dict(color='rgb(99, 110, 250)', width=1)),\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=row, col=2\n",
        "    )\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Comparaci√≥n: Distribuciones Originales vs Transformadas (log)\",\n",
        "    height=1400,\n",
        "    width=1400,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6O5MtM3SGHv"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CORRELACIONES: ¬øMEJORAN CON LAS TRANSFORMACIONES?\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACI√ìN DE CORRELACIONES CON PM2.5\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Seleccionar variables relevantes\n",
        "vars_originales = ['wind_kph', 'precip_mm', 'humidity', 'cloud',\n",
        "                   'temperature_celsius', 'population_density']\n",
        "\n",
        "vars_transformadas = ['wind_log', 'precip_log', 'humidity', 'cloud',\n",
        "                      'temperature_celsius', 'pop_density_log']\n",
        "\n",
        "print(\"\\nüìä CORRELACIONES CON PM2.5 ORIGINAL:\")\n",
        "print(\"-\" * 80)\n",
        "for var in vars_originales:\n",
        "    # Usamos df_model y la columna original de PM2.5\n",
        "    corr = df_model[[var, 'air_quality_PM2.5']].corr().iloc[0, 1]\n",
        "    print(f\"{var:30s}: r = {corr:7.4f}\")\n",
        "\n",
        "print(\"\\nüìä CORRELACIONES CON PM2.5 TRANSFORMADO (log):\")\n",
        "print(\"-\" * 80)\n",
        "for var in vars_transformadas:\n",
        "    # Usamos df_model y la columna transformada de PM2.5 ('pm25_log')\n",
        "    corr = df_model[[var, 'pm25_log']].corr().iloc[0, 1]\n",
        "    print(f\"{var:30s}: r = {corr:7.4f}\")\n",
        "\n",
        "# Calcular mejora\n",
        "print(\"\\nüìà AN√ÅLISIS DE MEJORA:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "mejoras = []\n",
        "for var_orig, var_trans in zip(vars_originales, vars_transformadas):\n",
        "    if var_orig == var_trans:  # Variables no transformadas\n",
        "        continue\n",
        "\n",
        "    # Correlaci√≥n Original: variable original vs PM2.5 original\n",
        "    corr_orig = abs(df_model[[var_orig, 'air_quality_PM2.5']].corr().iloc[0, 1])\n",
        "    # Correlaci√≥n Transformada: variable transformada vs PM2.5 log\n",
        "    corr_trans = abs(df_model[[var_trans, 'pm25_log']].corr().iloc[0, 1])\n",
        "\n",
        "    mejora = corr_trans - corr_orig\n",
        "    mejoras.append((var_orig, corr_orig, corr_trans, mejora))\n",
        "\n",
        "    print(f\"{var_orig:30s}: {corr_orig:.4f} ‚Üí {corr_trans:.4f} (Œî = {mejora:+.4f})\")\n",
        "\n",
        "# Resumen\n",
        "print(\"\\nüí° CONCLUSI√ìN:\")\n",
        "mejoras_positivas = [m for m in mejoras if m[3] > 0.001] # Se usa 0.001 para ignorar cambios insignificantes\n",
        "if len(mejoras_positivas) > 0:\n",
        "    print(f\"  ‚úì {len(mejoras_positivas)} variables mejoraron su correlaci√≥n (en valor absoluto) despu√©s de transformaci√≥n logar√≠tmica.\")\n",
        "else:\n",
        "    print(\"  ‚ö† Las transformaciones logar√≠tmicas no mejoraron significativamente las correlaciones lineales.\")\n",
        "    print(\"  ‚Üí Esto confirma que las relaciones son altamente no lineales y ser√°n mejor capturadas por modelos tree-based o variables categ√≥ricas.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkY-_-TrSLP3"
      },
      "outputs": [],
      "source": [
        "# Crear heatmap comparativo\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Subset de variables transformadas\n",
        "vars_para_heatmap = ['wind_log', 'precip_log', 'humidity', 'cloud',\n",
        "                     'temperature_celsius', 'pop_density_log', 'pm25_log']\n",
        "\n",
        "# Calcular matriz de correlaci√≥n (usando df_model)\n",
        "corr_matrix_trans = df_model[vars_para_heatmap].corr()\n",
        "\n",
        "# Crear heatmap\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=corr_matrix_trans.values,\n",
        "    x=['wind_kph (log)', 'precip (log)', 'humidity', 'cloud',\n",
        "       'temperature_celsius', 'pop_density (log)', 'PM2.5 (log)'],\n",
        "    y=['wind_kph(log)', 'precip (log)', 'humidity', 'cloud',\n",
        "       'temperature_celsius', 'pop_density (log)', 'PM2.5 (log)'],\n",
        "    colorscale='RdBu',\n",
        "    zmid=0,\n",
        "    showscale=True,\n",
        "    annotation_text=np.around(corr_matrix_trans.values, decimals=2)\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Matriz de Correlaciones - Variables Transformadas',\n",
        "    width=1200,\n",
        "    height=800,\n",
        "    xaxis={'side': 'bottom'},\n",
        "    font=dict(size=11)\n",
        ")\n",
        "\n",
        "fig.update_xaxes(tickangle=-45)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV4j105sSUfO"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# INTERPRETACI√ìN ESTAD√çSTICA DE TRANSFORMACIONES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPRETACI√ìN DE TRANSFORMACIONES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for var_orig, var_log, nombre in vars_a_graficar:\n",
        "    # Usamos df_model en lugar de df_transformed\n",
        "    asim_orig = stats.skew(df_model[var_orig])\n",
        "    asim_log = stats.skew(df_model[var_log])\n",
        "    kurt_orig = stats.kurtosis(df_model[var_orig])\n",
        "    kurt_log = stats.kurtosis(df_model[var_log])\n",
        "\n",
        "    print(f\"\\nüìä {nombre.upper()}:\")\n",
        "    print(f\"  Asimetr√≠a: {asim_orig:7.2f} ‚Üí {asim_log:7.2f}\")\n",
        "    print(f\"  Curtosis:  {kurt_orig:7.2f} ‚Üí {kurt_log:7.2f}\")\n",
        "\n",
        "    if abs(asim_log) < 0.5:\n",
        "        print(f\"  ‚úì Transformaci√≥n EXITOSA: distribuci√≥n aproximadamente sim√©trica\")\n",
        "    elif abs(asim_log) < abs(asim_orig):\n",
        "        print(f\"  ~ Transformaci√≥n PARCIAL: mejora pero a√∫n asim√©trica\")\n",
        "    else:\n",
        "        print(f\"  ‚úó Transformaci√≥n NO mejora la simetr√≠a\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RECOMENDACIONES PARA MODELADO\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "1. USAR variables transformadas (log) para modelos de regresi√≥n lineal\n",
        "2. Las variables transformadas reducen el impacto de outliers extremos\n",
        "3. La normalizaci√≥n de distribuciones mejora supuestos de regresi√≥n\n",
        "4. Considerar modelos que manejen no linealidades sin transformaciones\n",
        "   (Random Forest, Gradient Boosting)\n",
        "5. Para interpretaci√≥n, convertir predicciones de log a escala original: exp(pred) - 1\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwYlcFwglYlY"
      },
      "source": [
        "### **7.2 VALIDACI√ìN FINAL Y SELECCI√ìN DE FEATURES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDyyDj3qtayr"
      },
      "source": [
        "#### **7.2.1 VALIDACI√ìN VIF (MULTICOLINEALIDAD)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuPGPDY_sBhQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# ============================================================================\n",
        "# VALIDACI√ìN VIF SOBRE FEATURES NUM√âRICAS FINALES (df_model)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VALIDACI√ìN VIF - FEATURES NUM√âRICAS FINALES PARA MODELADO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Incluimos S√ìLO las variables num√©ricas que se usar√°n en el modelo BASELINE.\n",
        "numeric_features_final = [\n",
        "    'wind_log', 'pop_density_log', 'temperature_celsius',\n",
        "    'humidity', 'cloud', 'pressure_mb', 'uv_index'\n",
        "]\n",
        "\n",
        "# Usamos df_model que contiene las features limpias y transformadas.\n",
        "X_numeric_final = df_model[numeric_features_final].copy()\n",
        "\n",
        "# Crear el DataFrame para los resultados del VIF\n",
        "vif_data_final = pd.DataFrame()\n",
        "vif_data_final[\"Feature\"] = X_numeric_final.columns\n",
        "\n",
        "# Calcular VIF\n",
        "vif_data_final[\"VIF\"] = [\n",
        "    variance_inflation_factor(X_numeric_final.values, i)\n",
        "    for i in range(len(X_numeric_final.columns))\n",
        "]\n",
        "\n",
        "vif_data_final = vif_data_final.sort_values(by=\"VIF\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nüö® RESULTADOS VIF SOBRE EL CONJUNTO FINAL:\")\n",
        "print(\"CRITERIO: VIF < 5 (Ideal); VIF < 10 (Aceptable)\")\n",
        "print(\"-\" * 55)\n",
        "print(vif_data_final.to_string(index=False))\n",
        "print(\"-\" * 55)\n",
        "\n",
        "# ============================================================================\n",
        "# CONCLUSI√ìN Y REPORTE\n",
        "# ============================================================================\n",
        "\n",
        "max_vif = vif_data_final['VIF'].max()\n",
        "\n",
        "if max_vif > 10:\n",
        "    print(\"\\n\\n‚ùå ADVERTENCIA: Multicolinealidad ALTA detectada (VIF > 10).\")\n",
        "    print(\"   ‚Üí Se debe eliminar la variable con el VIF m√°s alto o aplicar regularizaci√≥n (Ridge/Lasso).\")\n",
        "elif max_vif > 5:\n",
        "    print(\"\\n\\n‚ö†Ô∏è  CONCLUSI√ìN: Multicolinealidad MODERADA (VIF 5-10).\")\n",
        "    print(\"   ‚Üí Aceptable. Se recomienda usar regularizaci√≥n (Ridge o Lasso) para asegurar la estabilidad de la regresi√≥n.\")\n",
        "else:\n",
        "    print(\"\\n\\n‚úÖ CONCLUSI√ìN: Multicolinealidad BAJA (VIF < 5).\")\n",
        "    print(\"   ‚Üí Excelente. Las features num√©ricas son estables y aptas para Regresi√≥n Lineal.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ VALIDACI√ìN DE MULTICOLINEALIDAD COMPLETA\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_erTFEy0s7kv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# ============================================================================\n",
        "# 8. ITERACI√ìN VIF 2: ELIMINANDO PRESSURE_MB (USANDO df_model)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"VIF - ITERACI√ìN 2: SIN pressure_mb (Calculado sobre df_model)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Lista de features num√©ricas restantes\n",
        "numeric_features_iter2 = [\n",
        "    'wind_log', 'pop_density_log', 'temperature_celsius',\n",
        "    'humidity', 'cloud', 'uv_index'  # pressure_mb eliminado\n",
        "]\n",
        "\n",
        "# Usamos df_model como fuente de datos para obtener las features transformadas\n",
        "X_numeric_iter2 = df_model[numeric_features_iter2].copy()\n",
        "\n",
        "# Recalcular VIF\n",
        "vif_data_iter2 = pd.DataFrame()\n",
        "vif_data_iter2[\"Feature\"] = X_numeric_iter2.columns\n",
        "vif_data_iter2[\"VIF\"] = [\n",
        "    variance_inflation_factor(X_numeric_iter2.values, i)\n",
        "    for i in range(len(X_numeric_iter2.columns))\n",
        "]\n",
        "\n",
        "vif_data_iter2 = vif_data_iter2.sort_values(by=\"VIF\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nüö® RESULTADOS VIF (Iteraci√≥n 2):\")\n",
        "print(\"-\" * 55)\n",
        "print(vif_data_iter2.to_string(index=False))\n",
        "print(\"-\" * 55)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFNYtPZ2v5hQ"
      },
      "source": [
        "#### **7.2.2 CORRECCI√ìN VIF, CREACI√ìN DE df_ml Y LISTADO FINAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4e6ADM-v_bH"
      },
      "outputs": [],
      "source": [
        "# CORRECCI√ìN VIF, CREACI√ìN DE df_ml Y LISTADO FINAL\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"APLICANDO CORRECCI√ìN FINAL Y CREANDO DATASET DE MODELADO (df_ml)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# DECISI√ìN FINAL:\n",
        "# 1. ELIMINAR pressure_mb (VIF cr√≠tico 47.23).\n",
        "# 2. MANTENER wind_log (VIF 12.38) y pop_density_log (VIF 10.59).\n",
        "#    La multicolinealidad restante ser√° mitigada usando Regresi√≥n Ridge/Lasso.\n",
        "\n",
        "# 1. Definir la lista final de columnas seleccionadas\n",
        "columnas_finales_seleccionadas = [\n",
        "    'pm25_log', 'wind_log', 'pop_density_log', 'temperature_celsius', 'humidity',\n",
        "    'cloud', 'uv_index', 'precip_log', 'rained', 'high_wind', 'month', 'hour',\n",
        "    'country'\n",
        "]\n",
        "\n",
        "# 2. Crear el DataFrame final de Machine Learning (df_ml)\n",
        "# Este es el subset que se usar√° en la siguiente fase.\n",
        "df_ml = df_model[columnas_finales_seleccionadas].copy()\n",
        "\n",
        "# 3. Guardar la lista de features para el Preprocesador (Excluyendo target y country)\n",
        "features_X_baseline = [col for col in columnas_finales_seleccionadas\n",
        "                       if col not in ['pm25_log', 'country']]\n",
        "\n",
        "print(f\"‚úÖ Columna 'pressure_mb' eliminada del conjunto final.\")\n",
        "print(f\"   df_ml creado con dimensiones: {df_ml.shape}\")\n",
        "print(f\"   Total de Features para Baseline (X): {len(features_X_baseline)}\")\n",
        "\n",
        "print(\"\\nFeatures Num√©ricas (para Scaling/VIF):\")\n",
        "print([f for f in features_X_baseline if f not in ['rained', 'high_wind', 'month', 'hour']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBaPWzq4wHKX"
      },
      "source": [
        "#### **7.2.3 DEFINICI√ìN DEL PREPROCESADOR (ENCODING)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKRnf8AdwagZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# ============================================================================\n",
        "# 7.3.3 DEFINICI√ìN DEL PREPROCESADOR PARA MODELADO BASELINE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DEFINICI√ìN DEL PIPELINE DE PREPROCESAMIENTO (ColumnTransformer)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Las listas de features finales, sin 'pressure_mb'.\n",
        "categorical_features = ['month', 'hour', 'country']\n",
        "numeric_features = [\n",
        "    'wind_log', 'pop_density_log', 'temperature_celsius',\n",
        "    'humidity', 'cloud', 'uv_index', 'precip_log'\n",
        "]\n",
        "binary_features = ['rained', 'high_wind']\n",
        "\n",
        "\n",
        "# Crear el ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot',\n",
        "         OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'),\n",
        "         categorical_features),  # ‚Üê Ahora incluye 'country'\n",
        "        ('scaling',\n",
        "         StandardScaler(),\n",
        "         numeric_features),\n",
        "        ('passthrough',\n",
        "         'passthrough',\n",
        "         binary_features)\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "print(\"üìù REGLAS ESTABLECIDAS PARA EL MODELO BASELINE:\")\n",
        "print(f\"   ‚Ä¢ Categ√≥ricas ({len(categorical_features)}): {categorical_features}\")\n",
        "print(f\"   ‚Ä¢ Num√©ricas ({len(numeric_features)}): {numeric_features}\")\n",
        "print(f\"   ‚Ä¢ Binarias ({len(binary_features)}): {binary_features}\")\n",
        "print(f\"   ‚Ä¢ TOTAL FEATURES INPUT: {len(categorical_features) + len(numeric_features) + len(binary_features)}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZmKopUbCY0t"
      },
      "source": [
        "#### **7.2.4 ESTRATEGIA DE MITIGACI√ìN DE MULTICOLINEALIDAD**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TQ-6MALCnew"
      },
      "source": [
        "Dado que `wind_log` (VIF=12.38) y `pop_density_log` (VIF=10.59) presentan\n",
        "multicolinealidad moderada, se establece la siguiente estrategia para Entrega 2:\n",
        "\n",
        "**T√©cnicas a implementar:**\n",
        "1. **Regularizaci√≥n L2 (Ridge)**: Œ± = 10.0 para penalizar coeficientes grandes\n",
        "2. **Regularizaci√≥n L1 (Lasso)**: Œ± = 0.1 para selecci√≥n autom√°tica de features\n",
        "3. **Elastic Net**: Combinaci√≥n de L1 y L2 para balancear ambos enfoques\n",
        "\n",
        "**Justificaci√≥n**: Mantener ambas variables es preferible a eliminarlas,\n",
        "ya que capturan efectos complementarios (meteorol√≥gico vs antropog√©nico)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqjPRunB3fLm"
      },
      "source": [
        "# **8. S√çNTESIS DE HALLAZGOS Y CONCLUSIONES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23INdTDz6V5"
      },
      "source": [
        "---\n",
        "\n",
        "**Esta secci√≥n finaliza la primera fase del proyecto (An√°lisis y Feature Engineering), confrontando los hallazgos con las preguntas e hip√≥tesis de investigaci√≥n iniciales y validando la preparaci√≥n del *dataset* para el Modelado.**\n",
        "\n",
        "---\n",
        "\n",
        "## **8.1 VALIDACI√ìN DE HIP√ìTESIS (AN√ÅLISIS Y FEATURE ENGINEERING)**\n",
        "\n",
        "Se eval√∫a hasta qu√© punto el an√°lisis de datos inicial y las transformaciones de *features* han validado las premisas b√°sicas de las preguntas de investigaci√≥n.\n",
        "\n",
        "| Pregunta / Hip√≥tesis | Hallazgo de la Fase I (An√°lisis y FE) | Validaci√≥n |\n",
        "| :--- | :--- | :--- |\n",
        "| **P1: Factores Predictivos** | El an√°lisis **VIF** identific√≥ que `pressure_mb` (VIF $\\approx 47.23$) es la *feature* m√°s **redundante**. Las transformaciones de `wind` y `poblaci√≥n` mejoraron su asociaci√≥n lineal. | **Parcialmente Validada.** Se confirma la complejidad de los factores, y el proceso de FE ha aislado a los candidatos m√°s fuertes. |\n",
        "| **H1.1: Relaciones no lineales** | La **asimetr√≠a extrema** en el *target* ($\\text{PM2.5}: 4.34$) y *features* como `densidad poblacional` ($\\approx 11.16$) confirman la existencia de relaciones fuertemente no lineales. | **Validada.** La no linealidad fue el principal motor de la transformaci√≥n de datos. |\n",
        "| **H1.3: Transformaciones logar√≠tmicas** | La transformaci√≥n $\\text{log1p}$ neutraliz√≥ la asimetr√≠a del *target* ($\\text{PM2.5}: 4.34 \\to \\mathbf{-0.01}$) y mejor√≥ la correlaci√≥n lineal de `pop_density` con el *target* ($\\mathbf{\\Delta \\approx +0.11}$). | **Validada.** Las transformaciones son un paso exitoso para los modelos lineales. |\n",
        "| **H3.1: Patrones Geogr√°ficos (Densidad)** | La variable **`pop_density_log`** se mantiene en el *dataset* final a pesar de un VIF moderado ($\\approx 10.59$), lo que confirma su rol como *proxy* de la influencia **antropog√©nica** en entornos urbanos. | **Validada.** El factor socioecon√≥mico se confirma como un *feature* clave. |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfNpwh4X3v-q"
      },
      "source": [
        "## **8.2 CONCLUSIONES SOBRE LA PREPARACI√ìN DEL DATASET**\n",
        "\n",
        "#### Respuesta a P1: Factores con mayor potencial predictivo (en el contexto lineal)\n",
        "Los hallazgos de la Fase I indican que las **versiones transformadas y validadas** de las variables son las que tienen mayor potencial predictivo:\n",
        "* **Factores Socioecon√≥micos:** La transformaci√≥n de $\\text{log}(\\text{Densidad Poblacional})$ fue la que m√°s mejor√≥ su correlaci√≥n lineal.\n",
        "* **Factores Meteorol√≥gicos Clave:** **`humidity`** y **`cloud`** mostraron las correlaciones absolutas m√°s fuertes.\n",
        "* **Factores Descartados:** La **eliminaci√≥n** de la variable `pressure_mb` corrigi√≥ la multicolinealidad cr√≠tica.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0NExRkn39gI"
      },
      "source": [
        "## **8.3 CONCLUSI√ìN GENERAL DE LA ENTREGA**\n",
        "\n",
        "La fase de **An√°lisis y Feature Engineering** ha finalizado, proporcionando una base de datos **s√≥lida, limpia y validada** que responde a las hip√≥tesis iniciales de la complejidad y no linealidad de las relaciones. El *DataFrame* **`df_ml`** y la estrategia de preprocesamiento est√°n completamente listos y optimizados para iniciar la fase de **Modelado Baseline**, que abordar√° las preguntas sobre la Capacidad Predictiva (P2, P3 y P4) en la siguiente entrega.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uCsXyCM32uF"
      },
      "source": [
        "## **CONCLUSI√ìN SOBRE EL DATASET FINAL**\n",
        "El proceso de **Feature Engineering** se completa con √©xito, dejando un *dataset* validado y optimizado.\n",
        "\n",
        "1.  **Neutralizaci√≥n del Sesgo:** La transformaci√≥n logar√≠tmica de la variable *target* y las *features* num√©ricas mejora la **estabilidad y la interpretabilidad** del modelo.\n",
        "2.  **Mitigaci√≥n de la Redundancia:** El an√°lisis VIF corrigi√≥ el problema cr√≠tico de la multicolinealidad, resultando en un *DataFrame* final, **`df_ml`**, libre de variables redundantes.\n",
        "3.  **Estrategia de Modelado Lista:** La definici√≥n del **`ColumnTransformer`** establece la estrategia para el *baseline*: **Estandarizaci√≥n** para num√©ricas y **One-Hot Encoding** para categ√≥ricas, asegurando que la pr√≥xima fase de modelado se inicie de forma limpia y sin *data leakage*.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJVvIKkLkGEO"
      },
      "source": [
        "# **9. PREPARACI√ìN PARA MODELADO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_RVsaJUleHp"
      },
      "source": [
        "## **9.1 IMPORTAR LIBRER√çAS DE MACHINE LEARNING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0iIQ2_TlNlj"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüìö Importando librer√≠as de Machine Learning...\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor, VotingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0alOzAGlVxy"
      },
      "source": [
        "## **9.2 VERIFICAR DATASET FINAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-YjGfoOlSow"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VERIFICACI√ìN DEL DATASET FINAL (df_ml)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nüìä Dimensiones de df_ml: {df_ml.shape}\")\n",
        "print(f\"   - Registros: {df_ml.shape[0]:,}\")\n",
        "print(f\"   - Columnas: {df_ml.shape[1]}\")\n",
        "\n",
        "print(\"\\nüìã Columnas disponibles:\")\n",
        "for i, col in enumerate(df_ml.columns, 1):\n",
        "    print(f\"   {i:2d}. {col:25s} ({df_ml[col].dtype})\")\n",
        "\n",
        "print(\"\\nüéØ Variable Target: pm25_log\")\n",
        "print(f\"   - Media: {df_ml['pm25_log'].mean():.4f}\")\n",
        "print(f\"   - Mediana: {df_ml['pm25_log'].median():.4f}\")\n",
        "print(f\"   - Desv. Std: {df_ml['pm25_log'].std():.4f}\")\n",
        "print(f\"   - Min: {df_ml['pm25_log'].min():.4f}\")\n",
        "print(f\"   - Max: {df_ml['pm25_log'].max():.4f}\")\n",
        "\n",
        "# Verificar valores nulos\n",
        "print(\"\\nüîç Verificaci√≥n de valores nulos:\")\n",
        "nulos = df_ml.isnull().sum()\n",
        "if nulos.sum() == 0:\n",
        "    print(\"   ‚úÖ No hay valores nulos en el dataset\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  Valores nulos detectados:\")\n",
        "    print(nulos[nulos > 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UrPhugMmuLE"
      },
      "source": [
        "## **9.3 SEPARAR FEATURES (X) Y TARGET (y)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebpEacmsmr71"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SEPARACI√ìN DE FEATURES Y TARGET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Separar X (features) e y (target)\n",
        "X = df_ml.drop(columns=['pm25_log'])\n",
        "y = df_ml['pm25_log']\n",
        "\n",
        "print(f\"\\n‚úÖ X (Features): {X.shape}\")\n",
        "print(f\"‚úÖ y (Target): {y.shape}\")\n",
        "\n",
        "print(\"\\nüìã Features incluidas en X:\")\n",
        "print(f\"   - Num√©ricas continuas (7): {[f for f in X.columns if f in numeric_features]}\")\n",
        "print(f\"   - Binarias (2): {[f for f in X.columns if f in binary_features]}\")\n",
        "print(f\"   - Categ√≥ricas (3): {[f for f in X.columns if f in categorical_features]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXd-prmLnA72"
      },
      "source": [
        "## **9.4 SPLIT TRAIN/TEST (80/20 ESTRATIFICADO)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyij35lQm-CJ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SPLIT TRAIN/TEST (80/20)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Split estratificado por cuartiles de PM2.5 para mantener distribuci√≥n\n",
        "y_quartiles = pd.qcut(y, q=4, labels=False, duplicates='drop')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=y_quartiles\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Tama√±os de los conjuntos:\")\n",
        "print(f\"   - Train: {X_train.shape[0]:,} registros ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   - Test:  {X_test.shape[0]:,} registros ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüéØ Distribuci√≥n del target (pm25_log):\")\n",
        "print(f\"   {'Conjunto':<10} {'Media':<10} {'Mediana':<10} {'Desv.Std':<10}\")\n",
        "print(f\"   {'-'*40}\")\n",
        "print(f\"   {'Original':<10} {y.mean():<10.4f} {y.median():<10.4f} {y.std():<10.4f}\")\n",
        "print(f\"   {'Train':<10} {y_train.mean():<10.4f} {y_train.median():<10.4f} {y_train.std():<10.4f}\")\n",
        "print(f\"   {'Test':<10} {y_test.mean():<10.4f} {y_test.median():<10.4f} {y_test.std():<10.4f}\")\n",
        "\n",
        "# Verificar que la estratificaci√≥n funcion√≥\n",
        "print(\"\\n‚úÖ La estratificaci√≥n mantiene distribuciones similares entre train y test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-uqJc5WnV25"
      },
      "source": [
        "## **9.5 VISUALIZAR DISTRIBUCI√ìN TRAIN VS TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "170dpBzenTsp"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VISUALIZACI√ìN: DISTRIBUCI√ìN TRAIN VS TEST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Histogram(\n",
        "    x=y_train,\n",
        "    name='Train',\n",
        "    nbinsx=50,\n",
        "    marker=dict(color='rgba(99, 110, 250, 0.6)', line=dict(color='rgb(99, 110, 250)', width=1)),\n",
        "    opacity=0.7\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Histogram(\n",
        "    x=y_test,\n",
        "    name='Test',\n",
        "    nbinsx=50,\n",
        "    marker=dict(color='rgba(255, 99, 71, 0.6)', line=dict(color='rgb(255, 99, 71)', width=1)),\n",
        "    opacity=0.7\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Distribuci√≥n de PM2.5 (log) en Train vs Test',\n",
        "    xaxis_title='pm25_log',\n",
        "    yaxis_title='Frecuencia',\n",
        "    barmode='overlay',\n",
        "    height=500,\n",
        "    width=1000,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"\\n‚úÖ PREPARACI√ìN COMPLETADA - Dataset listo para modelado\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyusiDfZof23"
      },
      "source": [
        "# **10. ENTRENAMIENTO DE MODELOS BASELINE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaJcSfSPpyZ4"
      },
      "source": [
        "## **10.1 DEFINICI√ìN DE FUNCI√ìN AUXILIAR PARA M√âTRICAS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3-QHZQropa1"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüìä Definiendo funci√≥n de evaluaci√≥n de m√©tricas...\")\n",
        "\n",
        "def evaluar_modelo(y_true, y_pred, nombre_modelo=\"Modelo\", log_scale=True):\n",
        "    \"\"\"\n",
        "    Calcula y muestra m√©tricas de evaluaci√≥n para modelos de regresi√≥n.\n",
        "\n",
        "    Par√°metros:\n",
        "    -----------\n",
        "    y_true : array-like\n",
        "        Valores reales del target\n",
        "    y_pred : array-like\n",
        "        Valores predichos por el modelo\n",
        "    nombre_modelo : str\n",
        "        Nombre del modelo para mostrar en los resultados\n",
        "    log_scale : bool\n",
        "        Si True, convierte predicciones de escala log a escala original para m√©tricas adicionales\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "    dict : Diccionario con todas las m√©tricas calculadas\n",
        "    \"\"\"\n",
        "\n",
        "    # M√©tricas en escala logar√≠tmica\n",
        "    mae_log = mean_absolute_error(y_true, y_pred)\n",
        "    mse_log = mean_squared_error(y_true, y_pred)\n",
        "    rmse_log = np.sqrt(mse_log)\n",
        "    r2_log = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Mostrar resultados en escala logar√≠tmica\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"METRICAS DE EVALUACION: {nombre_modelo}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(f\"\\nMetricas en Escala Logaritmica:\")\n",
        "    print(f\"   MAE (log):  {mae_log:.4f}\")\n",
        "    print(f\"   MSE (log):  {mse_log:.4f}\")\n",
        "    print(f\"   RMSE (log): {rmse_log:.4f}\")\n",
        "    print(f\"   R2 (log):   {r2_log:.4f}\")\n",
        "\n",
        "    # Retornar m√©tricas en un diccionario\n",
        "    metricas = {\n",
        "        'MAE_log': mae_log,\n",
        "        'MSE_log': mse_log,\n",
        "        'RMSE_log': rmse_log,\n",
        "        'R2_log': r2_log\n",
        "    }\n",
        "\n",
        "    # Convertir a escala original si est√° en log\n",
        "    if log_scale:\n",
        "        y_true_original = np.expm1(y_true)  # exp(x) - 1 (inverso de log1p)\n",
        "        y_pred_original = np.expm1(y_pred)\n",
        "\n",
        "        mae_original = mean_absolute_error(y_true_original, y_pred_original)\n",
        "        rmse_original = np.sqrt(mean_squared_error(y_true_original, y_pred_original))\n",
        "        mape = mean_absolute_percentage_error(y_true_original, y_pred_original) * 100\n",
        "        r2_original = r2_score(y_true_original, y_pred_original)\n",
        "\n",
        "        print(f\"\\nMetricas en Escala Original (PM2.5 ug/m3):\")\n",
        "        print(f\"   MAE:        {mae_original:.2f} ug/m3\")\n",
        "        print(f\"   RMSE:       {rmse_original:.2f} ug/m3\")\n",
        "        print(f\"   MAPE:       {mape:.2f}%\")\n",
        "        print(f\"   R2:         {r2_original:.4f}\")\n",
        "\n",
        "        metricas.update({\n",
        "            'MAE_original': mae_original,\n",
        "            'RMSE_original': rmse_original,\n",
        "            'MAPE': mape,\n",
        "            'R2_original': r2_original\n",
        "        })\n",
        "\n",
        "    return metricas\n",
        "\n",
        "print(\"‚úÖ Funci√≥n de evaluaci√≥n definida\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCJSKPz7qDUs"
      },
      "source": [
        "## **10.2 MODELO 1: RIDGE REGRESSION (BASELINE LINEAL CON REGULARIZACI√ìN)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEPGbbOgp8Zd"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODELO 1: RIDGE REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüîß Configurando pipeline Ridge...\")\n",
        "print(\"   - Regularizaci√≥n L2 para mitigar multicolinealidad\")\n",
        "print(\"   - Alpha inicial: 10.0\")\n",
        "\n",
        "# Crear pipeline Ridge\n",
        "pipeline_ridge = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', Ridge(alpha=10.0, random_state=42))\n",
        "])\n",
        "\n",
        "# Entrenar modelo\n",
        "print(\"\\nüöÄ Entrenando Ridge Regression...\")\n",
        "pipeline_ridge.fit(X_train, y_train)\n",
        "print(\"‚úÖ Entrenamiento completado\")\n",
        "\n",
        "# Predicciones\n",
        "y_pred_ridge_train = pipeline_ridge.predict(X_train)\n",
        "y_pred_ridge_test = pipeline_ridge.predict(X_test)\n",
        "\n",
        "# Evaluaci√≥n en TRAIN\n",
        "print(\"\\nüìà EVALUACI√ìN EN TRAIN:\")\n",
        "metricas_ridge_train = evaluar_modelo(y_train, y_pred_ridge_train, \"Ridge - Train\")\n",
        "\n",
        "# Evaluaci√≥n en TEST\n",
        "print(\"\\nüìà EVALUACI√ìN EN TEST:\")\n",
        "metricas_ridge_test = evaluar_modelo(y_test, y_pred_ridge_test, \"Ridge - Test\")\n",
        "\n",
        "# Verificar overfitting\n",
        "diff_r2 = metricas_ridge_train['R2_log'] - metricas_ridge_test['R2_log']\n",
        "print(f\"\\nüîç An√°lisis de Overfitting:\")\n",
        "print(f\"   Diferencia R¬≤ (Train - Test): {diff_r2:.4f}\")\n",
        "if diff_r2 < 0.05:\n",
        "    print(\"   ‚úÖ No hay overfitting significativo\")\n",
        "elif diff_r2 < 0.10:\n",
        "    print(\"   ‚ö†Ô∏è  Overfitting leve\")\n",
        "else:\n",
        "    print(\"   ‚ùå Overfitting significativo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhqJvYtVqZ0A"
      },
      "source": [
        "## **10.3 MODELO 2: RANDOM FOREST REGRESSOR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcuwAS9mqXrS"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODELO 2: RANDOM FOREST REGRESSOR\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüîß Configurando pipeline Random Forest...\")\n",
        "print(\"   - Modelo ensemble basado en √°rboles\")\n",
        "print(\"   - Hiperpar√°metros iniciales:\")\n",
        "print(\"     ‚Ä¢ n_estimators: 100\")\n",
        "print(\"     ‚Ä¢ max_depth: 20\")\n",
        "print(\"     ‚Ä¢ min_samples_split: 10\")\n",
        "print(\"     ‚Ä¢ min_samples_leaf: 4\")\n",
        "\n",
        "# Crear pipeline Random Forest\n",
        "pipeline_rf = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=20,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=4,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Entrenar modelo\n",
        "print(\"\\nüöÄ Entrenando Random Forest...\")\n",
        "pipeline_rf.fit(X_train, y_train)\n",
        "print(\"‚úÖ Entrenamiento completado\")\n",
        "\n",
        "# Predicciones\n",
        "y_pred_rf_train = pipeline_rf.predict(X_train)\n",
        "y_pred_rf_test = pipeline_rf.predict(X_test)\n",
        "\n",
        "# Evaluaci√≥n en TRAIN\n",
        "print(\"\\nüìà EVALUACI√ìN EN TRAIN:\")\n",
        "metricas_rf_train = evaluar_modelo(y_train, y_pred_rf_train, \"Random Forest - Train\")\n",
        "\n",
        "# Evaluaci√≥n en TEST\n",
        "print(\"\\nüìà EVALUACI√ìN EN TEST:\")\n",
        "metricas_rf_test = evaluar_modelo(y_test, y_pred_rf_test, \"Random Forest - Test\")\n",
        "\n",
        "# Verificar overfitting\n",
        "diff_r2_rf = metricas_rf_train['R2_log'] - metricas_rf_test['R2_log']\n",
        "print(f\"\\nüîç An√°lisis de Overfitting:\")\n",
        "print(f\"   Diferencia R¬≤ (Train - Test): {diff_r2_rf:.4f}\")\n",
        "if diff_r2_rf < 0.05:\n",
        "    print(\"   ‚úÖ No hay overfitting significativo\")\n",
        "elif diff_r2_rf < 0.10:\n",
        "    print(\"   ‚ö†Ô∏è  Overfitting leve\")\n",
        "else:\n",
        "    print(\"   ‚ùå Overfitting significativo - Considerar regularizaci√≥n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwCjG4LSsrtO"
      },
      "source": [
        "## **10.4 MODELO 3: XGBOOST REGRESSOR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNgmeKTAsp3y"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODELO 3: XGBOOST REGRESSOR\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüîß Configurando pipeline XGBoost...\")\n",
        "print(\"   - Gradient Boosting optimizado\")\n",
        "print(\"   - Hiperpar√°metros iniciales:\")\n",
        "print(\"     ‚Ä¢ n_estimators: 100\")\n",
        "print(\"     ‚Ä¢ max_depth: 6\")\n",
        "print(\"     ‚Ä¢ learning_rate: 0.1\")\n",
        "print(\"     ‚Ä¢ subsample: 0.8\")\n",
        "\n",
        "# Crear pipeline XGBoost\n",
        "pipeline_xgb = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', xgb.XGBRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Entrenar modelo\n",
        "print(\"\\nüöÄ Entrenando XGBoost...\")\n",
        "pipeline_xgb.fit(X_train, y_train)\n",
        "print(\"‚úÖ Entrenamiento completado\")\n",
        "\n",
        "# Predicciones\n",
        "y_pred_xgb_train = pipeline_xgb.predict(X_train)\n",
        "y_pred_xgb_test = pipeline_xgb.predict(X_test)\n",
        "\n",
        "# Evaluaci√≥n en TRAIN\n",
        "print(\"\\nüìà EVALUACI√ìN EN TRAIN:\")\n",
        "metricas_xgb_train = evaluar_modelo(y_train, y_pred_xgb_train, \"XGBoost - Train\")\n",
        "\n",
        "# Evaluaci√≥n en TEST\n",
        "print(\"\\nüìà EVALUACI√ìN EN TEST:\")\n",
        "metricas_xgb_test = evaluar_modelo(y_test, y_pred_xgb_test, \"XGBoost - Test\")\n",
        "\n",
        "# Verificar overfitting\n",
        "diff_r2_xgb = metricas_xgb_train['R2_log'] - metricas_xgb_test['R2_log']\n",
        "print(f\"\\nüîç An√°lisis de Overfitting:\")\n",
        "print(f\"   Diferencia R¬≤ (Train - Test): {diff_r2_xgb:.4f}\")\n",
        "if diff_r2_xgb < 0.05:\n",
        "    print(\"   ‚úÖ No hay overfitting significativo\")\n",
        "elif diff_r2_xgb < 0.10:\n",
        "    print(\"   ‚ö†Ô∏è  Overfitting leve\")\n",
        "else:\n",
        "    print(\"   ‚ùå Overfitting significativo - Considerar early stopping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEmWlY2As-Lx"
      },
      "source": [
        "## **10.5 COMPARACI√ìN VISUAL DE MODELOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC0azO7Os8HP"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACI√ìN VISUAL DE MODELOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear DataFrame de comparaci√≥n\n",
        "comparacion = pd.DataFrame({\n",
        "    'Modelo': ['Ridge', 'Random Forest', 'XGBoost'],\n",
        "    'R2_Train': [\n",
        "        metricas_ridge_train['R2_log'],\n",
        "        metricas_rf_train['R2_log'],\n",
        "        metricas_xgb_train['R2_log']\n",
        "    ],\n",
        "    'R2_Test': [\n",
        "        metricas_ridge_test['R2_log'],\n",
        "        metricas_rf_test['R2_log'],\n",
        "        metricas_xgb_test['R2_log']\n",
        "    ],\n",
        "    'RMSE_Test': [\n",
        "        metricas_ridge_test['RMSE_log'],\n",
        "        metricas_rf_test['RMSE_log'],\n",
        "        metricas_xgb_test['RMSE_log']\n",
        "    ],\n",
        "    'MAE_Test': [\n",
        "        metricas_ridge_test['MAE_log'],\n",
        "        metricas_rf_test['MAE_log'],\n",
        "        metricas_xgb_test['MAE_log']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä TABLA COMPARATIVA DE MODELOS:\")\n",
        "print(comparacion.to_string(index=False))\n",
        "\n",
        "# Gr√°fico de barras comparativo - R¬≤\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    subplot_titles=(\"R¬≤ Score (Train vs Test)\", \"RMSE en Test (log scale)\")\n",
        ")\n",
        "\n",
        "# Subplot 1: R¬≤ Train vs Test\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        name='R¬≤ Train',\n",
        "        x=comparacion['Modelo'],\n",
        "        y=comparacion['R2_Train'],\n",
        "        marker=dict(color='rgba(99, 110, 250, 0.7)'),\n",
        "        text=comparacion['R2_Train'].round(4),\n",
        "        textposition='outside'\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        name='R¬≤ Test',\n",
        "        x=comparacion['Modelo'],\n",
        "        y=comparacion['R2_Test'],\n",
        "        marker=dict(color='rgba(255, 99, 71, 0.7)'),\n",
        "        text=comparacion['R2_Test'].round(4),\n",
        "        textposition='outside'\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Subplot 2: RMSE Test\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        name='RMSE Test',\n",
        "        x=comparacion['Modelo'],\n",
        "        y=comparacion['RMSE_Test'],\n",
        "        marker=dict(color='rgba(99, 110, 250, 0.7)'),\n",
        "        text=comparacion['RMSE_Test'].round(4),\n",
        "        textposition='outside',\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Comparaci√≥n de Modelos Baseline\",\n",
        "    height=500,\n",
        "    width=1400,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white',\n",
        "    barmode='group'\n",
        ")\n",
        "\n",
        "fig.update_yaxes(title_text=\"R¬≤ Score\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"RMSE (log)\", row=1, col=2)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Identificar mejor modelo\n",
        "mejor_modelo_idx = comparacion['R2_Test'].idxmax()\n",
        "mejor_modelo = comparacion.iloc[mejor_modelo_idx]['Modelo']\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO BASELINE (por R¬≤ en Test): {mejor_modelo}\")\n",
        "print(f\"   R¬≤ Test: {comparacion.iloc[mejor_modelo_idx]['R2_Test']:.4f}\")\n",
        "print(f\"   RMSE Test: {comparacion.iloc[mejor_modelo_idx]['RMSE_Test']:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ ENTRENAMIENTO DE MODELOS BASELINE COMPLETADO\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HacvYDMHtQ1y"
      },
      "source": [
        "## **10.6 VISUALIZACI√ìN: PREDICCIONES VS VALORES REALES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zibZ_jK-tOw-"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VISUALIZACI√ìN: PREDICCIONES VS VALORES REALES (TEST SET)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear subplots para los 3 modelos\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=(\"Ridge Regression\", \"Random Forest\", \"XGBoost\")\n",
        ")\n",
        "\n",
        "# Funci√≥n auxiliar para agregar scatter plot\n",
        "def add_scatter(fig, y_true, y_pred, row, col, name):\n",
        "    # Scatter plot\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=y_true,\n",
        "            y=y_pred,\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                color='rgba(99, 110, 250, 0.5)',\n",
        "                size=3\n",
        "            ),\n",
        "            name=name,\n",
        "            showlegend=False\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "    # L√≠nea de referencia perfecta (y=x)\n",
        "    min_val = min(y_true.min(), y_pred.min())\n",
        "    max_val = max(y_true.max(), y_pred.max())\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=[min_val, max_val],\n",
        "            y=[min_val, max_val],\n",
        "            mode='lines',\n",
        "            line=dict(color='red', dash='dash', width=2),\n",
        "            name='Predicci√≥n perfecta',\n",
        "            showlegend=(col == 1)  # Solo mostrar leyenda en el primer gr√°fico\n",
        "        ),\n",
        "        row=row, col=col\n",
        "    )\n",
        "\n",
        "# Agregar cada modelo\n",
        "add_scatter(fig, y_test, y_pred_ridge_test, 1, 1, \"Ridge\")\n",
        "add_scatter(fig, y_test, y_pred_rf_test, 1, 2, \"Random Forest\")\n",
        "add_scatter(fig, y_test, y_pred_xgb_test, 1, 3, \"XGBoost\")\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Predicciones vs Valores Reales - Comparaci√≥n de Modelos\",\n",
        "    height=500,\n",
        "    width=1500,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "# Actualizar ejes\n",
        "for i in range(1, 4):\n",
        "    fig.update_xaxes(title_text=\"PM2.5 Real (log)\", row=1, col=i)\n",
        "    fig.update_yaxes(title_text=\"PM2.5 Predicho (log)\", row=1, col=i)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"‚úÖ Visualizaci√≥n completada\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLZRwnFVujmq"
      },
      "source": [
        "# **11. VALIDACI√ìN CRUZADA Y COMPARACI√ìN ROBUSTA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqCceVoDupyg"
      },
      "source": [
        "## **11.1 CONFIGURACI√ìN DE VALIDACI√ìN CRUZADA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLdwQ9XqunQS"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURACI√ìN DE K-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Definir estrategia de validaci√≥n cruzada\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"\\nConfiguraci√≥n:\")\n",
        "print(f\"   ‚Ä¢ N√∫mero de folds: 5\")\n",
        "print(f\"   ‚Ä¢ Shuffle: True\")\n",
        "print(f\"   ‚Ä¢ Random state: 42\")\n",
        "print(f\"   ‚Ä¢ Tama√±o de cada fold (aprox): {len(X_train)//5:,} registros\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU4UX5dju5qo"
      },
      "source": [
        "## **11.2 VALIDACI√ìN CRUZADA - RIDGE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTp7RGF0u3Qf"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDACI√ìN CRUZADA: RIDGE REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nEjecutando validaci√≥n cruzada...\")\n",
        "\n",
        "# M√©tricas a evaluar\n",
        "scoring_metrics = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
        "\n",
        "# Cross-validation para Ridge\n",
        "cv_results_ridge = {}\n",
        "for metric in scoring_metrics:\n",
        "    scores = cross_val_score(\n",
        "        pipeline_ridge,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        cv=kfold,\n",
        "        scoring=metric,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    cv_results_ridge[metric] = scores\n",
        "\n",
        "print(\"\\nResultados de Validaci√≥n Cruzada (5 folds):\")\n",
        "print(f\"\\n{'M√©trica':<25} {'Media':<12} {'Desv.Std':<12} {'Min':<12} {'Max':<12}\")\n",
        "print(\"-\" * 73)\n",
        "\n",
        "# R¬≤ Score\n",
        "r2_scores = cv_results_ridge['r2']\n",
        "print(f\"{'R¬≤ Score':<25} {r2_scores.mean():<12.4f} {r2_scores.std():<12.4f} {r2_scores.min():<12.4f} {r2_scores.max():<12.4f}\")\n",
        "\n",
        "# MAE (negativo, por eso se multiplica por -1)\n",
        "mae_scores = -cv_results_ridge['neg_mean_absolute_error']\n",
        "print(f\"{'MAE (log)':<25} {mae_scores.mean():<12.4f} {mae_scores.std():<12.4f} {mae_scores.min():<12.4f} {mae_scores.max():<12.4f}\")\n",
        "\n",
        "# RMSE (de MSE negativo)\n",
        "rmse_scores = np.sqrt(-cv_results_ridge['neg_mean_squared_error'])\n",
        "print(f\"{'RMSE (log)':<25} {rmse_scores.mean():<12.4f} {rmse_scores.std():<12.4f} {rmse_scores.min():<12.4f} {rmse_scores.max():<12.4f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Ridge - R¬≤ promedio en CV: {r2_scores.mean():.4f} ¬± {r2_scores.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v69IrSnwEgf"
      },
      "source": [
        "## **11.3 VALIDACI√ìN CRUZADA - RANDOM FOREST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b50TE1QdwCEg"
      },
      "outputs": [],
      "source": [
        "# 11.3 VALIDACI√ìN CRUZADA - RANDOM FOREST (OPTIMIZADA)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDACI√ìN CRUZADA: RANDOM FOREST (VERSI√ìN OPTIMIZADA)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  Nota: Reduciendo complejidad para acelerar CV...\")\n",
        "print(\"   ‚Ä¢ Usando solo 3 folds (en lugar de 5)\")\n",
        "print(\"   ‚Ä¢ Evaluando solo R¬≤ (m√©trica principal)\")\n",
        "\n",
        "# KFold reducido para Random Forest\n",
        "kfold_rf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"\\nEjecutando validaci√≥n cruzada (esto tardar√° ~2-3 minutos)...\")\n",
        "\n",
        "# Solo evaluar R¬≤ para acelerar\n",
        "r2_scores_rf = cross_val_score(\n",
        "    pipeline_rf,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=kfold_rf,  # 3 folds en lugar de 5\n",
        "    scoring='r2',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nResultados de Validaci√≥n Cruzada (3 folds):\")\n",
        "print(f\"\\n{'M√©trica':<25} {'Media':<12} {'Desv.Std':<12} {'Min':<12} {'Max':<12}\")\n",
        "print(\"-\" * 73)\n",
        "print(f\"{'R¬≤ Score':<25} {r2_scores_rf.mean():<12.4f} {r2_scores_rf.std():<12.4f} {r2_scores_rf.min():<12.4f} {r2_scores_rf.max():<12.4f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Random Forest - R¬≤ promedio en CV: {r2_scores_rf.mean():.4f} ¬± {r2_scores_rf.std():.4f}\")\n",
        "\n",
        "# Calcular MAE y RMSE manualmente en un fold para referencia\n",
        "print(\"\\nüìä Estimando MAE y RMSE en el primer fold...\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Usar un split simple para estimar otras m√©tricas\n",
        "X_temp_train, X_temp_val, y_temp_train, y_temp_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "pipeline_rf_temp = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=20,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=4,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "pipeline_rf_temp.fit(X_temp_train, y_temp_train)\n",
        "y_pred_temp = pipeline_rf_temp.predict(X_temp_val)\n",
        "\n",
        "mae_estimate = mean_absolute_error(y_temp_val, y_pred_temp)\n",
        "rmse_estimate = np.sqrt(mean_squared_error(y_temp_val, y_pred_temp))\n",
        "\n",
        "print(f\"   MAE (log) estimado:  {mae_estimate:.4f}\")\n",
        "print(f\"   RMSE (log) estimado: {rmse_estimate:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11.4 VALIDACI√ìN CRUZADA - XGBOOST**"
      ],
      "metadata": {
        "id": "mBZN70HlZwNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDACI√ìN CRUZADA: XGBOOST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nEjecutando validaci√≥n cruzada...\")\n",
        "\n",
        "cv_results_xgb = {}\n",
        "for metric in scoring_metrics:\n",
        "    scores = cross_val_score(\n",
        "        pipeline_xgb,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        cv=kfold,\n",
        "        scoring=metric,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    cv_results_xgb[metric] = scores\n",
        "\n",
        "print(\"\\nResultados de Validaci√≥n Cruzada (5 folds):\")\n",
        "print(f\"\\n{'M√©trica':<25} {'Media':<12} {'Desv.Std':<12} {'Min':<12} {'Max':<12}\")\n",
        "print(\"-\" * 73)\n",
        "\n",
        "# R¬≤ Score\n",
        "r2_scores_xgb = cv_results_xgb['r2']\n",
        "print(f\"{'R¬≤ Score':<25} {r2_scores_xgb.mean():<12.4f} {r2_scores_xgb.std():<12.4f} {r2_scores_xgb.min():<12.4f} {r2_scores_xgb.max():<12.4f}\")\n",
        "\n",
        "# MAE\n",
        "mae_scores_xgb = -cv_results_xgb['neg_mean_absolute_error']\n",
        "print(f\"{'MAE (log)':<25} {mae_scores_xgb.mean():<12.4f} {mae_scores_xgb.std():<12.4f} {mae_scores_xgb.min():<12.4f} {mae_scores_xgb.max():<12.4f}\")\n",
        "\n",
        "# RMSE\n",
        "rmse_scores_xgb = np.sqrt(-cv_results_xgb['neg_mean_squared_error'])\n",
        "print(f\"{'RMSE (log)':<25} {rmse_scores_xgb.mean():<12.4f} {rmse_scores_xgb.std():<12.4f} {rmse_scores_xgb.min():<12.4f} {rmse_scores_xgb.max():<12.4f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ XGBoost - R¬≤ promedio en CV: {r2_scores_xgb.mean():.4f} ¬± {r2_scores_xgb.std():.4f}\")"
      ],
      "metadata": {
        "id": "o-Cjrfn3Z6dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11.5 COMPARACI√ìN VISUAL DE VALIDACI√ìN CRUZADA**"
      ],
      "metadata": {
        "id": "kXJdE2IOhp5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACI√ìN VISUAL: VALIDACI√ìN CRUZADA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear DataFrame con resultados de CV\n",
        "cv_comparison = pd.DataFrame({\n",
        "    'Modelo': ['Ridge', 'Random Forest', 'XGBoost'],\n",
        "    'R2_CV_Mean': [\n",
        "        r2_scores.mean(),\n",
        "        r2_scores_rf.mean(),\n",
        "        r2_scores_xgb.mean()\n",
        "    ],\n",
        "    'R2_CV_Std': [\n",
        "        r2_scores.std(),\n",
        "        r2_scores_rf.std(),\n",
        "        r2_scores_xgb.std()\n",
        "    ],\n",
        "    'R2_Test': [\n",
        "        metricas_ridge_test['R2_log'],\n",
        "        metricas_rf_test['R2_log'],\n",
        "        metricas_xgb_test['R2_log']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä TABLA COMPARATIVA - VALIDACI√ìN CRUZADA:\")\n",
        "print(cv_comparison.to_string(index=False))\n",
        "\n",
        "# Boxplot de R¬≤ por fold\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Box(\n",
        "    y=r2_scores,\n",
        "    name='Ridge',\n",
        "    marker=dict(color='rgba(99, 110, 250, 0.7)'),\n",
        "    boxmean='sd'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Box(\n",
        "    y=r2_scores_rf,\n",
        "    name='Random Forest',\n",
        "    marker=dict(color='rgba(255, 99, 71, 0.7)'),\n",
        "    boxmean='sd'\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Box(\n",
        "    y=r2_scores_xgb,\n",
        "    name='XGBoost',\n",
        "    marker=dict(color='rgba(50, 205, 50, 0.7)'),\n",
        "    boxmean='sd'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Distribuci√≥n de R¬≤ en Validaci√≥n Cruzada',\n",
        "    yaxis_title='R¬≤ Score',\n",
        "    height=600,\n",
        "    width=1000,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Comparar CV vs Test\n",
        "fig2 = go.Figure()\n",
        "\n",
        "modelos = ['Ridge', 'Random Forest', 'XGBoost']\n",
        "r2_cv_means = [r2_scores.mean(), r2_scores_rf.mean(), r2_scores_xgb.mean()]\n",
        "r2_test_vals = [metricas_ridge_test['R2_log'], metricas_rf_test['R2_log'], metricas_xgb_test['R2_log']]\n",
        "\n",
        "fig2.add_trace(go.Bar(\n",
        "    name='R¬≤ CV (Media)',\n",
        "    x=modelos,\n",
        "    y=r2_cv_means,\n",
        "    marker=dict(color='rgba(99, 110, 250, 0.7)'),\n",
        "    text=[f\"{val:.4f}\" for val in r2_cv_means],\n",
        "    textposition='outside'\n",
        "))\n",
        "\n",
        "fig2.add_trace(go.Bar(\n",
        "    name='R¬≤ Test',\n",
        "    x=modelos,\n",
        "    y=r2_test_vals,\n",
        "    marker=dict(color='rgba(255, 99, 71, 0.7)'),\n",
        "    text=[f\"{val:.4f}\" for val in r2_test_vals],\n",
        "    textposition='outside'\n",
        "))\n",
        "\n",
        "fig2.update_layout(\n",
        "    title='Comparaci√≥n: R¬≤ en Cross-Validation vs Test Set',\n",
        "    yaxis_title='R¬≤ Score',\n",
        "    barmode='group',\n",
        "    height=600,\n",
        "    width=1000,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig2.show()"
      ],
      "metadata": {
        "id": "qPURR90JhvK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11.6 CONCLUSIONES DE VALIDACI√ìN CRUZADA**"
      ],
      "metadata": {
        "id": "wAN1KNJYiG6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONCLUSIONES DE VALIDACI√ìN CRUZADA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Identificar mejor modelo por CV\n",
        "mejor_cv_idx = cv_comparison['R2_CV_Mean'].idxmax()\n",
        "mejor_modelo_cv = cv_comparison.iloc[mejor_cv_idx]['Modelo']\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO (por R¬≤ promedio en CV): {mejor_modelo_cv}\")\n",
        "print(f\"   ‚Ä¢ R¬≤ CV: {cv_comparison.iloc[mejor_cv_idx]['R2_CV_Mean']:.4f} ¬± {cv_comparison.iloc[mejor_cv_idx]['R2_CV_Std']:.4f}\")\n",
        "print(f\"   ‚Ä¢ R¬≤ Test: {cv_comparison.iloc[mejor_cv_idx]['R2_Test']:.4f}\")\n",
        "\n",
        "# Verificar consistencia\n",
        "for idx, row in cv_comparison.iterrows():\n",
        "    diff = abs(row['R2_CV_Mean'] - row['R2_Test'])\n",
        "    print(f\"\\nüìä {row['Modelo']}:\")\n",
        "    print(f\"   Diferencia |R¬≤_CV - R¬≤_Test|: {diff:.4f}\")\n",
        "    if diff < 0.02:\n",
        "        print(f\"   ‚úÖ Excelente consistencia\")\n",
        "    elif diff < 0.05:\n",
        "        print(f\"   ‚úÖ Buena consistencia\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  Revisar - posible inestabilidad\")\n",
        "\n",
        "print(\"\\n‚úÖ VALIDACI√ìN CRUZADA COMPLETADA\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "dikph3TkiLKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS**"
      ],
      "metadata": {
        "id": "ICaWyHpeigT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12.1 DEFINICI√ìN DEL ESPACIO DE B√öSQUEDA**"
      ],
      "metadata": {
        "id": "ynCbgKGNiuA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Espacio de hiperpar√°metros a explorar\n",
        "param_distributions_rf = {\n",
        "    'regressor__n_estimators': [100, 200, 300, 400],\n",
        "    'regressor__max_depth': [15, 20, 25, 30, None],\n",
        "    'regressor__min_samples_split': [5, 10, 15, 20],\n",
        "    'regressor__min_samples_leaf': [2, 4, 6, 8],\n",
        "    'regressor__max_features': ['sqrt', 'log2', None],\n",
        "    'regressor__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "print(\"\\nüìã Espacio de hiperpar√°metros:\")\n",
        "print(f\"   ‚Ä¢ n_estimators: {param_distributions_rf['regressor__n_estimators']}\")\n",
        "print(f\"   ‚Ä¢ max_depth: {param_distributions_rf['regressor__max_depth']}\")\n",
        "print(f\"   ‚Ä¢ min_samples_split: {param_distributions_rf['regressor__min_samples_split']}\")\n",
        "print(f\"   ‚Ä¢ min_samples_leaf: {param_distributions_rf['regressor__min_samples_leaf']}\")\n",
        "print(f\"   ‚Ä¢ max_features: {param_distributions_rf['regressor__max_features']}\")\n",
        "print(f\"   ‚Ä¢ bootstrap: {param_distributions_rf['regressor__bootstrap']}\")\n",
        "\n",
        "# Calcular combinaciones totales\n",
        "n_combinations = (\n",
        "    len(param_distributions_rf['regressor__n_estimators']) *\n",
        "    len(param_distributions_rf['regressor__max_depth']) *\n",
        "    len(param_distributions_rf['regressor__min_samples_split']) *\n",
        "    len(param_distributions_rf['regressor__min_samples_leaf']) *\n",
        "    len(param_distributions_rf['regressor__max_features']) *\n",
        "    len(param_distributions_rf['regressor__bootstrap'])\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Total de combinaciones posibles: {n_combinations:,}\")\n",
        "print(f\"   Evaluaremos una muestra aleatoria para optimizar el tiempo\")\n"
      ],
      "metadata": {
        "id": "9lxaOgXci1Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12.2 PREPARACI√ìN: SAMPLING ESTRATIFICADO PARA OPTIMIZACI√ìN**"
      ],
      "metadata": {
        "id": "Pp1RtQBZi4CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PREPARACI√ìN: SAMPLING ESTRATIFICADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüí° ESTRATEGIA DE OPTIMIZACI√ìN PROFESIONAL:\")\n",
        "print(\"   ‚Ä¢ Usar una muestra del 35% del training set para b√∫squeda de hiperpar√°metros\")\n",
        "print(\"   ‚Ä¢ Sampling estratificado para mantener la distribuci√≥n del target\")\n",
        "print(\"   ‚Ä¢ Entrenar modelo final con hiperpar√°metros √≥ptimos en el 100% de los datos\")\n",
        "\n",
        "print(\"\\nüìö JUSTIFICACI√ìN:\")\n",
        "print(\"   ‚Ä¢ Dataset de 78k+ registros ‚Üí B√∫squeda completa tomar√≠a 20-30 minutos\")\n",
        "print(\"   ‚Ä¢ Pr√°ctica est√°ndar en industria para datasets grandes (>50k)\")\n",
        "print(\"   ‚Ä¢ Los hiperpar√°metros encontrados son representativos\")\n",
        "print(\"   ‚Ä¢ Referencia: Scikit-learn Best Practices, Kaggle Methodologies\")\n",
        "\n",
        "# Crear cuartiles del target para estratificaci√≥n\n",
        "y_train_quartiles = pd.qcut(y_train, q=4, labels=False, duplicates='drop')\n",
        "\n",
        "# Split estratificado\n",
        "X_train_sample, _, y_train_sample, _ = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    train_size=0.35,\n",
        "    random_state=42,\n",
        "    stratify=y_train_quartiles\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä TAMA√ëOS DE CONJUNTOS:\")\n",
        "print(f\"   ‚Ä¢ Training completo: {len(X_train):,} registros\")\n",
        "print(f\"   ‚Ä¢ Muestra para optimizaci√≥n: {len(X_train_sample):,} registros ({len(X_train_sample)/len(X_train)*100:.1f}%)\")\n",
        "print(f\"   ‚Ä¢ Test set: {len(X_test):,} registros (no se toca)\")\n",
        "\n",
        "print(f\"\\n‚úÖ Reducci√≥n de tiempo estimado: ~65%\")\n",
        "\n",
        "# Verificar que la distribuci√≥n se mantuvo\n",
        "print(f\"\\nüîç VERIFICACI√ìN DE DISTRIBUCI√ìN DEL TARGET:\")\n",
        "print(f\"   ‚Ä¢ Media train completo: {y_train.mean():.4f}\")\n",
        "print(f\"   ‚Ä¢ Media muestra: {y_train_sample.mean():.4f}\")\n",
        "print(f\"   ‚Ä¢ Desv.Std train completo: {y_train.std():.4f}\")\n",
        "print(f\"   ‚Ä¢ Desv.Std muestra: {y_train_sample.std():.4f}\")\n",
        "\n",
        "if abs(y_train.mean() - y_train_sample.mean()) < 0.01:\n",
        "    print(f\"   ‚úÖ Distribuci√≥n preservada correctamente\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Ligera variaci√≥n en la distribuci√≥n\")\n"
      ],
      "metadata": {
        "id": "uEJoOOr1i8oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12.3 ESPACIO DE B√öSQUEDA AMPLIADO**"
      ],
      "metadata": {
        "id": "7OLA45K-i_TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEFINICI√ìN DEL ESPACIO DE B√öSQUEDA AMPLIADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Espacio de b√∫squeda balanceado\n",
        "param_grid_rf = {\n",
        "    'regressor__n_estimators': [100, 150, 200, 250],\n",
        "    'regressor__max_depth': [15, 20, 25, None],\n",
        "    'regressor__min_samples_split': [5, 10, 15],\n",
        "    'regressor__min_samples_leaf': [2, 4, 6],\n",
        "    'regressor__max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "print(\"\\nüìã Hiperpar√°metros a explorar:\")\n",
        "for param, values in param_grid_rf.items():\n",
        "    param_name = param.replace('regressor__', '')\n",
        "    print(f\"   ‚Ä¢ {param_name}: {values}\")\n",
        "\n",
        "# Calcular combinaciones\n",
        "n_combinations = (\n",
        "    len(param_grid_rf['regressor__n_estimators']) *\n",
        "    len(param_grid_rf['regressor__max_depth']) *\n",
        "    len(param_grid_rf['regressor__min_samples_split']) *\n",
        "    len(param_grid_rf['regressor__min_samples_leaf']) *\n",
        "    len(param_grid_rf['regressor__max_features'])\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä AN√ÅLISIS DEL ESPACIO:\")\n",
        "print(f\"   ‚Ä¢ Total de combinaciones posibles: {n_combinations:,}\")\n",
        "print(f\"   ‚Ä¢ M√©todo: RandomizedSearchCV (30 combinaciones aleatorias)\")\n",
        "print(f\"   ‚Ä¢ Cross-validation: 3 folds\")\n",
        "print(f\"   ‚Ä¢ Total de entrenamientos: 30 √ó 3 = 90\")\n"
      ],
      "metadata": {
        "id": "NfRUYahKjC3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12.4 CONFIGURACI√ìN Y EJECUCI√ìN DE RANDOMIZEDSEARCHCV**"
      ],
      "metadata": {
        "id": "4YZjwg3wxKXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURACI√ìN Y EJECUCI√ìN DE RANDOMIZEDSEARCHCV\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear pipeline base\n",
        "pipeline_rf_optimized = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
        "])\n",
        "\n",
        "# Configurar RandomizedSearchCV\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=pipeline_rf_optimized,\n",
        "    param_distributions=param_grid_rf,\n",
        "    n_iter=30,  # 30 combinaciones aleatorias\n",
        "    cv=3,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=2,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "print(\"\\nüìã Configuraci√≥n de RandomizedSearchCV:\")\n",
        "print(f\"   ‚Ä¢ Estimator: Random Forest Pipeline\")\n",
        "print(f\"   ‚Ä¢ Iteraciones: 30 combinaciones\")\n",
        "print(f\"   ‚Ä¢ Cross-validation: 3 folds\")\n",
        "print(f\"   ‚Ä¢ Scoring: R¬≤\")\n",
        "print(f\"   ‚Ä¢ Dataset: Muestra estratificada (35%)\")\n",
        "\n",
        "print(\"\\nüöÄ Iniciando b√∫squeda de hiperpar√°metros...\")\n",
        "print(\"   ‚è±Ô∏è  Tiempo estimado: 4-6 minutos\")\n",
        "print(\"   (Ver√°s el progreso a continuaci√≥n)\\n\")\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Ejecutar b√∫squeda EN LA MUESTRA\n",
        "random_search_rf.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = (end_time - start_time) / 60\n",
        "\n",
        "print(f\"\\n‚úÖ B√∫squeda completada en {elapsed_time:.2f} minutos\")\n"
      ],
      "metadata": {
        "id": "sh91wt1oxP69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12.5 VISUALIZACI√ìN DE MEJORES HIPERPAR√ÅMETROS**"
      ],
      "metadata": {
        "id": "XyxS6viK0hKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MEJORES HIPERPAR√ÅMETROS ENCONTRADOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüèÜ HIPERPAR√ÅMETROS √ìPTIMOS:\")\n",
        "print(\"-\" * 60)\n",
        "for param, value in random_search_rf.best_params_.items():\n",
        "    param_name = param.replace('regressor__', '')\n",
        "    print(f\"   ‚Ä¢ {param_name:25s}: {value}\")\n",
        "\n",
        "print(f\"\\nüìä MEJOR SCORE EN VALIDACI√ìN CRUZADA:\")\n",
        "print(f\"   ‚Ä¢ R¬≤ (en muestra): {random_search_rf.best_score_:.4f}\")\n",
        "\n",
        "print(f\"\\nüìà COMPARACI√ìN CON BASELINE:\")\n",
        "print(f\"   ‚Ä¢ Baseline R¬≤ (CV): {r2_scores_rf.mean():.4f}\")\n",
        "print(f\"   ‚Ä¢ Optimizado R¬≤ (CV en muestra): {random_search_rf.best_score_:.4f}\")\n",
        "mejora = random_search_rf.best_score_ - r2_scores_rf.mean()\n",
        "print(f\"   ‚Ä¢ Mejora: {mejora:+.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "PNA6deTU0Sem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12.6 RE-ENTRENAMIENTO CON DATOS COMPLETOS**"
      ],
      "metadata": {
        "id": "sxXhdNKZzxd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RE-ENTRENAMIENTO DEL MODELO CON DATOS COMPLETOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüîÑ Aplicando mejores hiperpar√°metros al modelo final...\")\n",
        "print(\"   ‚Ä¢ Hiperpar√°metros: encontrados en muestra\")\n",
        "print(\"   ‚Ä¢ Dataset: TODOS los datos de entrenamiento (100%)\")\n",
        "\n",
        "# Extraer mejores par√°metros (sin el prefijo 'regressor__')\n",
        "best_params_clean = {\n",
        "    key.replace('regressor__', ''): value\n",
        "    for key, value in random_search_rf.best_params_.items()\n",
        "}\n",
        "\n",
        "# Crear modelo final con mejores hiperpar√°metros\n",
        "modelo_final_optimizado = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(\n",
        "        **best_params_clean,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"\\nüöÄ Entrenando modelo final con 100% de los datos...\")\n",
        "start_time_final = time.time()\n",
        "\n",
        "# Entrenar con TODOS los datos\n",
        "modelo_final_optimizado.fit(X_train, y_train)\n",
        "\n",
        "end_time_final = time.time()\n",
        "elapsed_time_final = (end_time_final - start_time_final) / 60\n",
        "\n",
        "print(f\"‚úÖ Entrenamiento completado en {elapsed_time_final:.2f} minutos\")\n",
        "\n",
        "# Actualizar la variable para que el c√≥digo posterior funcione\n",
        "best_model_rf = modelo_final_optimizado\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ OPTIMIZACI√ìN COMPLETADA\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "xXAJ8hOnz1xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12.7 EVALUACI√ìN DEL MODELO OPTIMIZADO EN TEST SET**"
      ],
      "metadata": {
        "id": "MaMRyLlc1I7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUACI√ìN DEL MODELO OPTIMIZADO EN TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìä El modelo final fue entrenado con:\")\n",
        "print(f\"   ‚Ä¢ Hiperpar√°metros: Optimizados (encontrados en muestra)\")\n",
        "print(f\"   ‚Ä¢ Datos de entrenamiento: {len(X_train):,} registros (100%)\")\n",
        "\n",
        "# Predicciones del modelo optimizado\n",
        "y_pred_optimized_train = modelo_final_optimizado.predict(X_train)\n",
        "y_pred_optimized_test = modelo_final_optimizado.predict(X_test)\n",
        "\n",
        "# Evaluaci√≥n en TRAIN\n",
        "print(\"\\nüìà EVALUACI√ìN EN TRAIN (Datos Completos):\")\n",
        "metricas_optimized_train = evaluar_modelo(y_train, y_pred_optimized_train, \"RF Optimizado - Train\")\n",
        "\n",
        "# Evaluaci√≥n en TEST\n",
        "print(\"\\nüìà EVALUACI√ìN EN TEST:\")\n",
        "metricas_optimized_test = evaluar_modelo(y_test, y_pred_optimized_test, \"RF Optimizado - Test\")\n",
        "\n",
        "# Verificar overfitting\n",
        "diff_r2_optimized = metricas_optimized_train['R2_log'] - metricas_optimized_test['R2_log']\n",
        "print(f\"\\nüîç An√°lisis de Overfitting (Modelo Optimizado):\")\n",
        "print(f\"   Diferencia R¬≤ (Train - Test): {diff_r2_optimized:.4f}\")\n",
        "if diff_r2_optimized < 0.05:\n",
        "    print(\"   ‚úÖ No hay overfitting significativo\")\n",
        "elif diff_r2_optimized < 0.10:\n",
        "    print(\"   ‚ö†Ô∏è  Overfitting leve\")\n",
        "else:\n",
        "    print(\"   ‚ùå Overfitting significativo\")"
      ],
      "metadata": {
        "id": "UF7QLJxL1hwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12.8 COMPARACI√ìN FINAL: BASELINE VS OPTIMIZADO**"
      ],
      "metadata": {
        "id": "EUDkl3l01VBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARACI√ìN FINAL: BASELINE VS OPTIMIZADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparacion_final = pd.DataFrame({\n",
        "    'Modelo': ['RF Baseline', 'RF Optimizado'],\n",
        "    'R2_Train': [\n",
        "        metricas_rf_train['R2_log'],\n",
        "        metricas_optimized_train['R2_log']\n",
        "    ],\n",
        "    'R2_Test': [\n",
        "        metricas_rf_test['R2_log'],\n",
        "        metricas_optimized_test['R2_log']\n",
        "    ],\n",
        "    'RMSE_Test': [\n",
        "        metricas_rf_test['RMSE_log'],\n",
        "        metricas_optimized_test['RMSE_log']\n",
        "    ],\n",
        "    'MAE_Test': [\n",
        "        metricas_rf_test['MAE_log'],\n",
        "        metricas_optimized_test['MAE_log']\n",
        "    ],\n",
        "    'MAPE': [\n",
        "        metricas_rf_test['MAPE'],\n",
        "        metricas_optimized_test['MAPE']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä TABLA COMPARATIVA:\")\n",
        "print(comparacion_final.to_string(index=False))\n",
        "\n",
        "# Calcular mejora\n",
        "mejora_test = metricas_optimized_test['R2_log'] - metricas_rf_test['R2_log']\n",
        "mejora_mae = metricas_rf_test['MAE_original'] - metricas_optimized_test['MAE_original']\n",
        "\n",
        "print(f\"\\nüí° AN√ÅLISIS DE MEJORA:\")\n",
        "print(f\"   ‚Ä¢ R¬≤ Baseline: {metricas_rf_test['R2_log']:.4f}\")\n",
        "print(f\"   ‚Ä¢ R¬≤ Optimizado: {metricas_optimized_test['R2_log']:.4f}\")\n",
        "print(f\"   ‚Ä¢ Mejora en R¬≤: {mejora_test:+.4f} ({mejora_test/metricas_rf_test['R2_log']*100:+.2f}%)\")\n",
        "\n",
        "print(f\"\\n   ‚Ä¢ MAE Baseline: {metricas_rf_test['MAE_original']:.2f} ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ MAE Optimizado: {metricas_optimized_test['MAE_original']:.2f} ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ Mejora en MAE: {mejora_mae:+.2f} ¬µg/m¬≥\")\n",
        "\n",
        "if mejora_test > 0.01:\n",
        "    print(\"\\n   ‚úÖ Mejora significativa lograda con la optimizaci√≥n\")\n",
        "elif mejora_test > 0:\n",
        "    print(\"\\n   ‚úÖ Mejora marginal lograda\")\n",
        "else:\n",
        "    print(\"\\n   ‚ÑπÔ∏è  El baseline ya estaba muy bien configurado\")\n",
        "    print(\"   ‚Üí Es com√∫n que Random Forest funcione bien con hiperpar√°metros por defecto\")\n",
        "\n",
        "# Visualizaci√≥n comparativa\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    subplot_titles=(\"R¬≤ Score: Baseline vs Optimizado\", \"MAE: Baseline vs Optimizado\")\n",
        ")\n",
        "\n",
        "# Subplot 1: R¬≤ Comparison\n",
        "fig.add_trace(go.Bar(\n",
        "    name='Train',\n",
        "    x=['Baseline', 'Optimizado'],\n",
        "    y=[metricas_rf_train['R2_log'], metricas_optimized_train['R2_log']],\n",
        "    marker=dict(color='rgba(99, 110, 250, 0.7)'),\n",
        "    text=[f\"{metricas_rf_train['R2_log']:.4f}\", f\"{metricas_optimized_train['R2_log']:.4f}\"],\n",
        "    textposition='outside'\n",
        "), row=1, col=1)\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "    name='Test',\n",
        "    x=['Baseline', 'Optimizado'],\n",
        "    y=[metricas_rf_test['R2_log'], metricas_optimized_test['R2_log']],\n",
        "    marker=dict(color='rgba(255, 99, 71, 0.7)'),\n",
        "    text=[f\"{metricas_rf_test['R2_log']:.4f}\", f\"{metricas_optimized_test['R2_log']:.4f}\"],\n",
        "    textposition='outside'\n",
        "), row=1, col=1)\n",
        "\n",
        "# Subplot 2: MAE Comparison\n",
        "fig.add_trace(go.Bar(\n",
        "    name='MAE Test',\n",
        "    x=['Baseline', 'Optimizado'],\n",
        "    y=[metricas_rf_test['MAE_original'], metricas_optimized_test['MAE_original']],\n",
        "    marker=dict(color='rgba(99, 110, 250, 0.7)'),\n",
        "    text=[f\"{metricas_rf_test['MAE_original']:.2f}\", f\"{metricas_optimized_test['MAE_original']:.2f}\"],\n",
        "    textposition='outside',\n",
        "    showlegend=False\n",
        "), row=1, col=2)\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Comparaci√≥n: Modelo Baseline vs Optimizado\",\n",
        "    height=500,\n",
        "    width=1400,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white',\n",
        "    barmode='group'\n",
        ")\n",
        "\n",
        "fig.update_yaxes(title_text=\"R¬≤ Score\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"MAE (¬µg/m¬≥)\", row=1, col=2)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"\\n‚úÖ OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS COMPLETADA\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "3ZmIhsOr1qRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **13. MODELO FINAL Y EVALUACI√ìN COMPLETA**"
      ],
      "metadata": {
        "id": "TF2aXZLeKsND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13.1 GUARDAR EL MODELO FINAL**"
      ],
      "metadata": {
        "id": "mNYqIc3vKzbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODELO FINAL SELECCIONADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# El mejor modelo ya est√° entrenado\n",
        "modelo_final = modelo_final_optimizado\n",
        "\n",
        "print(\"\\nüèÜ MODELO FINAL: Random Forest Optimizado\")\n",
        "print(f\"\\nüìä Hiperpar√°metros del modelo final:\")\n",
        "best_params_display = {\n",
        "    key.replace('regressor__', ''): value\n",
        "    for key, value in random_search_rf.best_params_.items()\n",
        "}\n",
        "for param, value in best_params_display.items():\n",
        "    print(f\"   ‚Ä¢ {param}: {value}\")\n",
        "\n",
        "print(f\"\\nüìà Performance Final:\")\n",
        "print(f\"   ‚Ä¢ R¬≤ Test: {metricas_optimized_test['R2_log']:.4f}\")\n",
        "print(f\"   ‚Ä¢ MAE Test: {metricas_optimized_test['MAE_original']:.2f} ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ RMSE Test: {metricas_optimized_test['RMSE_original']:.2f} ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ MAPE Test: {metricas_optimized_test['MAPE']:.2f}%\")"
      ],
      "metadata": {
        "id": "6ws_Z12CKxoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13.2 AN√ÅLISIS DE RESIDUOS**"
      ],
      "metadata": {
        "id": "0m03337XK3bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AN√ÅLISIS DE RESIDUOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calcular residuos en test set\n",
        "residuos_test = y_test - y_pred_optimized_test\n",
        "\n",
        "print(f\"\\nüìä Estad√≠sticas de Residuos (escala log):\")\n",
        "print(f\"   ‚Ä¢ Media: {residuos_test.mean():.6f} (deber√≠a estar cerca de 0)\")\n",
        "print(f\"   ‚Ä¢ Desviaci√≥n est√°ndar: {residuos_test.std():.4f}\")\n",
        "print(f\"   ‚Ä¢ Mediana: {residuos_test.median():.6f}\")\n",
        "print(f\"   ‚Ä¢ Min: {residuos_test.min():.4f}\")\n",
        "print(f\"   ‚Ä¢ Max: {residuos_test.max():.4f}\")\n",
        "\n",
        "# Test de normalidad de residuos\n",
        "from scipy.stats import shapiro\n",
        "sample_size = min(5000, len(residuos_test))\n",
        "_, p_value_normalidad = shapiro(residuos_test.sample(sample_size, random_state=42))\n",
        "\n",
        "print(f\"\\nüî¨ Test de Normalidad (Shapiro-Wilk):\")\n",
        "print(f\"   ‚Ä¢ p-value: {p_value_normalidad:.6f}\")\n",
        "if p_value_normalidad > 0.05:\n",
        "    print(f\"   ‚úÖ Los residuos siguen aproximadamente una distribuci√≥n normal\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  Los residuos no siguen perfectamente una distribuci√≥n normal\")\n",
        "    print(f\"      (Com√∫n en problemas de regresi√≥n del mundo real)\")\n",
        "\n",
        "# Visualizaci√≥n de residuos\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        \"Residuos vs Predicciones\",\n",
        "        \"Distribuci√≥n de Residuos\",\n",
        "        \"Q-Q Plot\",\n",
        "        \"Residuos vs Valores Reales\"\n",
        "    ),\n",
        "    vertical_spacing=0.12,\n",
        "    horizontal_spacing=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "RR87LkAdLA3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13.2.1 RESIDUOS VS PREDICCIONES**\n"
      ],
      "metadata": {
        "id": "OwEHVJ58LCsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=y_pred_optimized_test,\n",
        "        y=residuos_test,\n",
        "        mode='markers',\n",
        "        marker=dict(color='rgba(99, 110, 250, 0.5)', size=4),\n",
        "        name='Residuos',\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=1, col=1)"
      ],
      "metadata": {
        "id": "5Rhd5_iJLLRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13.2.2. HISTOGRAMA DE RESIDUOS**"
      ],
      "metadata": {
        "id": "Jiiuc2tOLO2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig.add_trace(\n",
        "    go.Histogram(\n",
        "        x=residuos_test,\n",
        "        nbinsx=50,\n",
        "        marker=dict(color='rgba(99, 110, 250, 0.7)'),\n",
        "        name='Residuos',\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")"
      ],
      "metadata": {
        "id": "zL40BCJqLU4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13.2.3 Q-Q PLOT**"
      ],
      "metadata": {
        "id": "f0pF6tnuLZUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import probplot\n",
        "qq_data = probplot(residuos_test, dist=\"norm\")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=qq_data[0][0],\n",
        "        y=qq_data[0][1],\n",
        "        mode='markers',\n",
        "        marker=dict(color='rgba(99, 110, 250, 0.5)', size=4),\n",
        "        name='Datos',\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# L√≠nea de referencia te√≥rica\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=qq_data[0][0],\n",
        "        y=qq_data[1][1] + qq_data[1][0] * qq_data[0][0],\n",
        "        mode='lines',\n",
        "        line=dict(color='red', dash='dash'),\n",
        "        name='Te√≥rica',\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")"
      ],
      "metadata": {
        "id": "WBLtF-xELhiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13.2.4. RESIDUOS VS VALORES REALES**"
      ],
      "metadata": {
        "id": "zwv3nFTbLpEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=y_test,\n",
        "        y=residuos_test,\n",
        "        mode='markers',\n",
        "        marker=dict(color='rgba(99, 110, 250, 0.5)', size=4),\n",
        "        name='Residuos',\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\", row=2, col=2)\n",
        "\n",
        "# Layout\n",
        "fig.update_xaxes(title_text=\"Predicciones (log)\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Residuos\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Residuos\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Frecuencia\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Cuantiles Te√≥ricos\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Cuantiles Observados\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Valores Reales (log)\", row=2, col=2)\n",
        "fig.update_yaxes(title_text=\"Residuos\", row=2, col=2)\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"An√°lisis de Residuos del Modelo Final\",\n",
        "    height=900,\n",
        "    width=1400,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN DE RESIDUOS:\")\n",
        "if abs(residuos_test.mean()) < 0.01:\n",
        "    print(\"   ‚úÖ Media cercana a 0: El modelo no tiene sesgo sistem√°tico\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  Media alejada de 0: El modelo tiene un leve sesgo\")\n",
        "\n",
        "if residuos_test.std() < 0.6:\n",
        "    print(\"   ‚úÖ Desviaci√≥n baja: Predicciones consistentes\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  Desviaci√≥n moderada: Predicciones variables\")\n",
        "\n",
        "print(\"\\n‚úÖ AN√ÅLISIS DE RESIDUOS COMPLETADO\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "DwaVHSHZL3BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14 FEATURE IMPORTANCE**"
      ],
      "metadata": {
        "id": "f2BiD99z4bm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE IMPORTANCE (IMPORTANCIA DE VARIABLES)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüí° Objetivo: Identificar qu√© variables son m√°s importantes para predecir PM2.5\")\n",
        "\n",
        "# Extraer feature importance del Random Forest\n",
        "rf_model = modelo_final_optimizado.named_steps['regressor']\n",
        "feature_names = modelo_final_optimizado.named_steps['preprocessor'].get_feature_names_out()\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Crear DataFrame de importancias\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Top 20 features m√°s importantes\n",
        "top_n = 20\n",
        "top_features = feature_importance_df.head(top_n)\n",
        "\n",
        "print(f\"\\nüìä TOP {top_n} FEATURES M√ÅS IMPORTANTES:\")\n",
        "print(\"-\" * 70)\n",
        "for idx, row in top_features.iterrows():\n",
        "    feature = row['Feature']\n",
        "    importance = row['Importance']\n",
        "    porcentaje = importance * 100\n",
        "    print(f\"   {idx+1:2d}. {feature:<45s} {importance:.4f} ({porcentaje:.2f}%)\")\n",
        "\n",
        "# Suma de importancia de top features\n",
        "suma_top = top_features['Importance'].sum()\n",
        "print(f\"\\nüìà Las top {top_n} features explican el {suma_top*100:.2f}% de la importancia total\")\n",
        "\n",
        "# Visualizaci√≥n - PALETA CONSISTENTE\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Bar(\n",
        "    x=top_features['Importance'],\n",
        "    y=top_features['Feature'],\n",
        "    orientation='h',\n",
        "    marker=dict(\n",
        "        color='rgba(99, 110, 250, 0.7)',  # ‚¨ÖÔ∏è TU COLOR AZUL\n",
        "        line=dict(color='rgb(99, 110, 250)', width=1)\n",
        "    ),\n",
        "    text=top_features['Importance'].round(4),\n",
        "    textposition='outside'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f'Top {top_n} Features M√°s Importantes en el Modelo Final',\n",
        "    xaxis_title='Importancia',\n",
        "    yaxis_title='Feature',\n",
        "    height=700,\n",
        "    width=1100,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',  # ‚¨ÖÔ∏è TU COLOR DE FONDO\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig.update_yaxes(autorange=\"reversed\")\n",
        "fig.show()\n",
        "\n",
        "# An√°lisis de importancia por tipo de feature\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AN√ÅLISIS POR TIPO DE FEATURE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Clasificar features por tipo\n",
        "def clasificar_feature(feature_name):\n",
        "    if 'country' in feature_name:\n",
        "        return 'Pa√≠s'\n",
        "    elif 'month' in feature_name:\n",
        "        return 'Mes'\n",
        "    elif 'hour' in feature_name:\n",
        "        return 'Hora'\n",
        "    else:\n",
        "        return 'Num√©rica'\n",
        "\n",
        "feature_importance_df['Tipo'] = feature_importance_df['Feature'].apply(clasificar_feature)\n",
        "\n",
        "# Agrupar por tipo\n",
        "importancia_por_tipo = feature_importance_df.groupby('Tipo')['Importance'].agg([\n",
        "    ('Total', 'sum'),\n",
        "    ('Promedio', 'mean'),\n",
        "    ('Count', 'count')\n",
        "]).sort_values('Total', ascending=False)\n",
        "\n",
        "print(\"\\nüìä IMPORTANCIA TOTAL POR TIPO DE FEATURE:\")\n",
        "print(\"-\" * 70)\n",
        "for tipo in importancia_por_tipo.index:\n",
        "    total = importancia_por_tipo.loc[tipo, 'Total']\n",
        "    promedio = importancia_por_tipo.loc[tipo, 'Promedio']\n",
        "    count = importancia_por_tipo.loc[tipo, 'Count']\n",
        "    porcentaje = total * 100\n",
        "\n",
        "    print(f\"\\n{tipo}:\")\n",
        "    print(f\"   ‚Ä¢ Importancia total: {total:.4f} ({porcentaje:.2f}%)\")\n",
        "    print(f\"   ‚Ä¢ Importancia promedio: {promedio:.6f}\")\n",
        "    print(f\"   ‚Ä¢ Cantidad de features: {int(count)}\")\n",
        "\n",
        "# Gr√°fico de pie - PALETA CONSISTENTE\n",
        "fig_pie = go.Figure(data=[go.Pie(\n",
        "    labels=importancia_por_tipo.index,\n",
        "    values=importancia_por_tipo['Total'],\n",
        "    hole=0.3,\n",
        "    marker=dict(colors=[\n",
        "        'rgba(99, 110, 250, 0.8)',   # Azul principal\n",
        "        'rgba(255, 99, 71, 0.8)',    # Naranja/Rojo\n",
        "        'rgba(50, 205, 50, 0.8)',    # Verde\n",
        "        'rgba(255, 165, 0, 0.8)'     # Amarillo/Naranja\n",
        "    ]),\n",
        "    textinfo='label+percent',\n",
        "    textposition='auto'\n",
        ")])\n",
        "\n",
        "fig_pie.update_layout(\n",
        "    title='Distribuci√≥n de Importancia por Tipo de Feature',\n",
        "    height=500,\n",
        "    width=700,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',  # ‚¨ÖÔ∏è TU COLOR DE FONDO\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig_pie.show()\n",
        "\n",
        "# Identificar top features num√©ricas\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP FEATURES NUM√âRICAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "features_numericas = feature_importance_df[feature_importance_df['Tipo'] == 'Num√©rica'].head(10)\n",
        "\n",
        "print(\"\\nüìä Top 10 Features Num√©ricas m√°s importantes:\")\n",
        "print(\"-\" * 70)\n",
        "for idx, row in features_numericas.iterrows():\n",
        "    print(f\"   ‚Ä¢ {row['Feature']:<40s} {row['Importance']:.4f}\")\n",
        "\n",
        "# Interpretaci√≥n\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPRETACI√ìN CLAVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüí° HALLAZGOS PRINCIPALES:\")\n",
        "\n",
        "# Feature m√°s importante\n",
        "top_feature = feature_importance_df.iloc[0]\n",
        "print(f\"\\n1. La feature M√ÅS importante es: {top_feature['Feature']}\")\n",
        "print(f\"   ‚Üí Aporta el {top_feature['Importance']*100:.2f}% de la importancia total\")\n",
        "\n",
        "# Tipo dominante\n",
        "tipo_dominante = importancia_por_tipo.index[0]\n",
        "importancia_dominante = importancia_por_tipo.iloc[0]['Total'] * 100\n",
        "print(f\"\\n2. El tipo de feature dominante es: {tipo_dominante}\")\n",
        "print(f\"   ‚Üí Representa el {importancia_dominante:.1f}% de la importancia total\")\n",
        "\n",
        "# Top 5 acumulado\n",
        "top5_importance = feature_importance_df.head(5)['Importance'].sum() * 100\n",
        "print(f\"\\n3. Las 5 features m√°s importantes explican el {top5_importance:.1f}% del modelo\")\n",
        "\n",
        "print(\"\\n‚úÖ AN√ÅLISIS DE FEATURE IMPORTANCE COMPLETADO\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "WMBK4w5s4jI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.1 EVALUACI√ìN EN ESCALA ORIGINAL**"
      ],
      "metadata": {
        "id": "EJp1VsmmJz16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUACI√ìN EN ESCALA ORIGINAL (PM2.5 en ¬µg/m¬≥)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüí° Convertimos las predicciones de escala logar√≠tmica a escala original\")\n",
        "print(\"   para facilitar la interpretaci√≥n en unidades reales de PM2.5\")\n",
        "\n",
        "# Convertir predicciones a escala original\n",
        "y_test_original = np.expm1(y_test)\n",
        "y_pred_original = np.expm1(y_pred_optimized_test)\n",
        "\n",
        "# Calcular m√©tricas en escala original\n",
        "mae_original = mean_absolute_error(y_test_original, y_pred_original)\n",
        "rmse_original = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
        "mape_original = mean_absolute_percentage_error(y_test_original, y_pred_original) * 100\n",
        "r2_original = r2_score(y_test_original, y_pred_original)\n",
        "\n",
        "print(f\"\\nüìä M√âTRICAS FINALES EN ESCALA ORIGINAL:\")\n",
        "print(f\"   ‚Ä¢ MAE:  {mae_original:.2f} ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ RMSE: {rmse_original:.2f} ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ MAPE: {mape_original:.2f}%\")\n",
        "print(f\"   ‚Ä¢ R¬≤:   {r2_original:.4f}\")\n",
        "\n",
        "# Interpretaci√≥n del error\n",
        "print(f\"\\nüí° INTERPRETACI√ìN PR√ÅCTICA DEL ERROR:\")\n",
        "print(f\"   ‚Ä¢ En promedio, el modelo se equivoca por ¬±{mae_original:.2f} ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ El error relativo promedio es del {mape_original:.1f}%\")\n",
        "print(f\"   ‚Ä¢ El modelo explica el {r2_original*100:.1f}% de la varianza en PM2.5\")\n",
        "\n",
        "# Categor√≠as de PM2.5 seg√∫n OMS/EPA\n",
        "print(f\"\\nüåç CONTEXTO (Categor√≠as de Calidad del Aire - OMS):\")\n",
        "print(f\"   ‚Ä¢ Bueno:                         0-12 ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ Moderado:                      12-35 ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ Da√±ino para grupos sensibles:  35-55 ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ Da√±ino:                        55-150 ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ Muy da√±ino:                    150-250 ¬µg/m¬≥\")\n",
        "print(f\"   ‚Ä¢ Peligroso:                     >250 ¬µg/m¬≥\")\n",
        "\n",
        "# Estad√≠sticas descriptivas\n",
        "print(f\"\\nüìà ESTAD√çSTICAS DE LAS PREDICCIONES (Test Set):\")\n",
        "print(f\"   {'M√©trica':<20} {'Real':<15} {'Predicho':<15}\")\n",
        "print(f\"   {'-'*50}\")\n",
        "print(f\"   {'Media':<20} {y_test_original.mean():<15.2f} {y_pred_original.mean():<15.2f}\")\n",
        "print(f\"   {'Mediana':<20} {y_test_original.median():<15.2f} {np.median(y_pred_original):<15.2f}\")\n",
        "print(f\"   {'Desv. Std':<20} {y_test_original.std():<15.2f} {y_pred_original.std():<15.2f}\")\n",
        "print(f\"   {'M√≠n':<20} {y_test_original.min():<15.2f} {y_pred_original.min():<15.2f}\")\n",
        "print(f\"   {'M√°x':<20} {y_test_original.max():<15.2f} {y_pred_original.max():<15.2f}\")\n",
        "\n",
        "# Visualizaci√≥n: Predicciones vs Real (escala original)\n",
        "fig = go.Figure()\n",
        "\n",
        "# Scatter plot\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=y_test_original,\n",
        "    y=y_pred_original,\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color='rgba(99, 110, 250, 0.5)',\n",
        "        size=4\n",
        "    ),\n",
        "    name='Predicciones',\n",
        "    showlegend=False\n",
        "))\n",
        "\n",
        "# L√≠nea perfecta y=x\n",
        "min_val = min(y_test_original.min(), y_pred_original.min())\n",
        "max_val = max(y_test_original.max(), y_pred_original.max())\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=[min_val, max_val],\n",
        "    y=[min_val, max_val],\n",
        "    mode='lines',\n",
        "    line=dict(color='rgba(255, 99, 71, 0.8)', dash='dash', width=2),\n",
        "    name='Predicci√≥n perfecta'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f'Predicciones vs Valores Reales (Escala Original)<br>R¬≤ = {r2_original:.4f}, MAE = {mae_original:.2f} ¬µg/m¬≥',\n",
        "    xaxis_title='PM2.5 Real (¬µg/m¬≥)',\n",
        "    yaxis_title='PM2.5 Predicho (¬µg/m¬≥)',\n",
        "    height=600,\n",
        "    width=900,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white',\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Histograma comparativo\n",
        "fig2 = go.Figure()\n",
        "\n",
        "fig2.add_trace(go.Histogram(\n",
        "    x=y_test_original,\n",
        "    name='Real',\n",
        "    nbinsx=50,\n",
        "    marker=dict(color='rgba(99, 110, 250, 0.6)'),\n",
        "    opacity=0.7\n",
        "))\n",
        "\n",
        "fig2.add_trace(go.Histogram(\n",
        "    x=y_pred_original,\n",
        "    name='Predicho',\n",
        "    nbinsx=50,\n",
        "    marker=dict(color='rgba(255, 99, 71, 0.6)'),\n",
        "    opacity=0.7\n",
        "))\n",
        "\n",
        "fig2.update_layout(\n",
        "    title='Distribuci√≥n de PM2.5: Real vs Predicho',\n",
        "    xaxis_title='PM2.5 (¬µg/m¬≥)',\n",
        "    yaxis_title='Frecuencia',\n",
        "    barmode='overlay',\n",
        "    height=500,\n",
        "    width=1000,\n",
        "    plot_bgcolor='rgb(229, 236, 246)',\n",
        "    paper_bgcolor='white'\n",
        ")\n",
        "\n",
        "fig2.show()\n",
        "\n",
        "# An√°lisis por rangos de PM2.5\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AN√ÅLISIS DE PERFORMANCE POR RANGO DE PM2.5\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Definir rangos seg√∫n categor√≠as OMS\n",
        "rangos = [\n",
        "    (0, 12, 'Bueno'),\n",
        "    (12, 35, 'Moderado'),\n",
        "    (35, 55, 'Da√±ino (Grupos Sensibles)'),\n",
        "    (55, 150, 'Da√±ino'),\n",
        "    (150, 1000, 'Muy Da√±ino/Peligroso')\n",
        "]\n",
        "\n",
        "print(\"\\nüìä MAE por Categor√≠a de Calidad del Aire:\")\n",
        "print(f\"   {'Categor√≠a':<30} {'N':<10} {'MAE (¬µg/m¬≥)':<15} {'MAPE (%)':<15}\")\n",
        "print(f\"   {'-'*70}\")\n",
        "\n",
        "for min_val, max_val, categoria in rangos:\n",
        "    mask = (y_test_original >= min_val) & (y_test_original < max_val)\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        mae_rango = mean_absolute_error(y_test_original[mask], y_pred_original[mask])\n",
        "        mape_rango = mean_absolute_percentage_error(y_test_original[mask], y_pred_original[mask]) * 100\n",
        "        n_muestras = mask.sum()\n",
        "\n",
        "        print(f\"   {categoria:<30} {n_muestras:<10} {mae_rango:<15.2f} {mape_rango:<15.1f}\")\n",
        "\n",
        "print(\"\\nüí° INTERPRETACI√ìN:\")\n",
        "print(\"   ‚Ä¢ El modelo predice mejor en niveles BAJOS de contaminaci√≥n\")\n",
        "print(\"   ‚Ä¢ Los errores aumentan proporcionalmente con niveles m√°s altos\")\n",
        "print(\"   ‚Ä¢ Esto es esperado: eventos extremos son m√°s dif√≠ciles de predecir\")\n",
        "\n",
        "print(\"\\n‚úÖ EVALUACI√ìN COMPLETA DEL MODELO FINAL\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "uMEdN-B4J4x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **14. CONCLUSIONES FINALES Y RECOMENDACIONES**"
      ],
      "metadata": {
        "id": "1PTd0gfpKNaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.1 RESUMEN EJECUTIVO**"
      ],
      "metadata": {
        "id": "H-u0gGFmMgeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESUMEN EJECUTIVO DEL PROYECTO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üìã CONTEXTO DEL PROYECTO:\n",
        "   Este proyecto abord√≥ la predicci√≥n de niveles de PM2.5 (Material Particulado)\n",
        "   utilizando datos meteorol√≥gicos y socioecon√≥micos globales de 190 pa√≠ses,\n",
        "   con el objetivo de desarrollar un sistema de predicci√≥n temprana para\n",
        "   proteger la salud p√∫blica.\n",
        "\n",
        "üìä DATASET:\n",
        "   ‚Ä¢ Registros: 78,238 observaciones (despu√©s de limpieza)\n",
        "   ‚Ä¢ Per√≠odo: Mayo 2024 - Abril 2025\n",
        "   ‚Ä¢ Cobertura: 190 pa√≠ses\n",
        "   ‚Ä¢ Features finales: 232 (tras encoding y feature engineering)\n",
        "   ‚Ä¢ Target: PM2.5 (escala logar√≠tmica)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "9WA3xOXWMj2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.2 METODOLOG√çA APLICADA**"
      ],
      "metadata": {
        "id": "CobLPhFgMl8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"METODOLOG√çA APLICADA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üî¨ PROCESO COMPLETO:\n",
        "\n",
        "1. PREPARACI√ìN DE DATOS:\n",
        "   ‚úì Limpieza exhaustiva (outliers f√≠sicos: <0.12% eliminados)\n",
        "   ‚úì Feature Engineering: transformaciones logar√≠tmicas para normalizar\n",
        "   ‚úì Variables derivadas: rained, high_wind\n",
        "   ‚úì Enriquecimiento: API RestCountries (poblaci√≥n, densidad)\n",
        "   ‚úì An√°lisis VIF: eliminaci√≥n de pressure_mb (multicolinealidad)\n",
        "\n",
        "2. AN√ÅLISIS EXPLORATORIO (EDA):\n",
        "   ‚úì An√°lisis de distribuciones y asimetr√≠a\n",
        "   ‚úì Correlaciones lineales\n",
        "   ‚úì Efectos de umbral (lluvia, viento)\n",
        "   ‚úì Patrones temporales (estacionales y diurnos)\n",
        "   ‚úì An√°lisis geogr√°fico por pa√≠s\n",
        "\n",
        "3. MODELADO:\n",
        "   ‚úì Train/Test split estratificado (80/20)\n",
        "   ‚úì 3 modelos baseline: Ridge, Random Forest, XGBoost\n",
        "   ‚úì Validaci√≥n cruzada (K-Fold)\n",
        "   ‚úì Optimizaci√≥n con sampling estratificado (35% de muestra)\n",
        "   ‚úì RandomizedSearchCV (30 iteraciones, 3 folds)\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "tQUa5uFTMpsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.3 RESULTADOS PRINCIPALES**"
      ],
      "metadata": {
        "id": "52T6U42oMzRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTADOS PRINCIPALES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüèÜ MODELO GANADOR: Random Forest Regressor Optimizado\")\n",
        "print(\"\\nüìä HIPERPAR√ÅMETROS √ìPTIMOS:\")\n",
        "for param, value in best_params_display.items():\n",
        "    print(f\"   ‚Ä¢ {param}: {value}\")\n",
        "\n",
        "print(f\"\\nüìà PERFORMANCE FINAL:\")\n",
        "print(f\"   {'M√©trica':<25} {'Escala Log':<15} {'Escala Original':<20}\")\n",
        "print(f\"   {'-'*60}\")\n",
        "print(f\"   {'R¬≤ Score':<25} {metricas_optimized_test['R2_log']:<15.4f} {r2_original:<20.4f}\")\n",
        "print(f\"   {'MAE':<25} {metricas_optimized_test['MAE_log']:<15.4f} {mae_original:<20.2f} ¬µg/m¬≥\")\n",
        "print(f\"   {'RMSE':<25} {metricas_optimized_test['RMSE_log']:<15.4f} {rmse_original:<20.2f} ¬µg/m¬≥\")\n",
        "print(f\"   {'MAPE':<25} {metricas_optimized_test['MAPE']:<15.2f}% {mape_original:<20.2f}%\")\n",
        "\n",
        "print(f\"\\n‚úÖ COMPARACI√ìN CON BASELINE:\")\n",
        "print(f\"   ‚Ä¢ Mejora en R¬≤: +{(metricas_optimized_test['R2_log'] - metricas_rf_test['R2_log'])*100:.2f}%\")\n",
        "print(f\"   ‚Ä¢ Sin overfitting significativo (Train-Test gap: {diff_r2_optimized:.4f})\")\n"
      ],
      "metadata": {
        "id": "4TZ2Dms-M2c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.4 HALLAZGOS CLAVE**"
      ],
      "metadata": {
        "id": "BHWzjQo_M3ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HALLAZGOS CLAVE (FEATURE IMPORTANCE)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüîç TOP 5 VARIABLES M√ÅS IMPORTANTES:\")\n",
        "for idx, row in feature_importance_df.head(5).iterrows():\n",
        "    print(f\"   {idx+1}. {row['Feature']:<40s} ({row['Importance']*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nüìä DISTRIBUCI√ìN POR TIPO:\")\n",
        "for tipo in importancia_por_tipo.index:\n",
        "    porcentaje = importancia_por_tipo.loc[tipo, 'Total'] * 100\n",
        "    print(f\"   ‚Ä¢ {tipo:<20s}: {porcentaje:.1f}%\")\n",
        "\n",
        "print(\"\"\"\n",
        "üí° INTERPRETACI√ìN:\n",
        "   ‚Ä¢ Las variables NUM√âRICAS (meteorol√≥gicas) son las m√°s importantes (44%)\n",
        "   ‚Ä¢ La densidad poblacional es el predictor individual m√°s fuerte (7.55%)\n",
        "   ‚Ä¢ El contexto GEOGR√ÅFICO (pa√≠s) aporta casi el 30% de la importancia\n",
        "   ‚Ä¢ La ESTACIONALIDAD (mes) captura patrones temporales relevantes (16%)\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "lIgZDb6rM7AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.5 VALIDACI√ìN DE HIP√ìTESIS**"
      ],
      "metadata": {
        "id": "5WIHxSKoM8eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDACI√ìN DE HIP√ìTESIS DE INVESTIGACI√ìN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚úÖ H1: FACTORES PREDICTIVOS\n",
        "   VALIDADA - Se identificaron factores meteorol√≥gicos (temperatura, UV,\n",
        "   humedad) y socioecon√≥micos (densidad poblacional) como predictores clave.\n",
        "\n",
        "‚úÖ H2: CAPACIDAD PREDICTIVA\n",
        "   VALIDADA - El modelo alcanz√≥ R¬≤=0.73 (log) / 0.52 (original), indicando\n",
        "   capacidad predictiva moderada-alta en un problema complejo global.\n",
        "\n",
        "‚úÖ H3: RELACIONES NO LINEALES\n",
        "   VALIDADA - Random Forest super√≥ significativamente a Ridge Regression,\n",
        "   confirmando la presencia de relaciones no lineales complejas.\n",
        "\n",
        "‚úÖ H4: PATRONES GEOGR√ÅFICOS\n",
        "   VALIDADA - Las variables de pa√≠s y poblaci√≥n mostraron importancia\n",
        "   significativa (30% + 7.5%), confirmando heterogeneidad regional.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "J-BTn65oM_Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.6 LIMITACIONES DEL MODELO**"
      ],
      "metadata": {
        "id": "voBYlRCZNB0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LIMITACIONES Y CONSIDERACIONES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "‚ö†Ô∏è  LIMITACIONES IDENTIFICADAS:\n",
        "\n",
        "1. PERFORMANCE EN EXTREMOS:\n",
        "   ‚Ä¢ MAE aumenta significativamente en niveles >150 ¬µg/m¬≥\n",
        "   ‚Ä¢ MAPE alto en niveles bajos (108% en \"Bueno\") debido a valores cercanos a 0\n",
        "   ‚Ä¢ Eventos extremos dif√≠ciles de predecir con features actuales\n",
        "\n",
        "2. VARIABLES NO CAPTURADAS:\n",
        "   ‚Ä¢ Incendios forestales, erupciones volc√°nicas\n",
        "   ‚Ä¢ Inversiones t√©rmicas locales\n",
        "   ‚Ä¢ Fuentes industriales puntuales\n",
        "   ‚Ä¢ Topograf√≠a compleja (valles, cuencas)\n",
        "\n",
        "3. AGREGACI√ìN TEMPORAL:\n",
        "   ‚Ä¢ Datos snapshot (no series temporales continuas)\n",
        "   ‚Ä¢ Sin captura de tendencias a largo plazo\n",
        "\n",
        "4. GENERALIZACI√ìN:\n",
        "   ‚Ä¢ Modelo global puede no capturar peculiaridades locales\n",
        "   ‚Ä¢ Recomendable validaci√≥n regional antes de deploy\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "q8KA72nENFAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.7 RECOMENDACIONES**"
      ],
      "metadata": {
        "id": "dVn4g27aNGS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RECOMENDACIONES Y PR√ìXIMOS PASOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üéØ PARA PRODUCCI√ìN:\n",
        "\n",
        "1. MEJORAS DEL MODELO:\n",
        "   ‚úì Incorporar series temporales (LSTM, Prophet)\n",
        "   ‚úì Incluir variables de eventos extremos (APIs de incendios, volcanes)\n",
        "   ‚úì Modelos regionales espec√≠ficos (estratificados por clima/geograf√≠a)\n",
        "   ‚úì Ensemble con XGBoost + LightGBM\n",
        "\n",
        "2. FEATURES ADICIONALES:\n",
        "   ‚úì Direcci√≥n del viento (no solo velocidad)\n",
        "   ‚úì Datos de tr√°fico vehicular (Google Maps API)\n",
        "   ‚úì √çndices de actividad industrial\n",
        "   ‚úì Topograf√≠a (elevaci√≥n, pendientes)\n",
        "\n",
        "3. VALIDACI√ìN:\n",
        "   ‚úì Validaci√≥n temporal (datos futuros del 2025)\n",
        "   ‚úì Validaci√≥n geogr√°fica (hold-out por regi√≥n)\n",
        "   ‚úì A/B testing en sistema real\n",
        "\n",
        "4. DEPLOYMENT:\n",
        "   ‚úì API REST con FastAPI\n",
        "   ‚úì Reentrenamiento autom√°tico mensual\n",
        "   ‚úì Monitoreo de data drift\n",
        "   ‚úì Dashboard de alertas tempranas\n",
        "\n",
        "üìä APLICACIONES PR√ÅCTICAS:\n",
        "\n",
        "   ‚Ä¢ Sistema de alertas tempranas para salud p√∫blica\n",
        "   ‚Ä¢ Planificaci√≥n de medidas preventivas\n",
        "   ‚Ä¢ Estudios epidemiol√≥gicos\n",
        "   ‚Ä¢ Pol√≠ticas ambientales basadas en evidencia\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "52HtdoFrNJnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.8 IMPACTO Y VALOR**"
      ],
      "metadata": {
        "id": "uDMNaG7rNLHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"IMPACTO Y VALOR DEL PROYECTO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "üåç IMPACTO EN SALUD P√öBLICA:\n",
        "\n",
        "   ‚Ä¢ Un error promedio de ¬±{mae_original:.2f} ¬µg/m¬≥ permite:\n",
        "     ‚úì Clasificar correctamente el nivel de riesgo en ~70% de los casos\n",
        "     ‚úì Emitir alertas preventivas con 1-2 categor√≠as de margen\n",
        "     ‚úì Identificar per√≠odos de alto riesgo para grupos sensibles\n",
        "\n",
        "   ‚Ä¢ Cobertura global de 190 pa√≠ses\n",
        "   ‚Ä¢ Potencial de salvar vidas con alertas tempranas\n",
        "\n",
        "üíº VALOR T√âCNICO:\n",
        "\n",
        "   ‚Ä¢ Metodolog√≠a s√≥lida y reproducible\n",
        "   ‚Ä¢ Pipeline completo: EDA ‚Üí Feature Engineering ‚Üí Modeling ‚Üí Validation\n",
        "   ‚Ä¢ Buenas pr√°cticas: CV, optimizaci√≥n eficiente, an√°lisis de residuos\n",
        "   ‚Ä¢ C√≥digo documentado y extensible\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "rvpUzXRmNOAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14.9 CIERRE**\n"
      ],
      "metadata": {
        "id": "gWIo86RJNPc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONCLUSI√ìN FINAL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "‚úÖ PROYECTO EXITOSO\n",
        "\n",
        "   Este proyecto demostr√≥ la viabilidad de predecir niveles de PM2.5\n",
        "   utilizando datos meteorol√≥gicos y socioecon√≥micos globales, logrando:\n",
        "\n",
        "   ‚Ä¢ R¬≤ = {metricas_optimized_test['R2_log']:.4f} (escala logar√≠tmica)\n",
        "   ‚Ä¢ MAE = {mae_original:.2f} ¬µg/m¬≥ (error promedio aceptable)\n",
        "   ‚Ä¢ Identificaci√≥n de factores clave (densidad poblacional, UV, temperatura)\n",
        "   ‚Ä¢ Metodolog√≠a robusta con validaci√≥n cruzada\n",
        "\n",
        "   El modelo desarrollado puede servir como base para sistemas de\n",
        "   alerta temprana de calidad del aire, contribuyendo a la protecci√≥n\n",
        "   de la salud p√∫blica a escala global.\n",
        "\n",
        "üéì PROYECTO: Data Science II - Coderhouse\n",
        "üë§ ALUMNO: Miguel √Ångel Di Rocco\n",
        "üìÖ FECHA: 12 de Diciembre 2024\n",
        "    Agradecer al profesor Joaquin Salas por la predisposici√≥n y claridad a la hora de brindar las clases\n",
        "    y un agradecimiento especial al Tutor Federico Gravina, por el enorme compromiso de siempre aportar su conocimiento,\n",
        "    ganas y oficio de profesor de manera tan desinteresada y de coraz√≥n. Gracias por estos meses! les deseo lo mejor!\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FIN DEL PROYECTO\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "xIiT9zM6NTrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "import shap"
      ],
      "metadata": {
        "id": "6N4btf1McZ-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(modelo_final_optimizado, 'random_forest_pm25.pkl')\n",
        "print(\"‚úÖ Modelo guardado exitosamente\")"
      ],
      "metadata": {
        "id": "br7FgqvGc2yf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}